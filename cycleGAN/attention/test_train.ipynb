{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from instancenormalization import InstanceNormalization\n",
    "\n",
    "from atten_Unet_5 import att_unet \n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader2 import data_loader\n",
    "from data_loader2 import load_data\n",
    "import numpy as np\n",
    "import os, random, sys, datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "# ramdom_seed = 5198\n",
    "# np.random.seed(ramdom_seed)\n",
    "# random.seed(ramdom_seed)\n",
    "# os.environ['PYTHONHASHSEED'] = str(ramdom_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        data_path1 = r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210624_delete\\train\\20210625_sub01\\Recordings'\n",
    "        data_path2 = r'D:\\DATA\\ENDOSCOPY\\yonsei\\normal_crop'\n",
    "        self.batch_size = 6\n",
    "        self.data1 = load_data(data_path1)\n",
    "        self.data2 = load_data(data_path2)\n",
    "        self.data_loader = data_loader(self.data1, self.data2, self.batch_size, aug = True)\n",
    "        self.dataset_name = 'images'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = att_unet(self.img_rows, self.img_cols, data_format='channels_last')\n",
    "        self.g_BA = att_unet(self.img_rows, self.img_cols, data_format='channels_last')\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mse', 'mse'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((self.batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((self.batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i in range(150):\n",
    "                imgs_A, imgs_B = next(self.data_loader)\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                if batch_i % 100 ==0:\n",
    "                    print (\"[Epoch %d/%d] [Batch %d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "            if epoch % 10 == 0:\n",
    "                gan.g_AB.save_weights('saved_model/AB_%04d.h5'%epoch)\n",
    "                gan.g_BA.save_weights('saved_model/BA_%04d.h5'%epoch)\n",
    "        \n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A, imgs_B = next(self.data_loader)\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A[0:1])\n",
    "        fake_A = self.g_BA.predict(imgs_B[0:1])\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A[0:1], fake_B, reconstr_A, imgs_B[0:1], fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.subplot(231)\n",
    "        plt.imshow(gen_imgs[0])\n",
    "        plt.subplot(232)\n",
    "        plt.imshow(gen_imgs[1])\n",
    "        plt.subplot(233)\n",
    "        plt.imshow(gen_imgs[2])\n",
    "        plt.subplot(234)\n",
    "        plt.imshow(gen_imgs[3])\n",
    "        plt.subplot(235)\n",
    "        plt.imshow(gen_imgs[4])\n",
    "        plt.subplot(236)\n",
    "        plt.imshow(gen_imgs[5])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"images/%s/%04d_%04d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1342/1342 [00:10<00:00, 124.59it/s]\n",
      "100%|██████████| 685/685 [00:15<00:00, 44.50it/s]\n"
     ]
    }
   ],
   "source": [
    "gan = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0/200] [Batch 0] [D loss: 12.251261, acc:  54%] [G loss: 48.408470, adv: 16.284626, recon: 0.724377, id: 0.687152] time: 0:00:36.181536 \n",
      "[Epoch 0/200] [Batch 100] [D loss: 0.493621, acc:   7%] [G loss: 3.342554, adv: 0.616946, recon: 0.096543, id: 0.073101] time: 0:03:12.839591 \n",
      "[Epoch 1/200] [Batch 0] [D loss: 0.362621, acc:  29%] [G loss: 3.735363, adv: 0.691148, recon: 0.107989, id: 0.137533] time: 0:04:32.454047 \n",
      "[Epoch 1/200] [Batch 100] [D loss: 0.552391, acc:   9%] [G loss: 2.721480, adv: 0.466244, recon: 0.076285, id: 0.151360] time: 0:07:03.496780 \n",
      "[Epoch 2/200] [Batch 0] [D loss: 0.397191, acc:  24%] [G loss: 2.487226, adv: 0.454807, recon: 0.070139, id: 0.089655] time: 0:08:19.542775 \n",
      "[Epoch 2/200] [Batch 100] [D loss: 0.329447, acc:  35%] [G loss: 2.210384, adv: 0.466734, recon: 0.054515, id: 0.119331] time: 0:10:59.285031 \n",
      "[Epoch 3/200] [Batch 0] [D loss: 0.330025, acc:  29%] [G loss: 2.289338, adv: 0.443241, recon: 0.061514, id: 0.103549] time: 0:12:17.513181 \n",
      "[Epoch 3/200] [Batch 100] [D loss: 0.289941, acc:  50%] [G loss: 2.549287, adv: 0.527388, recon: 0.067904, id: 0.078719] time: 0:14:54.863312 \n",
      "[Epoch 4/200] [Batch 0] [D loss: 0.290355, acc:  49%] [G loss: 2.231154, adv: 0.490350, recon: 0.054428, id: 0.101758] time: 0:16:13.279793 \n",
      "[Epoch 4/200] [Batch 100] [D loss: 0.254967, acc:  53%] [G loss: 2.356186, adv: 0.466517, recon: 0.060745, id: 0.111583] time: 0:18:49.887394 \n",
      "[Epoch 5/200] [Batch 0] [D loss: 0.213971, acc:  66%] [G loss: 2.690318, adv: 0.673569, recon: 0.056313, id: 0.124206] time: 0:20:09.796478 \n",
      "[Epoch 5/200] [Batch 100] [D loss: 0.233813, acc:  66%] [G loss: 2.538925, adv: 0.503823, recon: 0.069050, id: 0.039650] time: 0:22:54.137078 \n",
      "[Epoch 6/200] [Batch 0] [D loss: 0.229643, acc:  60%] [G loss: 2.547374, adv: 0.572976, recon: 0.062689, id: 0.073013] time: 0:24:15.096126 \n",
      "[Epoch 6/200] [Batch 100] [D loss: 0.213427, acc:  65%] [G loss: 2.501738, adv: 0.465560, recon: 0.069341, id: 0.115618] time: 0:26:52.135270 \n",
      "[Epoch 7/200] [Batch 0] [D loss: 0.251969, acc:  57%] [G loss: 2.733116, adv: 0.500976, recon: 0.077290, id: 0.096661] time: 0:28:10.358542 \n",
      "[Epoch 7/200] [Batch 100] [D loss: 0.307800, acc:  48%] [G loss: 2.448149, adv: 0.409792, recon: 0.071375, id: 0.117421] time: 0:30:45.744789 \n",
      "[Epoch 8/200] [Batch 0] [D loss: 0.240108, acc:  60%] [G loss: 2.571366, adv: 0.470867, recon: 0.072229, id: 0.100087] time: 0:32:06.717168 \n",
      "[Epoch 8/200] [Batch 100] [D loss: 0.243478, acc:  56%] [G loss: 2.526272, adv: 0.460712, recon: 0.068861, id: 0.156816] time: 0:34:52.418829 \n",
      "[Epoch 9/200] [Batch 0] [D loss: 0.273854, acc:  53%] [G loss: 2.087583, adv: 0.447884, recon: 0.050482, id: 0.102216] time: 0:36:15.013952 \n",
      "[Epoch 9/200] [Batch 100] [D loss: 0.178017, acc:  73%] [G loss: 2.183244, adv: 0.555901, recon: 0.041603, id: 0.131254] time: 0:38:54.422760 \n",
      "[Epoch 10/200] [Batch 0] [D loss: 0.218461, acc:  70%] [G loss: 2.582426, adv: 0.551999, recon: 0.067485, id: 0.048121] time: 0:40:11.246197 \n",
      "[Epoch 10/200] [Batch 100] [D loss: 0.218037, acc:  62%] [G loss: 2.446863, adv: 0.515883, recon: 0.061883, id: 0.062356] time: 0:42:38.507201 \n",
      "[Epoch 11/200] [Batch 0] [D loss: 0.251224, acc:  56%] [G loss: 2.233405, adv: 0.459588, recon: 0.061177, id: 0.024442] time: 0:43:52.877287 \n",
      "[Epoch 11/200] [Batch 100] [D loss: 0.216690, acc:  66%] [G loss: 2.410921, adv: 0.459882, recon: 0.068916, id: 0.037326] time: 0:46:20.237718 \n",
      "[Epoch 12/200] [Batch 0] [D loss: 0.318646, acc:  39%] [G loss: 2.088777, adv: 0.493945, recon: 0.049516, id: 0.033634] time: 0:47:35.357563 \n",
      "[Epoch 12/200] [Batch 100] [D loss: 0.185344, acc:  70%] [G loss: 2.200438, adv: 0.507838, recon: 0.051714, id: 0.073625] time: 0:50:02.698369 \n",
      "[Epoch 13/200] [Batch 0] [D loss: 0.255582, acc:  51%] [G loss: 2.114598, adv: 0.427693, recon: 0.057233, id: 0.038288] time: 0:51:16.775333 \n",
      "[Epoch 13/200] [Batch 100] [D loss: 0.225389, acc:  65%] [G loss: 2.578940, adv: 0.455046, recon: 0.072460, id: 0.142524] time: 0:53:43.997602 \n",
      "[Epoch 14/200] [Batch 0] [D loss: 0.256830, acc:  55%] [G loss: 2.022219, adv: 0.420552, recon: 0.051327, id: 0.081364] time: 0:54:57.947081 \n",
      "[Epoch 14/200] [Batch 100] [D loss: 0.243742, acc:  53%] [G loss: 1.991901, adv: 0.465123, recon: 0.047815, id: 0.017096] time: 0:57:26.382428 \n",
      "[Epoch 15/200] [Batch 0] [D loss: 0.263735, acc:  63%] [G loss: 2.372255, adv: 0.533367, recon: 0.060233, id: 0.034369] time: 0:58:40.425840 \n",
      "[Epoch 15/200] [Batch 100] [D loss: 0.237355, acc:  61%] [G loss: 2.164991, adv: 0.500209, recon: 0.052107, id: 0.062343] time: 1:01:07.734870 \n",
      "[Epoch 16/200] [Batch 0] [D loss: 0.212111, acc:  68%] [G loss: 1.961663, adv: 0.494300, recon: 0.045064, id: 0.014955] time: 1:02:21.922162 \n",
      "[Epoch 16/200] [Batch 100] [D loss: 0.303865, acc:  55%] [G loss: 2.785393, adv: 0.491492, recon: 0.079994, id: 0.172308] time: 1:04:49.324386 \n",
      "[Epoch 17/200] [Batch 0] [D loss: 0.245445, acc:  56%] [G loss: 2.238242, adv: 0.515230, recon: 0.055851, id: 0.015542] time: 1:06:03.393100 \n",
      "[Epoch 17/200] [Batch 100] [D loss: 0.240090, acc:  57%] [G loss: 2.128376, adv: 0.429655, recon: 0.057782, id: 0.015457] time: 1:08:31.857355 \n",
      "[Epoch 18/200] [Batch 0] [D loss: 0.164164, acc:  79%] [G loss: 2.317399, adv: 0.544498, recon: 0.057106, id: 0.030759] time: 1:09:45.982252 \n",
      "[Epoch 18/200] [Batch 100] [D loss: 0.289115, acc:  52%] [G loss: 1.807163, adv: 0.419681, recon: 0.040983, id: 0.082909] time: 1:12:13.346400 \n",
      "[Epoch 19/200] [Batch 0] [D loss: 0.212908, acc:  66%] [G loss: 2.134663, adv: 0.467017, recon: 0.056838, id: 0.011285] time: 1:13:28.611521 \n",
      "[Epoch 19/200] [Batch 100] [D loss: 0.205436, acc:  66%] [G loss: 2.565252, adv: 0.546856, recon: 0.069393, id: 0.025463] time: 1:16:05.318658 \n",
      "[Epoch 20/200] [Batch 0] [D loss: 0.191569, acc:  72%] [G loss: 2.162271, adv: 0.493136, recon: 0.055492, id: 0.010409] time: 1:17:23.600477 \n",
      "[Epoch 20/200] [Batch 100] [D loss: 0.235926, acc:  58%] [G loss: 2.060821, adv: 0.433302, recon: 0.053941, id: 0.021551] time: 1:19:59.365216 \n",
      "[Epoch 21/200] [Batch 0] [D loss: 0.175209, acc:  74%] [G loss: 2.337294, adv: 0.551809, recon: 0.057630, id: 0.011988] time: 1:21:17.927553 \n",
      "[Epoch 21/200] [Batch 100] [D loss: 0.202893, acc:  64%] [G loss: 2.356235, adv: 0.488770, recon: 0.065853, id: 0.010823] time: 1:23:48.491733 \n",
      "[Epoch 22/200] [Batch 0] [D loss: 0.176676, acc:  74%] [G loss: 2.170391, adv: 0.507571, recon: 0.054426, id: 0.011009] time: 1:25:11.031261 \n",
      "[Epoch 22/200] [Batch 100] [D loss: 0.202191, acc:  65%] [G loss: 2.467144, adv: 0.592698, recon: 0.054777, id: 0.135477] time: 1:28:01.792487 \n",
      "[Epoch 23/200] [Batch 0] [D loss: 0.232892, acc:  61%] [G loss: 2.012454, adv: 0.425049, recon: 0.055546, id: 0.013559] time: 1:29:31.076152 \n",
      "[Epoch 23/200] [Batch 100] [D loss: 0.246640, acc:  60%] [G loss: 2.055294, adv: 0.421276, recon: 0.055970, id: 0.019061] time: 1:32:17.369785 \n",
      "[Epoch 24/200] [Batch 0] [D loss: 0.282097, acc:  52%] [G loss: 1.901109, adv: 0.367816, recon: 0.054080, id: 0.025246] time: 1:33:40.853999 \n",
      "[Epoch 24/200] [Batch 100] [D loss: 0.227993, acc:  61%] [G loss: 1.860738, adv: 0.466124, recon: 0.043601, id: 0.014327] time: 1:36:47.407961 \n",
      "[Epoch 25/200] [Batch 0] [D loss: 0.224453, acc:  67%] [G loss: 2.312233, adv: 0.537750, recon: 0.057908, id: 0.058115] time: 1:38:22.437025 \n",
      "[Epoch 25/200] [Batch 100] [D loss: 0.220926, acc:  62%] [G loss: 1.923311, adv: 0.453688, recon: 0.049266, id: 0.009767] time: 1:41:26.923037 \n",
      "[Epoch 26/200] [Batch 0] [D loss: 0.227983, acc:  62%] [G loss: 1.985573, adv: 0.431640, recon: 0.053519, id: 0.041419] time: 1:42:54.713347 \n",
      "[Epoch 26/200] [Batch 100] [D loss: 0.222825, acc:  66%] [G loss: 1.988401, adv: 0.429798, recon: 0.053673, id: 0.034064] time: 1:45:50.142127 \n",
      "[Epoch 27/200] [Batch 0] [D loss: 0.240971, acc:  56%] [G loss: 2.120633, adv: 0.482179, recon: 0.056640, id: 0.010658] time: 1:47:18.519901 \n",
      "[Epoch 27/200] [Batch 100] [D loss: 0.278898, acc:  56%] [G loss: 1.938024, adv: 0.477964, recon: 0.045210, id: 0.041639] time: 1:50:16.749267 \n",
      "[Epoch 28/200] [Batch 0] [D loss: 0.262629, acc:  50%] [G loss: 1.838931, adv: 0.378158, recon: 0.050840, id: 0.007561] time: 1:51:46.666704 \n",
      "[Epoch 28/200] [Batch 100] [D loss: 0.194961, acc:  71%] [G loss: 2.043401, adv: 0.503100, recon: 0.049840, id: 0.005412] time: 1:54:45.959086 \n",
      "[Epoch 29/200] [Batch 0] [D loss: 0.216201, acc:  69%] [G loss: 2.019982, adv: 0.476148, recon: 0.051353, id: 0.011765] time: 1:56:15.865064 \n",
      "[Epoch 29/200] [Batch 100] [D loss: 0.162773, acc:  79%] [G loss: 2.080437, adv: 0.568500, recon: 0.045823, id: 0.014027] time: 1:59:14.732042 \n",
      "[Epoch 30/200] [Batch 0] [D loss: 0.243734, acc:  62%] [G loss: 1.856644, adv: 0.450239, recon: 0.045887, id: 0.025287] time: 2:00:44.593946 \n",
      "[Epoch 30/200] [Batch 100] [D loss: 0.257178, acc:  58%] [G loss: 2.042170, adv: 0.526764, recon: 0.048553, id: 0.008786] time: 2:03:41.753308 \n",
      "[Epoch 31/200] [Batch 0] [D loss: 0.216875, acc:  62%] [G loss: 1.910995, adv: 0.479414, recon: 0.045176, id: 0.013765] time: 2:05:02.129713 \n",
      "[Epoch 31/200] [Batch 100] [D loss: 0.209017, acc:  66%] [G loss: 2.242043, adv: 0.558849, recon: 0.055359, id: 0.005030] time: 2:07:36.638263 \n",
      "[Epoch 32/200] [Batch 0] [D loss: 0.222220, acc:  66%] [G loss: 1.761231, adv: 0.470642, recon: 0.039750, id: 0.011460] time: 2:09:02.228306 \n",
      "[Epoch 32/200] [Batch 100] [D loss: 0.237990, acc:  64%] [G loss: 2.143682, adv: 0.450396, recon: 0.060688, id: 0.018152] time: 2:11:47.813929 \n",
      "[Epoch 33/200] [Batch 0] [D loss: 0.249004, acc:  55%] [G loss: 1.873681, adv: 0.468244, recon: 0.044551, id: 0.016084] time: 2:13:10.956054 \n",
      "[Epoch 33/200] [Batch 100] [D loss: 0.249159, acc:  54%] [G loss: 1.933630, adv: 0.438586, recon: 0.049200, id: 0.008472] time: 2:15:50.114168 \n",
      "[Epoch 34/200] [Batch 0] [D loss: 0.291528, acc:  37%] [G loss: 1.754243, adv: 0.407026, recon: 0.044430, id: 0.042645] time: 2:17:09.802723 \n",
      "[Epoch 34/200] [Batch 100] [D loss: 0.227448, acc:  62%] [G loss: 1.920704, adv: 0.550879, recon: 0.039759, id: 0.011293] time: 2:19:49.068283 \n",
      "[Epoch 35/200] [Batch 0] [D loss: 0.168563, acc:  78%] [G loss: 2.275101, adv: 0.534024, recon: 0.055372, id: 0.037430] time: 2:21:04.980732 \n",
      "[Epoch 35/200] [Batch 100] [D loss: 0.265140, acc:  53%] [G loss: 1.968000, adv: 0.510584, recon: 0.046096, id: 0.007300] time: 2:23:33.300963 \n",
      "[Epoch 36/200] [Batch 0] [D loss: 0.200552, acc:  70%] [G loss: 2.060601, adv: 0.566916, recon: 0.045423, id: 0.006499] time: 2:24:49.005158 \n",
      "[Epoch 36/200] [Batch 100] [D loss: 0.162693, acc:  77%] [G loss: 2.152179, adv: 0.605991, recon: 0.046329, id: 0.005380] time: 2:27:20.149900 \n",
      "[Epoch 37/200] [Batch 0] [D loss: 0.200642, acc:  70%] [G loss: 1.967422, adv: 0.524011, recon: 0.044910, id: 0.011372] time: 2:28:35.119130 \n",
      "[Epoch 37/200] [Batch 100] [D loss: 0.275151, acc:  49%] [G loss: 1.725960, adv: 0.395333, recon: 0.044773, id: 0.011853] time: 2:31:11.933036 \n",
      "[Epoch 38/200] [Batch 0] [D loss: 0.232679, acc:  59%] [G loss: 1.746537, adv: 0.388041, recon: 0.047353, id: 0.009705] time: 2:32:28.330574 \n",
      "[Epoch 38/200] [Batch 100] [D loss: 0.175006, acc:  73%] [G loss: 2.446522, adv: 0.611389, recon: 0.058042, id: 0.039532] time: 2:34:58.862596 \n",
      "[Epoch 39/200] [Batch 0] [D loss: 0.238807, acc:  61%] [G loss: 1.808692, adv: 0.514002, recon: 0.038207, id: 0.006133] time: 2:36:14.117959 \n",
      "[Epoch 39/200] [Batch 100] [D loss: 0.268423, acc:  55%] [G loss: 2.090368, adv: 0.511390, recon: 0.052660, id: 0.005254] time: 2:38:48.048497 \n",
      "[Epoch 40/200] [Batch 0] [D loss: 0.226658, acc:  65%] [G loss: 2.170711, adv: 0.601994, recon: 0.047753, id: 0.006387] time: 2:40:03.711823 \n",
      "[Epoch 40/200] [Batch 100] [D loss: 0.189476, acc:  71%] [G loss: 2.243393, adv: 0.632675, recon: 0.048213, id: 0.006891] time: 2:42:33.428097 \n",
      "[Epoch 41/200] [Batch 0] [D loss: 0.136475, acc:  84%] [G loss: 2.253004, adv: 0.615924, recon: 0.049233, id: 0.027518] time: 2:43:47.295027 \n",
      "[Epoch 41/200] [Batch 100] [D loss: 0.216246, acc:  66%] [G loss: 2.012539, adv: 0.589445, recon: 0.041091, id: 0.004885] time: 2:46:13.624292 \n",
      "[Epoch 42/200] [Batch 0] [D loss: 0.193110, acc:  71%] [G loss: 2.894007, adv: 0.857968, recon: 0.056392, id: 0.042403] time: 2:47:28.701930 \n",
      "[Epoch 42/200] [Batch 100] [D loss: 0.200903, acc:  71%] [G loss: 2.202517, adv: 0.574396, recon: 0.049367, id: 0.058961] time: 2:49:54.874565 \n",
      "[Epoch 43/200] [Batch 0] [D loss: 0.281534, acc:  55%] [G loss: 2.637894, adv: 0.700600, recon: 0.060999, id: 0.010993] time: 2:51:11.214979 \n",
      "[Epoch 43/200] [Batch 100] [D loss: 0.143703, acc:  81%] [G loss: 2.665576, adv: 0.727112, recon: 0.059159, id: 0.015838] time: 2:53:43.145634 \n",
      "[Epoch 44/200] [Batch 0] [D loss: 0.158896, acc:  77%] [G loss: 2.353470, adv: 0.696351, recon: 0.044733, id: 0.025646] time: 2:54:58.640664 \n",
      "[Epoch 44/200] [Batch 100] [D loss: 0.233568, acc:  61%] [G loss: 2.153757, adv: 0.547379, recon: 0.052311, id: 0.006803] time: 2:57:24.537792 \n",
      "[Epoch 45/200] [Batch 0] [D loss: 0.183735, acc:  73%] [G loss: 2.304043, adv: 0.627763, recon: 0.051167, id: 0.016418] time: 2:58:39.819785 \n",
      "[Epoch 45/200] [Batch 100] [D loss: 0.179513, acc:  73%] [G loss: 2.102886, adv: 0.628263, recon: 0.041643, id: 0.006251] time: 3:01:15.180598 \n",
      "[Epoch 46/200] [Batch 0] [D loss: 0.210596, acc:  64%] [G loss: 2.231427, adv: 0.687871, recon: 0.042131, id: 0.006266] time: 3:02:32.023454 \n",
      "[Epoch 46/200] [Batch 100] [D loss: 0.155160, acc:  80%] [G loss: 2.785856, adv: 0.721003, recon: 0.063153, id: 0.069293] time: 3:05:06.028550 \n",
      "[Epoch 47/200] [Batch 0] [D loss: 0.161236, acc:  76%] [G loss: 2.622849, adv: 0.730317, recon: 0.056543, id: 0.006511] time: 3:06:23.307690 \n",
      "[Epoch 47/200] [Batch 100] [D loss: 0.158304, acc:  76%] [G loss: 2.434026, adv: 0.683032, recon: 0.052448, id: 0.011378] time: 3:08:56.532299 \n",
      "[Epoch 48/200] [Batch 0] [D loss: 0.258086, acc:  60%] [G loss: 2.205622, adv: 0.602016, recon: 0.048601, id: 0.004950] time: 3:10:16.159888 \n",
      "[Epoch 48/200] [Batch 100] [D loss: 0.120565, acc:  86%] [G loss: 2.379427, adv: 0.744498, recon: 0.043785, id: 0.006146] time: 3:12:53.600826 \n",
      "[Epoch 49/200] [Batch 0] [D loss: 0.211903, acc:  63%] [G loss: 2.269280, adv: 0.602967, recon: 0.050277, id: 0.049477] time: 3:14:11.921134 \n",
      "[Epoch 49/200] [Batch 100] [D loss: 0.182769, acc:  66%] [G loss: 1.960862, adv: 0.552704, recon: 0.041895, id: 0.009872] time: 3:16:43.522054 \n",
      "[Epoch 50/200] [Batch 0] [D loss: 0.223137, acc:  61%] [G loss: 2.085879, adv: 0.547162, recon: 0.048792, id: 0.004427] time: 3:17:57.052168 \n",
      "[Epoch 50/200] [Batch 100] [D loss: 0.143162, acc:  82%] [G loss: 2.160840, adv: 0.637360, recon: 0.043364, id: 0.007014] time: 3:20:29.161563 \n",
      "[Epoch 51/200] [Batch 0] [D loss: 0.139285, acc:  81%] [G loss: 2.422669, adv: 0.675446, recon: 0.052631, id: 0.008438] time: 3:21:45.704596 \n",
      "[Epoch 51/200] [Batch 100] [D loss: 0.204121, acc:  68%] [G loss: 1.835443, adv: 0.443358, recon: 0.042181, id: 0.008553] time: 3:24:17.189231 \n",
      "[Epoch 52/200] [Batch 0] [D loss: 0.221335, acc:  61%] [G loss: 1.761330, adv: 0.418601, recon: 0.045541, id: 0.005043] time: 3:25:34.445030 \n",
      "[Epoch 52/200] [Batch 100] [D loss: 0.227112, acc:  58%] [G loss: 1.663109, adv: 0.382570, recon: 0.040553, id: 0.009171] time: 3:28:11.105912 \n",
      "[Epoch 53/200] [Batch 0] [D loss: 0.228697, acc:  60%] [G loss: 1.710516, adv: 0.394803, recon: 0.044950, id: 0.006034] time: 3:29:29.410741 \n",
      "[Epoch 53/200] [Batch 100] [D loss: 0.221438, acc:  62%] [G loss: 1.720755, adv: 0.451777, recon: 0.037895, id: 0.036008] time: 3:32:02.606590 \n",
      "[Epoch 54/200] [Batch 0] [D loss: 0.189055, acc:  70%] [G loss: 1.696956, adv: 0.442166, recon: 0.039948, id: 0.004107] time: 3:33:17.350976 \n",
      "[Epoch 54/200] [Batch 100] [D loss: 0.189370, acc:  70%] [G loss: 1.681239, adv: 0.445261, recon: 0.038776, id: 0.004974] time: 3:35:49.100045 \n",
      "[Epoch 55/200] [Batch 0] [D loss: 0.218743, acc:  59%] [G loss: 1.698161, adv: 0.426288, recon: 0.041286, id: 0.008288] time: 3:37:04.632004 \n",
      "[Epoch 55/200] [Batch 100] [D loss: 0.203858, acc:  64%] [G loss: 1.834453, adv: 0.469226, recon: 0.044088, id: 0.004198] time: 3:39:35.088776 \n",
      "[Epoch 56/200] [Batch 0] [D loss: 0.218166, acc:  62%] [G loss: 1.884672, adv: 0.404667, recon: 0.052985, id: 0.004018] time: 3:40:50.707721 \n",
      "[Epoch 56/200] [Batch 100] [D loss: 0.186630, acc:  71%] [G loss: 1.927940, adv: 0.512705, recon: 0.044411, id: 0.005403] time: 3:43:21.045565 \n",
      "[Epoch 57/200] [Batch 0] [D loss: 0.207302, acc:  66%] [G loss: 1.828853, adv: 0.473751, recon: 0.042779, id: 0.003068] time: 3:44:36.553159 \n",
      "[Epoch 57/200] [Batch 100] [D loss: 0.207744, acc:  63%] [G loss: 1.784872, adv: 0.436562, recon: 0.045022, id: 0.003230] time: 3:47:06.572805 \n",
      "[Epoch 58/200] [Batch 0] [D loss: 0.216331, acc:  62%] [G loss: 2.203268, adv: 0.633947, recon: 0.045872, id: 0.008331] time: 3:48:22.029814 \n",
      "[Epoch 58/200] [Batch 100] [D loss: 0.166663, acc:  77%] [G loss: 2.123097, adv: 0.580864, recon: 0.047387, id: 0.003810] time: 3:50:52.313788 \n",
      "[Epoch 59/200] [Batch 0] [D loss: 0.231012, acc:  58%] [G loss: 1.704497, adv: 0.467165, recon: 0.037903, id: 0.003892] time: 3:52:07.961948 \n",
      "[Epoch 59/200] [Batch 100] [D loss: 0.211074, acc:  65%] [G loss: 1.933159, adv: 0.469066, recon: 0.049221, id: 0.004130] time: 3:54:40.116292 \n",
      "[Epoch 60/200] [Batch 0] [D loss: 0.245197, acc:  55%] [G loss: 1.942845, adv: 0.510807, recon: 0.045091, id: 0.003112] time: 3:55:55.760764 \n",
      "[Epoch 60/200] [Batch 100] [D loss: 0.202729, acc:  70%] [G loss: 1.895164, adv: 0.530492, recon: 0.041277, id: 0.002958] time: 3:58:26.092540 \n",
      "[Epoch 61/200] [Batch 0] [D loss: 0.183808, acc:  69%] [G loss: 1.981680, adv: 0.580819, recon: 0.040083, id: 0.008644] time: 3:59:41.998048 \n",
      "[Epoch 61/200] [Batch 100] [D loss: 0.154348, acc:  79%] [G loss: 2.088815, adv: 0.592962, recon: 0.044318, id: 0.005717] time: 4:02:12.368114 \n",
      "[Epoch 62/200] [Batch 0] [D loss: 0.234284, acc:  58%] [G loss: 1.699049, adv: 0.483439, recon: 0.035994, id: 0.004213] time: 4:03:27.949342 \n",
      "[Epoch 62/200] [Batch 100] [D loss: 0.201927, acc:  69%] [G loss: 2.183115, adv: 0.536800, recon: 0.054825, id: 0.004211] time: 4:05:58.347667 \n",
      "[Epoch 63/200] [Batch 0] [D loss: 0.213396, acc:  65%] [G loss: 1.889431, adv: 0.471685, recon: 0.046826, id: 0.003292] time: 4:07:13.956871 \n",
      "[Epoch 63/200] [Batch 100] [D loss: 0.205521, acc:  70%] [G loss: 2.014291, adv: 0.594903, recon: 0.040671, id: 0.004586] time: 4:09:44.273587 \n",
      "[Epoch 64/200] [Batch 0] [D loss: 0.214929, acc:  69%] [G loss: 1.773586, adv: 0.497483, recon: 0.038557, id: 0.002999] time: 4:11:01.587973 \n",
      "[Epoch 64/200] [Batch 100] [D loss: 0.217404, acc:  64%] [G loss: 2.096342, adv: 0.474365, recon: 0.052827, id: 0.085021] time: 4:13:32.057228 \n",
      "[Epoch 65/200] [Batch 0] [D loss: 0.191864, acc:  71%] [G loss: 1.702499, adv: 0.458500, recon: 0.038737, id: 0.004851] time: 4:14:47.666213 \n",
      "[Epoch 65/200] [Batch 100] [D loss: 0.191158, acc:  69%] [G loss: 1.915247, adv: 0.540485, recon: 0.040965, id: 0.006076] time: 4:17:18.066045 \n",
      "[Epoch 66/200] [Batch 0] [D loss: 0.234657, acc:  59%] [G loss: 1.976894, adv: 0.523202, recon: 0.045203, id: 0.004764] time: 4:18:33.714381 \n",
      "[Epoch 66/200] [Batch 100] [D loss: 0.195299, acc:  71%] [G loss: 1.923879, adv: 0.486818, recon: 0.046862, id: 0.004252] time: 4:21:04.074646 \n",
      "[Epoch 67/200] [Batch 0] [D loss: 0.182912, acc:  73%] [G loss: 1.833612, adv: 0.463808, recon: 0.044572, id: 0.006590] time: 4:22:19.681320 \n",
      "[Epoch 67/200] [Batch 100] [D loss: 0.246499, acc:  57%] [G loss: 1.761714, adv: 0.455823, recon: 0.041948, id: 0.003221] time: 4:24:50.048567 \n",
      "[Epoch 68/200] [Batch 0] [D loss: 0.135058, acc:  83%] [G loss: 2.111323, adv: 0.635130, recon: 0.041446, id: 0.005085] time: 4:26:05.639088 \n",
      "[Epoch 68/200] [Batch 100] [D loss: 0.221460, acc:  65%] [G loss: 1.785719, adv: 0.502040, recon: 0.038687, id: 0.003057] time: 4:28:37.800207 \n",
      "[Epoch 69/200] [Batch 0] [D loss: 0.229909, acc:  62%] [G loss: 1.878462, adv: 0.485772, recon: 0.044398, id: 0.008682] time: 4:29:53.345322 \n",
      "[Epoch 69/200] [Batch 100] [D loss: 0.204537, acc:  69%] [G loss: 1.882354, adv: 0.474122, recon: 0.046137, id: 0.003593] time: 4:32:28.430389 \n",
      "[Epoch 70/200] [Batch 0] [D loss: 0.204050, acc:  68%] [G loss: 2.002790, adv: 0.517671, recon: 0.047958, id: 0.003507] time: 4:33:51.351167 \n",
      "[Epoch 70/200] [Batch 100] [D loss: 0.225117, acc:  61%] [G loss: 1.838163, adv: 0.465310, recon: 0.044911, id: 0.004325] time: 4:36:36.492076 \n",
      "[Epoch 71/200] [Batch 0] [D loss: 0.223352, acc:  62%] [G loss: 1.924537, adv: 0.519627, recon: 0.043795, id: 0.004005] time: 4:37:59.930598 \n",
      "[Epoch 71/200] [Batch 100] [D loss: 0.204831, acc:  65%] [G loss: 1.982778, adv: 0.525822, recon: 0.044138, id: 0.003588] time: 4:40:45.691240 \n",
      "[Epoch 72/200] [Batch 0] [D loss: 0.151630, acc:  80%] [G loss: 2.040832, adv: 0.564965, recon: 0.045062, id: 0.003937] time: 4:42:03.483905 \n",
      "[Epoch 72/200] [Batch 100] [D loss: 0.238661, acc:  59%] [G loss: 1.791230, adv: 0.495434, recon: 0.039338, id: 0.008087] time: 4:44:51.546247 \n",
      "[Epoch 73/200] [Batch 0] [D loss: 0.195479, acc:  67%] [G loss: 1.875294, adv: 0.540653, recon: 0.039014, id: 0.005738] time: 4:46:16.602781 \n",
      "[Epoch 73/200] [Batch 100] [D loss: 0.177978, acc:  74%] [G loss: 2.064823, adv: 0.645574, recon: 0.038128, id: 0.004787] time: 4:48:46.735872 \n",
      "[Epoch 74/200] [Batch 0] [D loss: 0.213444, acc:  64%] [G loss: 1.911980, adv: 0.560953, recon: 0.038774, id: 0.007584] time: 4:50:00.170826 \n",
      "[Epoch 74/200] [Batch 100] [D loss: 0.200960, acc:  71%] [G loss: 2.110507, adv: 0.538312, recon: 0.051115, id: 0.004068] time: 4:52:26.432691 \n",
      "[Epoch 75/200] [Batch 0] [D loss: 0.165843, acc:  75%] [G loss: 2.186996, adv: 0.633266, recon: 0.045519, id: 0.002852] time: 4:53:39.967419 \n",
      "[Epoch 75/200] [Batch 100] [D loss: 0.177406, acc:  75%] [G loss: 2.107500, adv: 0.610276, recon: 0.042679, id: 0.026298] time: 4:56:06.241120 \n",
      "[Epoch 76/200] [Batch 0] [D loss: 0.231994, acc:  60%] [G loss: 2.008009, adv: 0.576631, recon: 0.042054, id: 0.005319] time: 4:57:19.711977 \n",
      "[Epoch 76/200] [Batch 100] [D loss: 0.182444, acc:  72%] [G loss: 2.242611, adv: 0.622190, recon: 0.049363, id: 0.003532] time: 4:59:45.650294 \n",
      "[Epoch 77/200] [Batch 0] [D loss: 0.212901, acc:  66%] [G loss: 2.113305, adv: 0.524159, recon: 0.052577, id: 0.003012] time: 5:00:58.917772 \n",
      "[Epoch 77/200] [Batch 100] [D loss: 0.202803, acc:  68%] [G loss: 2.282377, adv: 0.677336, recon: 0.045547, id: 0.010906] time: 5:03:24.874898 \n",
      "[Epoch 78/200] [Batch 0] [D loss: 0.156461, acc:  82%] [G loss: 2.314528, adv: 0.704424, recon: 0.044547, id: 0.009732] time: 5:04:38.330859 \n",
      "[Epoch 78/200] [Batch 100] [D loss: 0.155511, acc:  76%] [G loss: 2.260914, adv: 0.684856, recon: 0.044146, id: 0.003499] time: 5:07:04.982044 \n",
      "[Epoch 79/200] [Batch 0] [D loss: 0.155021, acc:  78%] [G loss: 2.183442, adv: 0.666856, recon: 0.041627, id: 0.010843] time: 5:08:20.586063 \n",
      "[Epoch 79/200] [Batch 100] [D loss: 0.166165, acc:  74%] [G loss: 2.046179, adv: 0.656245, recon: 0.036175, id: 0.003948] time: 5:10:46.731263 \n",
      "[Epoch 80/200] [Batch 0] [D loss: 0.222915, acc:  64%] [G loss: 2.517541, adv: 0.721222, recon: 0.053102, id: 0.004773] time: 5:12:00.165300 \n",
      "[Epoch 80/200] [Batch 100] [D loss: 0.178941, acc:  75%] [G loss: 2.045846, adv: 0.623603, recon: 0.039332, id: 0.005979] time: 5:14:26.151786 \n",
      "[Epoch 81/200] [Batch 0] [D loss: 0.164589, acc:  76%] [G loss: 2.200500, adv: 0.635197, recon: 0.046026, id: 0.003169] time: 5:15:39.888081 \n",
      "[Epoch 81/200] [Batch 100] [D loss: 0.124843, acc:  86%] [G loss: 2.242319, adv: 0.658613, recon: 0.045758, id: 0.004272] time: 5:18:05.811181 \n",
      "[Epoch 82/200] [Batch 0] [D loss: 0.201365, acc:  69%] [G loss: 2.213075, adv: 0.648472, recon: 0.045221, id: 0.005087] time: 5:19:19.193225 \n",
      "[Epoch 82/200] [Batch 100] [D loss: 0.189010, acc:  70%] [G loss: 2.262658, adv: 0.657458, recon: 0.046923, id: 0.003462] time: 5:21:45.223160 \n",
      "[Epoch 83/200] [Batch 0] [D loss: 0.177054, acc:  72%] [G loss: 2.389148, adv: 0.716367, recon: 0.047427, id: 0.003896] time: 5:22:58.685225 \n",
      "[Epoch 83/200] [Batch 100] [D loss: 0.173537, acc:  76%] [G loss: 2.059169, adv: 0.572478, recon: 0.045264, id: 0.004828] time: 5:25:24.847562 \n",
      "[Epoch 84/200] [Batch 0] [D loss: 0.169743, acc:  75%] [G loss: 2.301631, adv: 0.660169, recon: 0.048576, id: 0.003898] time: 5:26:38.481233 \n",
      "[Epoch 84/200] [Batch 100] [D loss: 0.303916, acc:  42%] [G loss: 2.123725, adv: 0.634240, recon: 0.041994, id: 0.005797] time: 5:29:06.822513 \n",
      "[Epoch 85/200] [Batch 0] [D loss: 0.133612, acc:  82%] [G loss: 2.683903, adv: 0.791495, recon: 0.054504, id: 0.003743] time: 5:30:20.385691 \n",
      "[Epoch 85/200] [Batch 100] [D loss: 0.239170, acc:  64%] [G loss: 2.325101, adv: 0.665822, recon: 0.049063, id: 0.002891] time: 5:32:46.609265 \n",
      "[Epoch 86/200] [Batch 0] [D loss: 0.153051, acc:  81%] [G loss: 2.232394, adv: 0.677774, recon: 0.043212, id: 0.003728] time: 5:34:00.224904 \n",
      "[Epoch 86/200] [Batch 100] [D loss: 0.199653, acc:  67%] [G loss: 2.274956, adv: 0.731267, recon: 0.040115, id: 0.003180] time: 5:36:26.581449 \n",
      "[Epoch 87/200] [Batch 0] [D loss: 0.206868, acc:  68%] [G loss: 2.239355, adv: 0.622015, recon: 0.049186, id: 0.004725] time: 5:37:40.145714 \n",
      "[Epoch 87/200] [Batch 100] [D loss: 0.153510, acc:  75%] [G loss: 2.252631, adv: 0.657489, recon: 0.046405, id: 0.004504] time: 5:40:06.355839 \n",
      "[Epoch 88/200] [Batch 0] [D loss: 0.166718, acc:  75%] [G loss: 2.302161, adv: 0.727478, recon: 0.041907, id: 0.004808] time: 5:41:19.809950 \n",
      "[Epoch 88/200] [Batch 100] [D loss: 0.136070, acc:  80%] [G loss: 2.337131, adv: 0.704010, recon: 0.045810, id: 0.007416] time: 5:43:45.891043 \n",
      "[Epoch 89/200] [Batch 0] [D loss: 0.192294, acc:  70%] [G loss: 2.217749, adv: 0.612848, recon: 0.049115, id: 0.002995] time: 5:44:59.455461 \n",
      "[Epoch 89/200] [Batch 100] [D loss: 0.293556, acc:  49%] [G loss: 2.240838, adv: 0.673687, recon: 0.044262, id: 0.003915] time: 5:47:25.821181 \n",
      "[Epoch 90/200] [Batch 0] [D loss: 0.257936, acc:  58%] [G loss: 1.978235, adv: 0.556796, recon: 0.042711, id: 0.006429] time: 5:48:41.517858 \n",
      "[Epoch 90/200] [Batch 100] [D loss: 0.135002, acc:  85%] [G loss: 2.211078, adv: 0.687844, recon: 0.041376, id: 0.003102] time: 5:51:07.856207 \n",
      "[Epoch 91/200] [Batch 0] [D loss: 0.207803, acc:  67%] [G loss: 3.051362, adv: 1.063488, recon: 0.045804, id: 0.003522] time: 5:52:21.783444 \n",
      "[Epoch 91/200] [Batch 100] [D loss: 0.186142, acc:  74%] [G loss: 2.205300, adv: 0.593902, recon: 0.050247, id: 0.003188] time: 5:54:48.146883 \n",
      "[Epoch 92/200] [Batch 0] [D loss: 0.181957, acc:  74%] [G loss: 2.135573, adv: 0.654990, recon: 0.040887, id: 0.003625] time: 5:56:01.757432 \n",
      "[Epoch 92/200] [Batch 100] [D loss: 0.192356, acc:  73%] [G loss: 2.064439, adv: 0.578633, recon: 0.044748, id: 0.003806] time: 5:58:28.067608 \n",
      "[Epoch 93/200] [Batch 0] [D loss: 0.194023, acc:  70%] [G loss: 2.428891, adv: 0.705035, recon: 0.050505, id: 0.003761] time: 5:59:41.643865 \n",
      "[Epoch 93/200] [Batch 100] [D loss: 0.153508, acc:  77%] [G loss: 2.323737, adv: 0.678718, recon: 0.047773, id: 0.005089] time: 6:02:08.012628 \n",
      "[Epoch 94/200] [Batch 0] [D loss: 0.161998, acc:  80%] [G loss: 1.934014, adv: 0.596350, recon: 0.036585, id: 0.004516] time: 6:03:21.631697 \n",
      "[Epoch 94/200] [Batch 100] [D loss: 0.181849, acc:  72%] [G loss: 1.973166, adv: 0.624833, recon: 0.035744, id: 0.004094] time: 6:05:47.979957 \n",
      "[Epoch 95/200] [Batch 0] [D loss: 0.195863, acc:  70%] [G loss: 2.052140, adv: 0.585127, recon: 0.043637, id: 0.002984] time: 6:07:01.625375 \n",
      "[Epoch 95/200] [Batch 100] [D loss: 0.134515, acc:  84%] [G loss: 2.173788, adv: 0.673850, recon: 0.040856, id: 0.002900] time: 6:09:30.268424 \n",
      "[Epoch 96/200] [Batch 0] [D loss: 0.240598, acc:  57%] [G loss: 1.991665, adv: 0.609557, recon: 0.038060, id: 0.004968] time: 6:10:43.708424 \n",
      "[Epoch 96/200] [Batch 100] [D loss: 0.195810, acc:  71%] [G loss: 2.189227, adv: 0.653944, recon: 0.043357, id: 0.005806] time: 6:13:10.030535 \n",
      "[Epoch 97/200] [Batch 0] [D loss: 0.213660, acc:  62%] [G loss: 2.479112, adv: 0.733868, recon: 0.049899, id: 0.005309] time: 6:14:23.645384 \n",
      "[Epoch 97/200] [Batch 100] [D loss: 0.191051, acc:  71%] [G loss: 2.234153, adv: 0.702246, recon: 0.040828, id: 0.008670] time: 6:16:50.029577 \n",
      "[Epoch 98/200] [Batch 0] [D loss: 0.165714, acc:  77%] [G loss: 2.304453, adv: 0.649269, recon: 0.048622, id: 0.028098] time: 6:18:03.607823 \n",
      "[Epoch 98/200] [Batch 100] [D loss: 0.162372, acc:  75%] [G loss: 2.236058, adv: 0.716962, recon: 0.039595, id: 0.004817] time: 6:20:29.959269 \n",
      "[Epoch 99/200] [Batch 0] [D loss: 0.265115, acc:  49%] [G loss: 2.081474, adv: 0.566160, recon: 0.046928, id: 0.004940] time: 6:21:43.589842 \n",
      "[Epoch 99/200] [Batch 100] [D loss: 0.235909, acc:  60%] [G loss: 2.199802, adv: 0.572594, recon: 0.051961, id: 0.010375] time: 6:24:10.024829 \n",
      "[Epoch 100/200] [Batch 0] [D loss: 0.234236, acc:  64%] [G loss: 2.458708, adv: 0.704042, recon: 0.051801, id: 0.004960] time: 6:25:23.600103 \n",
      "[Epoch 100/200] [Batch 100] [D loss: 0.178076, acc:  71%] [G loss: 2.093637, adv: 0.630839, recon: 0.041121, id: 0.004670] time: 6:27:50.240595 \n",
      "[Epoch 101/200] [Batch 0] [D loss: 0.188516, acc:  74%] [G loss: 2.483885, adv: 0.749832, recon: 0.047916, id: 0.020285] time: 6:29:04.083814 \n",
      "[Epoch 101/200] [Batch 100] [D loss: 0.167101, acc:  77%] [G loss: 2.291415, adv: 0.670712, recon: 0.046908, id: 0.007874] time: 6:31:32.614828 \n",
      "[Epoch 102/200] [Batch 0] [D loss: 0.183532, acc:  74%] [G loss: 2.145986, adv: 0.647757, recon: 0.042021, id: 0.004681] time: 6:32:45.887472 \n",
      "[Epoch 102/200] [Batch 100] [D loss: 0.282435, acc:  55%] [G loss: 2.241354, adv: 0.620871, recon: 0.049209, id: 0.007695] time: 6:35:11.938748 \n",
      "[Epoch 103/200] [Batch 0] [D loss: 0.161242, acc:  77%] [G loss: 2.282856, adv: 0.705229, recon: 0.043053, id: 0.004928] time: 6:36:25.346614 \n",
      "[Epoch 103/200] [Batch 100] [D loss: 0.214754, acc:  66%] [G loss: 2.302139, adv: 0.647645, recon: 0.049873, id: 0.003835] time: 6:38:51.310788 \n",
      "[Epoch 104/200] [Batch 0] [D loss: 0.140115, acc:  82%] [G loss: 2.134741, adv: 0.646711, recon: 0.041708, id: 0.003460] time: 6:40:04.705394 \n",
      "[Epoch 104/200] [Batch 100] [D loss: 0.164037, acc:  74%] [G loss: 2.612870, adv: 0.777300, recon: 0.052368, id: 0.004449] time: 6:42:30.605501 \n",
      "[Epoch 105/200] [Batch 0] [D loss: 0.171583, acc:  78%] [G loss: 1.922539, adv: 0.574115, recon: 0.038384, id: 0.002926] time: 6:43:43.899713 \n",
      "[Epoch 105/200] [Batch 100] [D loss: 0.188645, acc:  70%] [G loss: 2.347872, adv: 0.750918, recon: 0.041838, id: 0.004358] time: 6:46:09.558111 \n",
      "[Epoch 106/200] [Batch 0] [D loss: 0.239200, acc:  58%] [G loss: 2.264086, adv: 0.654029, recon: 0.047231, id: 0.004448] time: 6:47:22.771461 \n",
      "[Epoch 106/200] [Batch 100] [D loss: 0.169852, acc:  75%] [G loss: 2.294129, adv: 0.632842, recon: 0.050975, id: 0.003538] time: 6:49:48.274189 \n",
      "[Epoch 107/200] [Batch 0] [D loss: 0.138108, acc:  82%] [G loss: 2.491828, adv: 0.777072, recon: 0.045539, id: 0.020752] time: 6:51:01.457199 \n",
      "[Epoch 107/200] [Batch 100] [D loss: 0.211663, acc:  64%] [G loss: 2.645206, adv: 0.706248, recon: 0.061146, id: 0.003992] time: 6:53:27.095455 \n",
      "[Epoch 108/200] [Batch 0] [D loss: 0.178671, acc:  75%] [G loss: 1.938957, adv: 0.601175, recon: 0.036333, id: 0.005876] time: 6:54:42.606898 \n",
      "[Epoch 108/200] [Batch 100] [D loss: 0.196333, acc:  69%] [G loss: 2.499948, adv: 0.771433, recon: 0.047282, id: 0.003700] time: 6:57:08.108849 \n",
      "[Epoch 109/200] [Batch 0] [D loss: 0.181318, acc:  73%] [G loss: 2.173909, adv: 0.663777, recon: 0.041916, id: 0.002858] time: 6:58:21.309433 \n",
      "[Epoch 109/200] [Batch 100] [D loss: 0.243254, acc:  58%] [G loss: 2.742703, adv: 0.778961, recon: 0.058744, id: 0.004012] time: 7:00:46.802949 \n",
      "[Epoch 110/200] [Batch 0] [D loss: 0.200120, acc:  68%] [G loss: 2.379136, adv: 0.724239, recon: 0.046084, id: 0.004442] time: 7:02:00.033767 \n",
      "[Epoch 110/200] [Batch 100] [D loss: 0.275033, acc:  66%] [G loss: 2.225025, adv: 0.621576, recon: 0.048587, id: 0.004305] time: 7:04:25.800017 \n",
      "[Epoch 111/200] [Batch 0] [D loss: 0.192778, acc:  71%] [G loss: 2.213062, adv: 0.647702, recon: 0.045474, id: 0.003319] time: 7:05:39.435063 \n",
      "[Epoch 111/200] [Batch 100] [D loss: 0.210707, acc:  67%] [G loss: 2.402033, adv: 0.697121, recon: 0.049754, id: 0.007125] time: 7:08:05.337158 \n",
      "[Epoch 112/200] [Batch 0] [D loss: 0.198513, acc:  73%] [G loss: 1.984120, adv: 0.626024, recon: 0.036225, id: 0.003349] time: 7:09:18.658795 \n",
      "[Epoch 112/200] [Batch 100] [D loss: 0.183643, acc:  72%] [G loss: 2.408380, adv: 0.763749, recon: 0.043637, id: 0.002843] time: 7:11:44.489178 \n",
      "[Epoch 113/200] [Batch 0] [D loss: 0.204008, acc:  68%] [G loss: 2.289743, adv: 0.672995, recon: 0.045004, id: 0.035094] time: 7:12:57.830620 \n",
      "[Epoch 113/200] [Batch 100] [D loss: 0.237993, acc:  58%] [G loss: 2.115072, adv: 0.533335, recon: 0.051994, id: 0.003135] time: 7:15:23.732195 \n",
      "[Epoch 114/200] [Batch 0] [D loss: 0.203645, acc:  70%] [G loss: 2.361117, adv: 0.710384, recon: 0.046264, id: 0.005381] time: 7:16:37.231407 \n",
      "[Epoch 114/200] [Batch 100] [D loss: 0.172597, acc:  74%] [G loss: 2.335090, adv: 0.732883, recon: 0.043083, id: 0.003097] time: 7:19:05.599741 \n",
      "[Epoch 115/200] [Batch 0] [D loss: 0.217308, acc:  65%] [G loss: 2.163697, adv: 0.660555, recon: 0.041754, id: 0.002636] time: 7:20:19.029211 \n",
      "[Epoch 115/200] [Batch 100] [D loss: 0.257026, acc:  63%] [G loss: 2.084767, adv: 0.611199, recon: 0.042671, id: 0.002691] time: 7:22:45.009667 \n",
      "[Epoch 116/200] [Batch 0] [D loss: 0.154138, acc:  80%] [G loss: 2.313534, adv: 0.734703, recon: 0.041845, id: 0.002928] time: 7:23:58.450477 \n",
      "[Epoch 116/200] [Batch 100] [D loss: 0.221477, acc:  63%] [G loss: 2.102391, adv: 0.627411, recon: 0.041964, id: 0.003520] time: 7:26:24.422175 \n",
      "[Epoch 117/200] [Batch 0] [D loss: 0.192263, acc:  72%] [G loss: 2.316769, adv: 0.695331, recon: 0.045819, id: 0.003624] time: 7:27:37.820660 \n",
      "[Epoch 117/200] [Batch 100] [D loss: 0.226181, acc:  64%] [G loss: 2.280753, adv: 0.712180, recon: 0.041687, id: 0.003549] time: 7:30:03.687529 \n",
      "[Epoch 118/200] [Batch 0] [D loss: 0.216247, acc:  63%] [G loss: 2.052251, adv: 0.636934, recon: 0.038171, id: 0.002209] time: 7:31:17.096827 \n",
      "[Epoch 118/200] [Batch 100] [D loss: 0.125918, acc:  85%] [G loss: 3.136322, adv: 1.113271, recon: 0.044953, id: 0.003768] time: 7:33:43.058108 \n",
      "[Epoch 119/200] [Batch 0] [D loss: 0.200215, acc:  67%] [G loss: 2.255514, adv: 0.674808, recon: 0.044754, id: 0.003217] time: 7:34:56.435888 \n",
      "[Epoch 119/200] [Batch 100] [D loss: 0.143016, acc:  80%] [G loss: 2.532911, adv: 0.769698, recon: 0.049239, id: 0.004392] time: 7:37:22.375947 \n",
      "[Epoch 120/200] [Batch 0] [D loss: 0.269246, acc:  55%] [G loss: 2.254236, adv: 0.666682, recon: 0.045570, id: 0.003806] time: 7:38:35.758044 \n",
      "[Epoch 120/200] [Batch 100] [D loss: 0.196328, acc:  67%] [G loss: 2.503551, adv: 0.779882, recon: 0.046311, id: 0.011111] time: 7:41:01.748889 \n",
      "[Epoch 121/200] [Batch 0] [D loss: 0.136001, acc:  82%] [G loss: 2.280740, adv: 0.669056, recon: 0.046084, id: 0.015510] time: 7:42:15.503421 \n",
      "[Epoch 121/200] [Batch 100] [D loss: 0.175361, acc:  74%] [G loss: 2.498402, adv: 0.788209, recon: 0.045700, id: 0.004092] time: 7:44:43.955301 \n",
      "[Epoch 122/200] [Batch 0] [D loss: 0.166559, acc:  77%] [G loss: 2.198876, adv: 0.665291, recon: 0.038458, id: 0.011661] time: 7:45:57.412690 \n",
      "[Epoch 122/200] [Batch 100] [D loss: 0.234097, acc:  63%] [G loss: 2.478735, adv: 0.776803, recon: 0.045475, id: 0.002953] time: 7:48:23.497561 \n",
      "[Epoch 123/200] [Batch 0] [D loss: 0.170350, acc:  75%] [G loss: 2.539722, adv: 0.798839, recon: 0.046529, id: 0.006669] time: 7:49:36.949539 \n",
      "[Epoch 123/200] [Batch 100] [D loss: 0.146040, acc:  84%] [G loss: 2.395184, adv: 0.736706, recon: 0.045609, id: 0.004534] time: 7:52:02.987435 \n",
      "[Epoch 124/200] [Batch 0] [D loss: 0.161859, acc:  76%] [G loss: 2.497595, adv: 0.816790, recon: 0.042819, id: 0.002606] time: 7:53:16.448743 \n",
      "[Epoch 124/200] [Batch 100] [D loss: 0.129181, acc:  84%] [G loss: 2.665661, adv: 0.843673, recon: 0.048270, id: 0.006560] time: 7:55:42.504431 \n",
      "[Epoch 125/200] [Batch 0] [D loss: 0.132288, acc:  84%] [G loss: 2.614787, adv: 0.828999, recon: 0.047222, id: 0.005884] time: 7:56:55.960830 \n",
      "[Epoch 125/200] [Batch 100] [D loss: 0.169333, acc:  73%] [G loss: 2.404294, adv: 0.715449, recon: 0.048161, id: 0.005209] time: 7:59:21.997222 \n",
      "[Epoch 126/200] [Batch 0] [D loss: 0.219888, acc:  66%] [G loss: 2.410971, adv: 0.785635, recon: 0.041523, id: 0.003875] time: 8:00:35.416418 \n",
      "[Epoch 126/200] [Batch 100] [D loss: 0.274318, acc:  53%] [G loss: 1.766101, adv: 0.506033, recon: 0.037237, id: 0.005557] time: 8:03:01.367324 \n",
      "[Epoch 127/200] [Batch 0] [D loss: 0.184394, acc:  73%] [G loss: 2.428607, adv: 0.808565, recon: 0.040158, id: 0.004476] time: 8:04:14.812537 \n",
      "[Epoch 127/200] [Batch 100] [D loss: 0.195729, acc:  66%] [G loss: 2.485120, adv: 0.776691, recon: 0.045981, id: 0.006700] time: 8:06:41.088288 \n",
      "[Epoch 128/200] [Batch 0] [D loss: 0.167354, acc:  76%] [G loss: 2.671212, adv: 0.803395, recon: 0.052658, id: 0.003826] time: 8:07:54.559197 \n",
      "[Epoch 128/200] [Batch 100] [D loss: 0.156756, acc:  75%] [G loss: 2.389064, adv: 0.705066, recon: 0.048423, id: 0.003702] time: 8:10:23.129184 \n",
      "[Epoch 129/200] [Batch 0] [D loss: 0.216137, acc:  65%] [G loss: 2.114605, adv: 0.600481, recon: 0.043754, id: 0.034948] time: 8:11:36.627591 \n",
      "[Epoch 129/200] [Batch 100] [D loss: 0.139949, acc:  79%] [G loss: 2.529830, adv: 0.818660, recon: 0.044168, id: 0.003569] time: 8:14:02.711051 \n",
      "[Epoch 130/200] [Batch 0] [D loss: 0.125191, acc:  87%] [G loss: 2.462564, adv: 0.765780, recon: 0.046100, id: 0.003286] time: 8:15:16.177826 \n",
      "[Epoch 130/200] [Batch 100] [D loss: 0.142445, acc:  78%] [G loss: 2.234795, adv: 0.717473, recon: 0.039613, id: 0.004024] time: 8:17:42.257912 \n",
      "[Epoch 131/200] [Batch 0] [D loss: 0.222752, acc:  65%] [G loss: 2.327588, adv: 0.739488, recon: 0.040974, id: 0.025640] time: 8:18:56.052643 \n",
      "[Epoch 131/200] [Batch 100] [D loss: 0.218742, acc:  65%] [G loss: 2.159241, adv: 0.671501, recon: 0.040264, id: 0.006925] time: 8:21:22.125209 \n",
      "[Epoch 132/200] [Batch 0] [D loss: 0.161265, acc:  78%] [G loss: 2.238912, adv: 0.669794, recon: 0.044118, id: 0.010097] time: 8:22:35.651067 \n",
      "[Epoch 132/200] [Batch 100] [D loss: 0.154793, acc:  79%] [G loss: 2.418273, adv: 0.668295, recon: 0.053645, id: 0.003935] time: 8:25:01.727040 \n",
      "[Epoch 133/200] [Batch 0] [D loss: 0.092368, acc:  93%] [G loss: 2.510467, adv: 0.754854, recon: 0.049566, id: 0.004252] time: 8:26:15.212914 \n",
      "[Epoch 133/200] [Batch 100] [D loss: 0.220742, acc:  69%] [G loss: 1.943295, adv: 0.554563, recon: 0.041300, id: 0.003456] time: 8:28:41.144194 \n",
      "[Epoch 134/200] [Batch 0] [D loss: 0.165892, acc:  77%] [G loss: 2.485474, adv: 0.796416, recon: 0.044120, id: 0.005001] time: 8:29:54.466793 \n",
      "[Epoch 134/200] [Batch 100] [D loss: 0.197010, acc:  72%] [G loss: 2.402524, adv: 0.756658, recon: 0.044073, id: 0.002622] time: 8:32:20.430852 \n",
      "[Epoch 135/200] [Batch 0] [D loss: 0.178140, acc:  72%] [G loss: 2.464713, adv: 0.765806, recon: 0.046250, id: 0.003500] time: 8:33:33.833185 \n",
      "[Epoch 135/200] [Batch 100] [D loss: 0.131455, acc:  85%] [G loss: 2.259388, adv: 0.702483, recon: 0.042342, id: 0.003595] time: 8:36:02.483700 \n",
      "[Epoch 136/200] [Batch 0] [D loss: 0.167560, acc:  78%] [G loss: 2.441244, adv: 0.714950, recon: 0.050195, id: 0.003036] time: 8:37:15.909684 \n",
      "[Epoch 136/200] [Batch 100] [D loss: 0.155144, acc:  78%] [G loss: 2.216917, adv: 0.705054, recon: 0.039960, id: 0.003657] time: 8:39:41.939623 \n",
      "[Epoch 137/200] [Batch 0] [D loss: 0.148873, acc:  81%] [G loss: 2.637538, adv: 0.888126, recon: 0.042734, id: 0.002759] time: 8:40:55.352114 \n",
      "[Epoch 137/200] [Batch 100] [D loss: 0.156346, acc:  79%] [G loss: 2.505431, adv: 0.822875, recon: 0.042638, id: 0.003141] time: 8:43:21.271392 \n",
      "[Epoch 138/200] [Batch 0] [D loss: 0.195905, acc:  68%] [G loss: 2.247859, adv: 0.712262, recon: 0.040797, id: 0.003006] time: 8:44:34.737546 \n",
      "[Epoch 138/200] [Batch 100] [D loss: 0.203582, acc:  67%] [G loss: 2.312349, adv: 0.738447, recon: 0.041339, id: 0.004293] time: 8:47:00.946219 \n",
      "[Epoch 139/200] [Batch 0] [D loss: 0.225092, acc:  61%] [G loss: 2.364655, adv: 0.735865, recon: 0.044186, id: 0.005016] time: 8:48:14.509989 \n",
      "[Epoch 139/200] [Batch 100] [D loss: 0.191570, acc:  70%] [G loss: 2.350603, adv: 0.691139, recon: 0.046927, id: 0.003990] time: 8:50:40.725495 \n",
      "[Epoch 140/200] [Batch 0] [D loss: 0.201949, acc:  71%] [G loss: 2.222016, adv: 0.655377, recon: 0.044861, id: 0.003924] time: 8:51:54.241446 \n",
      "[Epoch 140/200] [Batch 100] [D loss: 0.198932, acc:  67%] [G loss: 2.124337, adv: 0.654180, recon: 0.040265, id: 0.003709] time: 8:54:20.433301 \n",
      "[Epoch 141/200] [Batch 0] [D loss: 0.144268, acc:  81%] [G loss: 2.359711, adv: 0.694778, recon: 0.047851, id: 0.003854] time: 8:55:34.254042 \n",
      "[Epoch 141/200] [Batch 100] [D loss: 0.223314, acc:  60%] [G loss: 2.242291, adv: 0.722649, recon: 0.039436, id: 0.002725] time: 8:58:00.667939 \n",
      "[Epoch 142/200] [Batch 0] [D loss: 0.192818, acc:  69%] [G loss: 2.592652, adv: 0.837215, recon: 0.045413, id: 0.003790] time: 8:59:14.219172 \n",
      "[Epoch 142/200] [Batch 100] [D loss: 0.225391, acc:  64%] [G loss: 2.181810, adv: 0.736278, recon: 0.035052, id: 0.003253] time: 9:01:40.554649 \n",
      "[Epoch 143/200] [Batch 0] [D loss: 0.126635, acc:  82%] [G loss: 2.423869, adv: 0.810678, recon: 0.039635, id: 0.004562] time: 9:02:56.971994 \n",
      "[Epoch 143/200] [Batch 100] [D loss: 0.150749, acc:  79%] [G loss: 2.474077, adv: 0.773292, recon: 0.045762, id: 0.004227] time: 9:05:23.205920 \n",
      "[Epoch 144/200] [Batch 0] [D loss: 0.193716, acc:  71%] [G loss: 2.432306, adv: 0.802678, recon: 0.040949, id: 0.002834] time: 9:06:36.852664 \n",
      "[Epoch 144/200] [Batch 100] [D loss: 0.231413, acc:  60%] [G loss: 2.338242, adv: 0.733491, recon: 0.043136, id: 0.004090] time: 9:09:03.149939 \n",
      "[Epoch 145/200] [Batch 0] [D loss: 0.123675, acc:  86%] [G loss: 2.317431, adv: 0.725962, recon: 0.042895, id: 0.003193] time: 9:10:16.746058 \n",
      "[Epoch 145/200] [Batch 100] [D loss: 0.222584, acc:  66%] [G loss: 2.414358, adv: 0.685800, recon: 0.051745, id: 0.003303] time: 9:12:43.047565 \n",
      "[Epoch 146/200] [Batch 0] [D loss: 0.192447, acc:  72%] [G loss: 2.480172, adv: 0.797352, recon: 0.043799, id: 0.003142] time: 9:13:56.672488 \n",
      "[Epoch 146/200] [Batch 100] [D loss: 0.199771, acc:  67%] [G loss: 2.666530, adv: 0.857837, recon: 0.047079, id: 0.003602] time: 9:16:23.048794 \n",
      "[Epoch 147/200] [Batch 0] [D loss: 0.148973, acc:  83%] [G loss: 2.403276, adv: 0.752001, recon: 0.044346, id: 0.002900] time: 9:17:36.603191 \n",
      "[Epoch 147/200] [Batch 100] [D loss: 0.164321, acc:  75%] [G loss: 2.510834, adv: 0.830908, recon: 0.042067, id: 0.003547] time: 9:20:02.852945 \n",
      "[Epoch 148/200] [Batch 0] [D loss: 0.141773, acc:  80%] [G loss: 2.563254, adv: 0.822928, recon: 0.045279, id: 0.003324] time: 9:21:16.415423 \n",
      "[Epoch 148/200] [Batch 100] [D loss: 0.223042, acc:  68%] [G loss: 2.331500, adv: 0.763884, recon: 0.039685, id: 0.002551] time: 9:23:42.739733 \n",
      "[Epoch 149/200] [Batch 0] [D loss: 0.155490, acc:  81%] [G loss: 2.099770, adv: 0.669277, recon: 0.037697, id: 0.002382] time: 9:24:56.291978 \n",
      "[Epoch 149/200] [Batch 100] [D loss: 0.141754, acc:  82%] [G loss: 2.553218, adv: 0.783387, recon: 0.048845, id: 0.004162] time: 9:27:22.640039 \n",
      "[Epoch 150/200] [Batch 0] [D loss: 0.162748, acc:  77%] [G loss: 2.392006, adv: 0.725806, recon: 0.046608, id: 0.002913] time: 9:28:36.207202 \n",
      "[Epoch 150/200] [Batch 100] [D loss: 0.199098, acc:  73%] [G loss: 2.259244, adv: 0.717895, recon: 0.040781, id: 0.002925] time: 9:31:02.580878 \n",
      "[Epoch 151/200] [Batch 0] [D loss: 0.155536, acc:  79%] [G loss: 2.220371, adv: 0.696622, recon: 0.040951, id: 0.002286] time: 9:32:19.433633 \n",
      "[Epoch 151/200] [Batch 100] [D loss: 0.154328, acc:  77%] [G loss: 2.410774, adv: 0.796179, recon: 0.040459, id: 0.003292] time: 9:34:45.821699 \n",
      "[Epoch 152/200] [Batch 0] [D loss: 0.205687, acc:  68%] [G loss: 2.362771, adv: 0.746881, recon: 0.042464, id: 0.015187] time: 9:35:59.503058 \n",
      "[Epoch 152/200] [Batch 100] [D loss: 0.217066, acc:  65%] [G loss: 2.261924, adv: 0.726689, recon: 0.040066, id: 0.003490] time: 9:38:25.642199 \n",
      "[Epoch 153/200] [Batch 0] [D loss: 0.212923, acc:  66%] [G loss: 2.302675, adv: 0.732242, recon: 0.041576, id: 0.003055] time: 9:39:39.057314 \n",
      "[Epoch 153/200] [Batch 100] [D loss: 0.171522, acc:  79%] [G loss: 2.643816, adv: 0.923097, recon: 0.039537, id: 0.002236] time: 9:42:05.079445 \n",
      "[Epoch 154/200] [Batch 0] [D loss: 0.201854, acc:  68%] [G loss: 2.559376, adv: 0.802436, recon: 0.047086, id: 0.003186] time: 9:43:18.454659 \n",
      "[Epoch 154/200] [Batch 100] [D loss: 0.191999, acc:  72%] [G loss: 2.095819, adv: 0.622789, recon: 0.042181, id: 0.002515] time: 9:45:44.235121 \n",
      "[Epoch 155/200] [Batch 0] [D loss: 0.190904, acc:  70%] [G loss: 2.155033, adv: 0.693996, recon: 0.037996, id: 0.003195] time: 9:46:57.553245 \n",
      "[Epoch 155/200] [Batch 100] [D loss: 0.223410, acc:  67%] [G loss: 2.571173, adv: 0.747111, recon: 0.050456, id: 0.059895] time: 9:49:23.316682 \n",
      "[Epoch 156/200] [Batch 0] [D loss: 0.195589, acc:  70%] [G loss: 2.458712, adv: 0.747957, recon: 0.047545, id: 0.005749] time: 9:50:36.629847 \n",
      "[Epoch 156/200] [Batch 100] [D loss: 0.183787, acc:  71%] [G loss: 2.324020, adv: 0.746337, recon: 0.041196, id: 0.002836] time: 9:53:02.312466 \n",
      "[Epoch 157/200] [Batch 0] [D loss: 0.177026, acc:  73%] [G loss: 2.476323, adv: 0.803198, recon: 0.043023, id: 0.002706] time: 9:54:15.571156 \n",
      "[Epoch 157/200] [Batch 100] [D loss: 0.160296, acc:  81%] [G loss: 2.629558, adv: 0.908744, recon: 0.040288, id: 0.002669] time: 9:56:41.229280 \n",
      "[Epoch 158/200] [Batch 0] [D loss: 0.201165, acc:  64%] [G loss: 2.301229, adv: 0.788798, recon: 0.035775, id: 0.002876] time: 9:57:54.522284 \n",
      "[Epoch 158/200] [Batch 100] [D loss: 0.206714, acc:  67%] [G loss: 2.147672, adv: 0.683729, recon: 0.038689, id: 0.002554] time: 10:00:20.308611 \n",
      "[Epoch 159/200] [Batch 0] [D loss: 0.167096, acc:  76%] [G loss: 2.630122, adv: 0.847981, recon: 0.046229, id: 0.003908] time: 10:01:36.628764 \n",
      "[Epoch 159/200] [Batch 100] [D loss: 0.193394, acc:  69%] [G loss: 2.099443, adv: 0.654019, recon: 0.039144, id: 0.003474] time: 10:04:02.608424 \n",
      "[Epoch 160/200] [Batch 0] [D loss: 0.152984, acc:  78%] [G loss: 2.445985, adv: 0.754388, recon: 0.046383, id: 0.002961] time: 10:05:16.066021 \n",
      "[Epoch 160/200] [Batch 100] [D loss: 0.177994, acc:  71%] [G loss: 2.585095, adv: 0.863122, recon: 0.042595, id: 0.003092] time: 10:07:42.153343 \n",
      "[Epoch 161/200] [Batch 0] [D loss: 0.157294, acc:  75%] [G loss: 2.222733, adv: 0.740341, recon: 0.036741, id: 0.002843] time: 10:08:55.893224 \n",
      "[Epoch 161/200] [Batch 100] [D loss: 0.193869, acc:  74%] [G loss: 2.627119, adv: 0.857799, recon: 0.045112, id: 0.004520] time: 10:11:21.973764 \n",
      "[Epoch 162/200] [Batch 0] [D loss: 0.162975, acc:  74%] [G loss: 2.285211, adv: 0.712059, recon: 0.042585, id: 0.003795] time: 10:12:35.430870 \n",
      "[Epoch 162/200] [Batch 100] [D loss: 0.203217, acc:  68%] [G loss: 2.179267, adv: 0.729122, recon: 0.035751, id: 0.002352] time: 10:15:01.451541 \n",
      "[Epoch 163/200] [Batch 0] [D loss: 0.228119, acc:  67%] [G loss: 2.212162, adv: 0.694415, recon: 0.040739, id: 0.002618] time: 10:16:14.881308 \n",
      "[Epoch 163/200] [Batch 100] [D loss: 0.184265, acc:  74%] [G loss: 2.278047, adv: 0.671834, recon: 0.045736, id: 0.014938] time: 10:18:40.864392 \n",
      "[Epoch 164/200] [Batch 0] [D loss: 0.147317, acc:  81%] [G loss: 2.612552, adv: 0.838743, recon: 0.046379, id: 0.003953] time: 10:19:54.231157 \n",
      "[Epoch 164/200] [Batch 100] [D loss: 0.243402, acc:  57%] [G loss: 2.282612, adv: 0.762019, recon: 0.037599, id: 0.002739] time: 10:22:20.144059 \n",
      "[Epoch 165/200] [Batch 0] [D loss: 0.146640, acc:  80%] [G loss: 2.423162, adv: 0.762442, recon: 0.044513, id: 0.003402] time: 10:23:33.529909 \n",
      "[Epoch 165/200] [Batch 100] [D loss: 0.202604, acc:  70%] [G loss: 2.394177, adv: 0.788193, recon: 0.040337, id: 0.003733] time: 10:25:59.489231 \n",
      "[Epoch 166/200] [Batch 0] [D loss: 0.181263, acc:  73%] [G loss: 2.454407, adv: 0.759010, recon: 0.046330, id: 0.003541] time: 10:27:12.941416 \n",
      "[Epoch 166/200] [Batch 100] [D loss: 0.165018, acc:  78%] [G loss: 2.566756, adv: 0.766717, recon: 0.051112, id: 0.004523] time: 10:29:39.049988 \n",
      "[Epoch 167/200] [Batch 0] [D loss: 0.156313, acc:  79%] [G loss: 2.449665, adv: 0.790930, recon: 0.042934, id: 0.002317] time: 10:30:52.573985 \n",
      "[Epoch 167/200] [Batch 100] [D loss: 0.231132, acc:  61%] [G loss: 2.095351, adv: 0.662124, recon: 0.038222, id: 0.002491] time: 10:33:21.745697 \n",
      "[Epoch 168/200] [Batch 0] [D loss: 0.162909, acc:  75%] [G loss: 2.540713, adv: 0.802059, recon: 0.046448, id: 0.003946] time: 10:34:35.206425 \n",
      "[Epoch 168/200] [Batch 100] [D loss: 0.172029, acc:  78%] [G loss: 2.223969, adv: 0.694769, recon: 0.041290, id: 0.004596] time: 10:37:01.347406 \n",
      "[Epoch 169/200] [Batch 0] [D loss: 0.140970, acc:  84%] [G loss: 2.500491, adv: 0.795657, recon: 0.045024, id: 0.003448] time: 10:38:14.906635 \n",
      "[Epoch 169/200] [Batch 100] [D loss: 0.209188, acc:  68%] [G loss: 2.304384, adv: 0.700825, recon: 0.044718, id: 0.003938] time: 10:40:41.197172 \n",
      "[Epoch 170/200] [Batch 0] [D loss: 0.182493, acc:  74%] [G loss: 2.286260, adv: 0.774775, recon: 0.036454, id: 0.004245] time: 10:41:54.792633 \n",
      "[Epoch 170/200] [Batch 100] [D loss: 0.166029, acc:  76%] [G loss: 2.184525, adv: 0.674761, recon: 0.041226, id: 0.004224] time: 10:44:21.130731 \n",
      "[Epoch 171/200] [Batch 0] [D loss: 0.219306, acc:  66%] [G loss: 2.111532, adv: 0.656747, recon: 0.039610, id: 0.002494] time: 10:45:35.028215 \n",
      "[Epoch 171/200] [Batch 100] [D loss: 0.204375, acc:  66%] [G loss: 2.523227, adv: 0.837667, recon: 0.041903, id: 0.002454] time: 10:48:01.372169 \n",
      "[Epoch 172/200] [Batch 0] [D loss: 0.160991, acc:  76%] [G loss: 2.336145, adv: 0.775103, recon: 0.038934, id: 0.002709] time: 10:49:14.941118 \n",
      "[Epoch 172/200] [Batch 100] [D loss: 0.190863, acc:  71%] [G loss: 2.360583, adv: 0.720941, recon: 0.045540, id: 0.003012] time: 10:51:41.350493 \n",
      "[Epoch 173/200] [Batch 0] [D loss: 0.181333, acc:  72%] [G loss: 2.594704, adv: 0.840375, recon: 0.045249, id: 0.002448] time: 10:52:54.897043 \n",
      "[Epoch 173/200] [Batch 100] [D loss: 0.147034, acc:  81%] [G loss: 2.282987, adv: 0.741063, recon: 0.039668, id: 0.003537] time: 10:55:21.261470 \n",
      "[Epoch 174/200] [Batch 0] [D loss: 0.144080, acc:  81%] [G loss: 2.580924, adv: 0.802350, recon: 0.048378, id: 0.003135] time: 10:56:34.912063 \n",
      "[Epoch 174/200] [Batch 100] [D loss: 0.212686, acc:  65%] [G loss: 2.412033, adv: 0.737430, recon: 0.046549, id: 0.002422] time: 10:59:01.325589 \n",
      "[Epoch 175/200] [Batch 0] [D loss: 0.163747, acc:  78%] [G loss: 2.272902, adv: 0.737181, recon: 0.039562, id: 0.002457] time: 11:00:14.949168 \n",
      "[Epoch 175/200] [Batch 100] [D loss: 0.191842, acc:  73%] [G loss: 2.373563, adv: 0.744077, recon: 0.043923, id: 0.003303] time: 11:02:41.339863 \n",
      "[Epoch 176/200] [Batch 0] [D loss: 0.190769, acc:  72%] [G loss: 2.649313, adv: 0.894872, recon: 0.042537, id: 0.003735] time: 11:03:58.366685 \n",
      "[Epoch 176/200] [Batch 100] [D loss: 0.122203, acc:  87%] [G loss: 2.808603, adv: 0.939300, recon: 0.046008, id: 0.003637] time: 11:06:24.722020 \n",
      "[Epoch 177/200] [Batch 0] [D loss: 0.175881, acc:  75%] [G loss: 2.375839, adv: 0.749683, recon: 0.043408, id: 0.003016] time: 11:07:38.349021 \n",
      "[Epoch 177/200] [Batch 100] [D loss: 0.169532, acc:  78%] [G loss: 2.157739, adv: 0.667233, recon: 0.040805, id: 0.002744] time: 11:10:04.696383 \n",
      "[Epoch 178/200] [Batch 0] [D loss: 0.148987, acc:  79%] [G loss: 2.499966, adv: 0.742516, recon: 0.045729, id: 0.002805] time: 11:11:18.326634 \n",
      "[Epoch 178/200] [Batch 100] [D loss: 0.216240, acc:  66%] [G loss: 2.585682, adv: 0.767649, recon: 0.051818, id: 0.003289] time: 11:13:44.732824 \n",
      "[Epoch 179/200] [Batch 0] [D loss: 0.120961, acc:  85%] [G loss: 2.716003, adv: 0.870693, recon: 0.048100, id: 0.003497] time: 11:14:58.332570 \n",
      "[Epoch 179/200] [Batch 100] [D loss: 0.158977, acc:  78%] [G loss: 2.547820, adv: 0.777442, recon: 0.049104, id: 0.003985] time: 11:17:24.710017 \n",
      "[Epoch 180/200] [Batch 0] [D loss: 0.122931, acc:  89%] [G loss: 2.746578, adv: 0.858929, recon: 0.050885, id: 0.005325] time: 11:18:38.283508 \n",
      "[Epoch 180/200] [Batch 100] [D loss: 0.187183, acc:  69%] [G loss: 2.272976, adv: 0.675119, recon: 0.045406, id: 0.008067] time: 11:21:04.587881 \n",
      "[Epoch 181/200] [Batch 0] [D loss: 0.150128, acc:  81%] [G loss: 2.287930, adv: 0.713855, recon: 0.042573, id: 0.003147] time: 11:22:18.472886 \n",
      "[Epoch 181/200] [Batch 100] [D loss: 0.111307, acc:  89%] [G loss: 2.434775, adv: 0.792117, recon: 0.042114, id: 0.003065] time: 11:24:44.791287 \n",
      "[Epoch 182/200] [Batch 0] [D loss: 0.143330, acc:  85%] [G loss: 2.408884, adv: 0.782964, recon: 0.041799, id: 0.002803] time: 11:25:58.403063 \n",
      "[Epoch 182/200] [Batch 100] [D loss: 0.172579, acc:  75%] [G loss: 2.457233, adv: 0.741749, recon: 0.048202, id: 0.003321] time: 11:28:24.686101 \n",
      "[Epoch 183/200] [Batch 0] [D loss: 0.123696, acc:  87%] [G loss: 2.454185, adv: 0.775295, recon: 0.044814, id: 0.003131] time: 11:29:38.248762 \n",
      "[Epoch 183/200] [Batch 100] [D loss: 0.257586, acc:  59%] [G loss: 2.481514, adv: 0.748608, recon: 0.048826, id: 0.002669] time: 11:32:04.621545 \n",
      "[Epoch 184/200] [Batch 0] [D loss: 0.145526, acc:  81%] [G loss: 2.672447, adv: 0.879453, recon: 0.045174, id: 0.003475] time: 11:33:18.219825 \n",
      "[Epoch 184/200] [Batch 100] [D loss: 0.182160, acc:  74%] [G loss: 2.307399, adv: 0.734289, recon: 0.041535, id: 0.004083] time: 11:35:44.589758 \n",
      "[Epoch 185/200] [Batch 0] [D loss: 0.180979, acc:  73%] [G loss: 2.264661, adv: 0.723809, recon: 0.040461, id: 0.003274] time: 11:37:01.786702 \n",
      "[Epoch 185/200] [Batch 100] [D loss: 0.143273, acc:  84%] [G loss: 2.449105, adv: 0.765418, recon: 0.045426, id: 0.002871] time: 11:39:27.971020 \n",
      "[Epoch 186/200] [Batch 0] [D loss: 0.150167, acc:  79%] [G loss: 2.402248, adv: 0.746678, recon: 0.045040, id: 0.002787] time: 11:40:41.576066 \n",
      "[Epoch 186/200] [Batch 100] [D loss: 0.215303, acc:  67%] [G loss: 2.166512, adv: 0.699719, recon: 0.038032, id: 0.003007] time: 11:43:07.805674 \n",
      "[Epoch 187/200] [Batch 0] [D loss: 0.155533, acc:  80%] [G loss: 2.391077, adv: 0.764304, recon: 0.042758, id: 0.002727] time: 11:44:21.309514 \n",
      "[Epoch 187/200] [Batch 100] [D loss: 0.173241, acc:  75%] [G loss: 2.578359, adv: 0.776842, recon: 0.050565, id: 0.006497] time: 11:46:47.509359 \n",
      "[Epoch 188/200] [Batch 0] [D loss: 0.208248, acc:  67%] [G loss: 2.370658, adv: 0.672750, recon: 0.049792, id: 0.024096] time: 11:48:01.070784 \n",
      "[Epoch 188/200] [Batch 100] [D loss: 0.135833, acc:  83%] [G loss: 2.428912, adv: 0.824204, recon: 0.038716, id: 0.002383] time: 11:50:27.233871 \n",
      "[Epoch 189/200] [Batch 0] [D loss: 0.170841, acc:  75%] [G loss: 2.444552, adv: 0.797143, recon: 0.041905, id: 0.008825] time: 11:51:40.754844 \n",
      "[Epoch 189/200] [Batch 100] [D loss: 0.169116, acc:  73%] [G loss: 2.624530, adv: 0.910386, recon: 0.039842, id: 0.002954] time: 11:54:06.959032 \n",
      "[Epoch 190/200] [Batch 0] [D loss: 0.140244, acc:  85%] [G loss: 2.254382, adv: 0.728440, recon: 0.039467, id: 0.002711] time: 11:55:20.458230 \n",
      "[Epoch 190/200] [Batch 100] [D loss: 0.170697, acc:  80%] [G loss: 2.412218, adv: 0.758562, recon: 0.044382, id: 0.002740] time: 11:57:46.573708 \n",
      "[Epoch 191/200] [Batch 0] [D loss: 0.127536, acc:  85%] [G loss: 2.531681, adv: 0.778065, recon: 0.047684, id: 0.005238] time: 11:59:00.347021 \n",
      "[Epoch 191/200] [Batch 100] [D loss: 0.134938, acc:  82%] [G loss: 2.640997, adv: 0.881812, recon: 0.043471, id: 0.002759] time: 12:01:26.486808 \n",
      "[Epoch 192/200] [Batch 0] [D loss: 0.149800, acc:  80%] [G loss: 2.347384, adv: 0.760124, recon: 0.040993, id: 0.003173] time: 12:02:40.013736 \n",
      "[Epoch 192/200] [Batch 100] [D loss: 0.213563, acc:  65%] [G loss: 2.541632, adv: 0.843724, recon: 0.042383, id: 0.002698] time: 12:05:06.188196 \n",
      "[Epoch 193/200] [Batch 0] [D loss: 0.178961, acc:  72%] [G loss: 2.570624, adv: 0.827452, recon: 0.045344, id: 0.003526] time: 12:06:19.696716 \n",
      "[Epoch 193/200] [Batch 100] [D loss: 0.204861, acc:  70%] [G loss: 2.633821, adv: 0.891353, recon: 0.042173, id: 0.004203] time: 12:08:45.957988 \n",
      "[Epoch 194/200] [Batch 0] [D loss: 0.147340, acc:  81%] [G loss: 2.545362, adv: 0.840261, recon: 0.042858, id: 0.003449] time: 12:09:59.508232 \n",
      "[Epoch 194/200] [Batch 100] [D loss: 0.168681, acc:  75%] [G loss: 2.516884, adv: 0.809298, recon: 0.044565, id: 0.002710] time: 12:12:29.384376 \n",
      "[Epoch 195/200] [Batch 0] [D loss: 0.160679, acc:  78%] [G loss: 2.819654, adv: 0.913930, recon: 0.049005, id: 0.003163] time: 12:13:42.948583 \n",
      "[Epoch 195/200] [Batch 100] [D loss: 0.119099, acc:  87%] [G loss: 2.560549, adv: 0.819803, recon: 0.045671, id: 0.002649] time: 12:16:09.252929 \n",
      "[Epoch 196/200] [Batch 0] [D loss: 0.166540, acc:  77%] [G loss: 2.522011, adv: 0.822907, recon: 0.043452, id: 0.002738] time: 12:17:22.760594 \n",
      "[Epoch 196/200] [Batch 100] [D loss: 0.138240, acc:  83%] [G loss: 2.427986, adv: 0.775725, recon: 0.043295, id: 0.002629] time: 12:19:48.901279 \n",
      "[Epoch 197/200] [Batch 0] [D loss: 0.242907, acc:  58%] [G loss: 2.286507, adv: 0.672133, recon: 0.046632, id: 0.003551] time: 12:21:02.464314 \n",
      "[Epoch 197/200] [Batch 100] [D loss: 0.138314, acc:  80%] [G loss: 2.663517, adv: 0.843654, recon: 0.048266, id: 0.003449] time: 12:23:28.656642 \n",
      "[Epoch 198/200] [Batch 0] [D loss: 0.201745, acc:  68%] [G loss: 2.744985, adv: 0.911921, recon: 0.045528, id: 0.006152] time: 12:24:42.186864 \n",
      "[Epoch 198/200] [Batch 100] [D loss: 0.164598, acc:  76%] [G loss: 2.404433, adv: 0.816400, recon: 0.038178, id: 0.001944] time: 12:27:08.354777 \n",
      "[Epoch 199/200] [Batch 0] [D loss: 0.141875, acc:  82%] [G loss: 2.407805, adv: 0.764296, recon: 0.043600, id: 0.002531] time: 12:28:21.848774 \n",
      "[Epoch 199/200] [Batch 100] [D loss: 0.159780, acc:  80%] [G loss: 2.562667, adv: 0.793135, recon: 0.048330, id: 0.004269] time: 12:30:48.030411 \n"
     ]
    }
   ],
   "source": [
    "gan.train(epochs=200, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 256, 256, 3) 0                                            \n__________________________________________________________________________________________________\nconv2d_10 (Conv2D)              (None, 256, 256, 32) 896         input_3[0][0]                    \n__________________________________________________________________________________________________\ninstance_normalization_6 (Insta (None, 256, 256, 32) 2           conv2d_10[0][0]                  \n__________________________________________________________________________________________________\nconv2d_11 (Conv2D)              (None, 256, 256, 32) 9248        instance_normalization_6[0][0]   \n__________________________________________________________________________________________________\ninstance_normalization_7 (Insta (None, 256, 256, 32) 2           conv2d_11[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 128, 128, 32) 0           instance_normalization_7[0][0]   \n__________________________________________________________________________________________________\nconv2d_12 (Conv2D)              (None, 128, 128, 64) 18496       max_pooling2d[0][0]              \n__________________________________________________________________________________________________\ninstance_normalization_8 (Insta (None, 128, 128, 64) 2           conv2d_12[0][0]                  \n__________________________________________________________________________________________________\nconv2d_13 (Conv2D)              (None, 128, 128, 64) 36928       instance_normalization_8[0][0]   \n__________________________________________________________________________________________________\ninstance_normalization_9 (Insta (None, 128, 128, 64) 2           conv2d_13[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 64, 64, 64)   0           instance_normalization_9[0][0]   \n__________________________________________________________________________________________________\nconv2d_14 (Conv2D)              (None, 64, 64, 128)  73856       max_pooling2d_1[0][0]            \n__________________________________________________________________________________________________\ninstance_normalization_10 (Inst (None, 64, 64, 128)  2           conv2d_14[0][0]                  \n__________________________________________________________________________________________________\nconv2d_15 (Conv2D)              (None, 64, 64, 128)  147584      instance_normalization_10[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_11 (Inst (None, 64, 64, 128)  2           conv2d_15[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 32, 32, 128)  0           instance_normalization_11[0][0]  \n__________________________________________________________________________________________________\nconv2d_16 (Conv2D)              (None, 32, 32, 256)  295168      max_pooling2d_2[0][0]            \n__________________________________________________________________________________________________\ninstance_normalization_12 (Inst (None, 32, 32, 256)  2           conv2d_16[0][0]                  \n__________________________________________________________________________________________________\nconv2d_17 (Conv2D)              (None, 32, 32, 256)  590080      instance_normalization_12[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_13 (Inst (None, 32, 32, 256)  2           conv2d_17[0][0]                  \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 16, 16, 256)  0           instance_normalization_13[0][0]  \n__________________________________________________________________________________________________\nconv2d_18 (Conv2D)              (None, 16, 16, 512)  1180160     max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\ninstance_normalization_14 (Inst (None, 16, 16, 512)  2           conv2d_18[0][0]                  \n__________________________________________________________________________________________________\nconv2d_19 (Conv2D)              (None, 16, 16, 512)  2359808     instance_normalization_14[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_15 (Inst (None, 16, 16, 512)  2           conv2d_19[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d (UpSampling2D)    (None, 32, 32, 512)  0           instance_normalization_15[0][0]  \n__________________________________________________________________________________________________\nconv2d_20 (Conv2D)              (None, 32, 32, 128)  32896       instance_normalization_13[0][0]  \n__________________________________________________________________________________________________\nconv2d_21 (Conv2D)              (None, 32, 32, 128)  65664       up_sampling2d[0][0]              \n__________________________________________________________________________________________________\nadd (Add)                       (None, 32, 32, 128)  0           conv2d_20[0][0]                  \n                                                                 conv2d_21[0][0]                  \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 32, 32, 128)  0           add[0][0]                        \n__________________________________________________________________________________________________\nconv2d_22 (Conv2D)              (None, 32, 32, 1)    129         activation[0][0]                 \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 32, 32, 1)    0           conv2d_22[0][0]                  \n__________________________________________________________________________________________________\nmultiply (Multiply)             (None, 32, 32, 256)  0           instance_normalization_13[0][0]  \n                                                                 activation_1[0][0]               \n__________________________________________________________________________________________________\nlambda (Lambda)                 (None, 32, 32, 768)  0           up_sampling2d[0][0]              \n                                                                 multiply[0][0]                   \n__________________________________________________________________________________________________\nconv2d_23 (Conv2D)              (None, 32, 32, 256)  1769728     lambda[0][0]                     \n__________________________________________________________________________________________________\ninstance_normalization_16 (Inst (None, 32, 32, 256)  2           conv2d_23[0][0]                  \n__________________________________________________________________________________________________\nconv2d_24 (Conv2D)              (None, 32, 32, 256)  590080      instance_normalization_16[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_17 (Inst (None, 32, 32, 256)  2           conv2d_24[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_1 (UpSampling2D)  (None, 64, 64, 256)  0           instance_normalization_17[0][0]  \n__________________________________________________________________________________________________\nconv2d_25 (Conv2D)              (None, 64, 64, 64)   8256        instance_normalization_11[0][0]  \n__________________________________________________________________________________________________\nconv2d_26 (Conv2D)              (None, 64, 64, 64)   16448       up_sampling2d_1[0][0]            \n__________________________________________________________________________________________________\nadd_1 (Add)                     (None, 64, 64, 64)   0           conv2d_25[0][0]                  \n                                                                 conv2d_26[0][0]                  \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 64, 64, 64)   0           add_1[0][0]                      \n__________________________________________________________________________________________________\nconv2d_27 (Conv2D)              (None, 64, 64, 1)    65          activation_2[0][0]               \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 64, 64, 1)    0           conv2d_27[0][0]                  \n__________________________________________________________________________________________________\nmultiply_1 (Multiply)           (None, 64, 64, 128)  0           instance_normalization_11[0][0]  \n                                                                 activation_3[0][0]               \n__________________________________________________________________________________________________\nlambda_1 (Lambda)               (None, 64, 64, 384)  0           up_sampling2d_1[0][0]            \n                                                                 multiply_1[0][0]                 \n__________________________________________________________________________________________________\nconv2d_28 (Conv2D)              (None, 64, 64, 128)  442496      lambda_1[0][0]                   \n__________________________________________________________________________________________________\ninstance_normalization_18 (Inst (None, 64, 64, 128)  2           conv2d_28[0][0]                  \n__________________________________________________________________________________________________\nconv2d_29 (Conv2D)              (None, 64, 64, 128)  147584      instance_normalization_18[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_19 (Inst (None, 64, 64, 128)  2           conv2d_29[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_2 (UpSampling2D)  (None, 128, 128, 128 0           instance_normalization_19[0][0]  \n__________________________________________________________________________________________________\nconv2d_30 (Conv2D)              (None, 128, 128, 32) 2080        instance_normalization_9[0][0]   \n__________________________________________________________________________________________________\nconv2d_31 (Conv2D)              (None, 128, 128, 32) 4128        up_sampling2d_2[0][0]            \n__________________________________________________________________________________________________\nadd_2 (Add)                     (None, 128, 128, 32) 0           conv2d_30[0][0]                  \n                                                                 conv2d_31[0][0]                  \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 128, 128, 32) 0           add_2[0][0]                      \n__________________________________________________________________________________________________\nconv2d_32 (Conv2D)              (None, 128, 128, 1)  33          activation_4[0][0]               \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 128, 128, 1)  0           conv2d_32[0][0]                  \n__________________________________________________________________________________________________\nmultiply_2 (Multiply)           (None, 128, 128, 64) 0           instance_normalization_9[0][0]   \n                                                                 activation_5[0][0]               \n__________________________________________________________________________________________________\nlambda_2 (Lambda)               (None, 128, 128, 192 0           up_sampling2d_2[0][0]            \n                                                                 multiply_2[0][0]                 \n__________________________________________________________________________________________________\nconv2d_33 (Conv2D)              (None, 128, 128, 64) 110656      lambda_2[0][0]                   \n__________________________________________________________________________________________________\ninstance_normalization_20 (Inst (None, 128, 128, 64) 2           conv2d_33[0][0]                  \n__________________________________________________________________________________________________\nconv2d_34 (Conv2D)              (None, 128, 128, 64) 36928       instance_normalization_20[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_21 (Inst (None, 128, 128, 64) 2           conv2d_34[0][0]                  \n__________________________________________________________________________________________________\nup_sampling2d_3 (UpSampling2D)  (None, 256, 256, 64) 0           instance_normalization_21[0][0]  \n__________________________________________________________________________________________________\nconv2d_35 (Conv2D)              (None, 256, 256, 16) 528         instance_normalization_7[0][0]   \n__________________________________________________________________________________________________\nconv2d_36 (Conv2D)              (None, 256, 256, 16) 1040        up_sampling2d_3[0][0]            \n__________________________________________________________________________________________________\nadd_3 (Add)                     (None, 256, 256, 16) 0           conv2d_35[0][0]                  \n                                                                 conv2d_36[0][0]                  \n__________________________________________________________________________________________________\nactivation_6 (Activation)       (None, 256, 256, 16) 0           add_3[0][0]                      \n__________________________________________________________________________________________________\nconv2d_37 (Conv2D)              (None, 256, 256, 1)  17          activation_6[0][0]               \n__________________________________________________________________________________________________\nactivation_7 (Activation)       (None, 256, 256, 1)  0           conv2d_37[0][0]                  \n__________________________________________________________________________________________________\nmultiply_3 (Multiply)           (None, 256, 256, 32) 0           instance_normalization_7[0][0]   \n                                                                 activation_7[0][0]               \n__________________________________________________________________________________________________\nlambda_3 (Lambda)               (None, 256, 256, 96) 0           up_sampling2d_3[0][0]            \n                                                                 multiply_3[0][0]                 \n__________________________________________________________________________________________________\nconv2d_38 (Conv2D)              (None, 256, 256, 32) 27680       lambda_3[0][0]                   \n__________________________________________________________________________________________________\ninstance_normalization_22 (Inst (None, 256, 256, 32) 2           conv2d_38[0][0]                  \n__________________________________________________________________________________________________\nconv2d_39 (Conv2D)              (None, 256, 256, 32) 9248        instance_normalization_22[0][0]  \n__________________________________________________________________________________________________\ninstance_normalization_23 (Inst (None, 256, 256, 32) 2           conv2d_39[0][0]                  \n__________________________________________________________________________________________________\nconv2d_40 (Conv2D)              (None, 256, 256, 3)  99          instance_normalization_23[0][0]  \n__________________________________________________________________________________________________\nactivation_8 (Activation)       (None, 256, 256, 3)  0           conv2d_40[0][0]                  \n==================================================================================================\nTotal params: 7,978,043\nTrainable params: 7,978,043\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gan.g_AB.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputlayer = gan.g_AB.get_layer('activation_7').output\n",
    "attention = Model(gan.g_AB.inputs, outputlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1569/1569 [00:03<00:00, 445.41it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path1 = r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210625\\train\\20210625_sub01\\Recordings'\n",
    "data1 = load_data(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(data1),20):\n",
    "    predict = attention.predict(data1[i:i+1])\n",
    "    plt.figure(figsize=(10,20))\n",
    "    plt.subplot(3,1,1)\n",
    "    plt.imshow(data1[i])\n",
    "    plt.subplot(3,1,2)\n",
    "    plt.imshow(predict[0])\n",
    "\n",
    "    imgA = data1[i:i+1]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.subplot(3,1,3)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.savefig(os.path.join('images/attention_img', f'{i}.png'), dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.g_AB.load_weights('saved_model/AB_0190.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 14914/14914 [18:46<00:00, 13.24it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "data_list = glob.glob(r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210624_delete\\*' + '/*/Recordings/*.png')\n",
    "for tmp in tqdm.tqdm(data_list): \n",
    "    img = cv2.cvtColor(cv2.imread(tmp),cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256,256))/127.5 - 1\n",
    "    predictB = gan.g_AB.predict(img[np.newaxis,...])\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    cv2.imwrite(os.path.join(tmp.replace('Recordings', 'trans3')), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1569/1569 [01:43<00:00, 15.14it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "data_list = glob.glob(r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210624_delete\\im\\Recordings\\*.png')\n",
    "for tmp in tqdm.tqdm(data_list): \n",
    "    img = cv2.cvtColor(cv2.imread(tmp),cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256,256))/127.5 - 1\n",
    "    predictB = gan.g_AB.predict(img[np.newaxis,...])\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    cv2.imwrite(os.path.join(tmp.replace('Recordings', 'test3')), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in data1:\n",
    "    imgA = i[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.close()\n",
    "    cv2.imwrite(os.path.join(r'C:\\Users\\HW-steve\\Documents\\GitHub\\endo_pytorch_cycleGAN\\images\\a', 'trans_%04d.png'%count), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'images/trans_images/'\n",
    "img_path = 'images/A/'\n",
    "\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))\n",
    "\n",
    "gan.g_AB.load_weights('saved_model/flip_0831/type1_AB_0110.h5')\n",
    "gan.g_BA.load_weights('saved_model/flip_0831/type1_BA_0110.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.show()\n",
    "#     cv2.imwrite(os.path.join(save_path, 'trans_%04d.png'%i), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/bong/data/depth_train'\n",
    "save_path = '/bong/data/type1'\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "#      plt.imshow(img_trans)\n",
    "#      plt.show()\n",
    "    cv2.imwrite(os.path.join(save_path, 'type1_'+os.path.basename(img_list[i].split('.')[0])+'.png'), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data_file)):\n",
    "    path = glob.glob(os.path.join(data_path, data_file[j]) + '/*.png')\n",
    "    up_file_name = 'type1_%04d_'%j\n",
    "    for i in range(len(path)):\n",
    "        ori = cv2.imread(path[i])\n",
    "        img = ori[:,:256,:]\n",
    "        dep = ori[:,256:,:]\n",
    "        cv2.imwrite(os.path.join(save_img_path, up_file_name + os.path.basename(path[i])), img)\n",
    "        cv2.imwrite(os.path.join(save_dep_path, up_file_name + os.path.basename(path[i])), dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pathA = 'images/A/0001_image_0456.png'\n",
    "img_pathB = 'images/B/0000.png'\n",
    "imgA = cv2.cvtColor(cv2.imread(img_pathA),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "imgB = cv2.cvtColor(cv2.imread(img_pathB),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'saved_model/flip_0831'\n",
    "AB_save_list = sorted(glob.glob(save_path + '/type1_AB_*.h5'))\n",
    "BA_save_list = sorted(glob.glob(save_path + '/type1_BA_*.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "for i in range(len(AB_save_list)):\n",
    "    gan.g_AB.load_weights(AB_save_list[i])\n",
    "    gan.g_BA.load_weights(BA_save_list[i])\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    predictA = gan.g_BA.predict(predictB)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(0.5 * imgA[0,...] + 0.5)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(0.5 * predictB[0,...] + 0.5)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(0.5 * predictA[0,...] + 0.5)\n",
    "    plt.show()\n",
    "    print('iter :', AB_save_list[i], 'loss :' , np.mean(np.abs(predictA - imgA)))\n",
    "    loss.append(np.mean(np.abs(predictA - imgA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(loss).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = [i for i in range(len(loss)) if loss[i] == 0.04742805290815668]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_save_list[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tensor25': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "9e149ba8c4c5ea18857919ddbadd3050e3e74d2ebf3e9aab941bca58425ee743"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}