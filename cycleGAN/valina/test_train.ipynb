{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from instancenormalization import InstanceNormalization\n",
    "\n",
    "from atten_Unet import att_unet \n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader2 import data_loader\n",
    "from data_loader2 import load_data\n",
    "import numpy as np\n",
    "import os, random, sys, datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "ramdom_seed = 5198\n",
    "np.random.seed(ramdom_seed)\n",
    "random.seed(ramdom_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(ramdom_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        data_path1 = r'images/Recording'\n",
    "        data_path2 = r'iamges/normal_crop'\n",
    "        self.batch_size = 16\n",
    "        self.data1 = load_data(data_path1)\n",
    "        self.data2 = load_data(data_path2)\n",
    "        self.data_loader = data_loader(self.data1, self.data2, self.batch_size, aug = True)\n",
    "        self.dataset_name = 'images'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mse', 'mse'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((self.batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((self.batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i in range(150):\n",
    "                imgs_A, imgs_B = next(self.data_loader)\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                if batch_i % 100 ==0:\n",
    "                    print (\"[Epoch %d/%d] [Batch %d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "            if epoch % 10 == 0:\n",
    "                gan.g_AB.save_weights('saved_model/AB_%04d.h5'%epoch)\n",
    "                gan.g_BA.save_weights('saved_model/BA_%04d.h5'%epoch)\n",
    "        \n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A, imgs_B = next(self.data_loader)\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A[0:1])\n",
    "        fake_A = self.g_BA.predict(imgs_B[0:1])\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A[0:1], fake_B, reconstr_A, imgs_B[0:1], fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.subplot(231)\n",
    "        plt.imshow(gen_imgs[0])\n",
    "        plt.subplot(232)\n",
    "        plt.imshow(gen_imgs[1])\n",
    "        plt.subplot(233)\n",
    "        plt.imshow(gen_imgs[2])\n",
    "        plt.subplot(234)\n",
    "        plt.imshow(gen_imgs[3])\n",
    "        plt.subplot(235)\n",
    "        plt.imshow(gen_imgs[4])\n",
    "        plt.subplot(236)\n",
    "        plt.imshow(gen_imgs[5])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"images/%s/%04d_%04d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "gan = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/500] [Batch 0] [D loss: 0.000332, acc: 100%] [G loss: 0.372617, adv: 0.007197, recon: 0.017791, id: 0.000900] time: 17:28:19.557318 \n",
      "[Epoch 431/500] [Batch 100] [D loss: 0.000251, acc: 100%] [G loss: 0.367055, adv: 0.009605, recon: 0.017276, id: 0.000760] time: 17:29:57.138406 \n",
      "[Epoch 432/500] [Batch 0] [D loss: 0.000406, acc: 100%] [G loss: 0.350461, adv: 0.007626, recon: 0.016629, id: 0.000686] time: 17:30:46.330883 \n",
      "[Epoch 432/500] [Batch 100] [D loss: 0.054915, acc:  93%] [G loss: 1.626979, adv: 0.457183, recon: 0.034238, id: 0.002098] time: 17:32:23.781034 \n",
      "[Epoch 433/500] [Batch 0] [D loss: 0.164717, acc:  73%] [G loss: 1.533662, adv: 0.310088, recon: 0.043893, id: 0.002648] time: 17:33:12.956590 \n",
      "[Epoch 433/500] [Batch 100] [D loss: 0.026972, acc:  98%] [G loss: 0.460160, adv: 0.022209, recon: 0.020263, id: 0.005110] time: 17:34:50.459864 \n",
      "[Epoch 434/500] [Batch 0] [D loss: 0.012660, acc:  98%] [G loss: 0.419671, adv: 0.012858, recon: 0.019425, id: 0.001238] time: 17:35:39.657156 \n",
      "[Epoch 434/500] [Batch 100] [D loss: 0.000620, acc: 100%] [G loss: 0.490263, adv: 0.011666, recon: 0.023087, id: 0.002479] time: 17:37:17.145006 \n",
      "[Epoch 435/500] [Batch 0] [D loss: 0.002476, acc: 100%] [G loss: 0.400788, adv: 0.011749, recon: 0.018690, id: 0.001214] time: 17:38:06.405240 \n",
      "[Epoch 435/500] [Batch 100] [D loss: 0.001221, acc: 100%] [G loss: 0.392374, adv: 0.008452, recon: 0.018598, id: 0.001252] time: 17:39:43.907617 \n",
      "[Epoch 436/500] [Batch 0] [D loss: 0.000841, acc: 100%] [G loss: 0.349363, adv: 0.008934, recon: 0.016446, id: 0.000975] time: 17:40:33.168397 \n",
      "[Epoch 436/500] [Batch 100] [D loss: 0.003326, acc: 100%] [G loss: 0.348959, adv: 0.013972, recon: 0.015933, id: 0.000869] time: 17:42:10.860691 \n",
      "[Epoch 437/500] [Batch 0] [D loss: 0.000797, acc: 100%] [G loss: 0.364197, adv: 0.006233, recon: 0.017434, id: 0.001167] time: 17:43:00.115843 \n",
      "[Epoch 437/500] [Batch 100] [D loss: 0.000928, acc: 100%] [G loss: 0.378193, adv: 0.011366, recon: 0.017628, id: 0.000784] time: 17:44:37.611845 \n",
      "[Epoch 438/500] [Batch 0] [D loss: 0.000259, acc: 100%] [G loss: 0.376159, adv: 0.009371, recon: 0.017715, id: 0.000899] time: 17:45:26.838553 \n",
      "[Epoch 438/500] [Batch 100] [D loss: 0.000614, acc: 100%] [G loss: 0.380785, adv: 0.006411, recon: 0.018245, id: 0.000949] time: 17:47:04.328522 \n",
      "[Epoch 439/500] [Batch 0] [D loss: 0.005071, acc: 100%] [G loss: 0.370901, adv: 0.010636, recon: 0.017330, id: 0.000752] time: 17:47:53.550388 \n",
      "[Epoch 439/500] [Batch 100] [D loss: 0.006502, acc:  99%] [G loss: 0.381313, adv: 0.007338, recon: 0.018177, id: 0.001082] time: 17:49:30.922018 \n",
      "[Epoch 440/500] [Batch 0] [D loss: 0.000518, acc: 100%] [G loss: 0.403329, adv: 0.009140, recon: 0.019090, id: 0.001034] time: 17:50:20.162986 \n",
      "[Epoch 440/500] [Batch 100] [D loss: 0.000723, acc: 100%] [G loss: 0.378893, adv: 0.008123, recon: 0.017967, id: 0.000899] time: 17:51:57.645152 \n",
      "[Epoch 441/500] [Batch 0] [D loss: 0.000598, acc: 100%] [G loss: 0.394099, adv: 0.007064, recon: 0.018858, id: 0.001057] time: 17:52:47.056704 \n",
      "[Epoch 441/500] [Batch 100] [D loss: 0.000672, acc: 100%] [G loss: 0.379885, adv: 0.007938, recon: 0.018088, id: 0.000959] time: 17:54:24.539776 \n",
      "[Epoch 442/500] [Batch 0] [D loss: 0.000594, acc: 100%] [G loss: 0.366991, adv: 0.008882, recon: 0.017282, id: 0.000909] time: 17:55:13.714965 \n",
      "[Epoch 442/500] [Batch 100] [D loss: 0.001037, acc: 100%] [G loss: 0.351635, adv: 0.007831, recon: 0.016650, id: 0.001093] time: 17:56:51.105060 \n",
      "[Epoch 443/500] [Batch 0] [D loss: 0.000404, acc: 100%] [G loss: 0.400863, adv: 0.008258, recon: 0.019063, id: 0.000848] time: 17:57:40.212178 \n",
      "[Epoch 443/500] [Batch 100] [D loss: 0.000305, acc: 100%] [G loss: 0.344398, adv: 0.006100, recon: 0.016488, id: 0.000799] time: 17:59:17.393039 \n",
      "[Epoch 444/500] [Batch 0] [D loss: 0.000360, acc: 100%] [G loss: 0.337663, adv: 0.005857, recon: 0.016157, id: 0.000792] time: 18:00:06.519317 \n",
      "[Epoch 444/500] [Batch 100] [D loss: 0.000457, acc: 100%] [G loss: 0.362399, adv: 0.011525, recon: 0.016830, id: 0.000924] time: 18:01:43.783459 \n",
      "[Epoch 445/500] [Batch 0] [D loss: 0.000541, acc: 100%] [G loss: 0.378664, adv: 0.008530, recon: 0.017946, id: 0.000949] time: 18:02:33.016876 \n",
      "[Epoch 445/500] [Batch 100] [D loss: 0.000375, acc: 100%] [G loss: 0.384853, adv: 0.007687, recon: 0.018332, id: 0.000934] time: 18:04:10.701181 \n",
      "[Epoch 446/500] [Batch 0] [D loss: 0.000338, acc: 100%] [G loss: 0.344410, adv: 0.008155, recon: 0.016284, id: 0.000856] time: 18:05:00.186933 \n",
      "[Epoch 446/500] [Batch 100] [D loss: 0.001478, acc: 100%] [G loss: 0.362297, adv: 0.005501, recon: 0.017431, id: 0.000883] time: 18:06:38.208713 \n",
      "[Epoch 447/500] [Batch 0] [D loss: 0.000348, acc: 100%] [G loss: 0.378554, adv: 0.006867, recon: 0.018104, id: 0.001018] time: 18:07:27.724292 \n",
      "[Epoch 447/500] [Batch 100] [D loss: 0.075370, acc:  88%] [G loss: 0.442256, adv: 0.009275, recon: 0.021042, id: 0.001037] time: 18:09:14.909747 \n",
      "[Epoch 448/500] [Batch 0] [D loss: 0.001343, acc: 100%] [G loss: 0.379626, adv: 0.008120, recon: 0.017983, id: 0.001936] time: 18:10:04.145804 \n",
      "[Epoch 448/500] [Batch 100] [D loss: 0.000424, acc: 100%] [G loss: 0.346385, adv: 0.011219, recon: 0.016082, id: 0.000686] time: 18:11:41.827968 \n",
      "[Epoch 449/500] [Batch 0] [D loss: 0.000415, acc: 100%] [G loss: 0.369959, adv: 0.006753, recon: 0.017695, id: 0.000772] time: 18:12:31.230058 \n",
      "[Epoch 449/500] [Batch 100] [D loss: 0.000814, acc: 100%] [G loss: 0.353885, adv: 0.009844, recon: 0.016586, id: 0.000900] time: 18:14:09.254795 \n",
      "[Epoch 450/500] [Batch 0] [D loss: 0.002790, acc: 100%] [G loss: 0.377327, adv: 0.011085, recon: 0.017222, id: 0.008925] time: 18:14:58.809525 \n",
      "[Epoch 450/500] [Batch 100] [D loss: 0.000978, acc: 100%] [G loss: 0.351159, adv: 0.007014, recon: 0.016710, id: 0.001164] time: 18:16:36.887617 \n",
      "[Epoch 451/500] [Batch 0] [D loss: 0.000356, acc: 100%] [G loss: 0.343376, adv: 0.010560, recon: 0.015965, id: 0.000785] time: 18:17:26.507003 \n",
      "[Epoch 451/500] [Batch 100] [D loss: 0.000325, acc: 100%] [G loss: 0.395553, adv: 0.011930, recon: 0.018438, id: 0.000862] time: 18:19:04.433611 \n",
      "[Epoch 452/500] [Batch 0] [D loss: 0.000983, acc: 100%] [G loss: 0.439547, adv: 0.012026, recon: 0.020635, id: 0.000999] time: 18:19:53.926460 \n",
      "[Epoch 452/500] [Batch 100] [D loss: 0.001273, acc: 100%] [G loss: 0.384566, adv: 0.008817, recon: 0.018207, id: 0.001056] time: 18:21:31.936790 \n",
      "[Epoch 453/500] [Batch 0] [D loss: 0.000955, acc: 100%] [G loss: 0.393442, adv: 0.011807, recon: 0.018354, id: 0.000733] time: 18:22:21.368262 \n",
      "[Epoch 453/500] [Batch 100] [D loss: 0.000240, acc: 100%] [G loss: 0.351465, adv: 0.006992, recon: 0.016747, id: 0.000895] time: 18:23:59.428968 \n",
      "[Epoch 454/500] [Batch 0] [D loss: 0.000840, acc: 100%] [G loss: 0.360685, adv: 0.006969, recon: 0.017214, id: 0.000936] time: 18:24:48.928582 \n",
      "[Epoch 454/500] [Batch 100] [D loss: 0.025013, acc:  98%] [G loss: 0.396476, adv: 0.011665, recon: 0.018470, id: 0.001534] time: 18:26:26.883484 \n",
      "[Epoch 455/500] [Batch 0] [D loss: 0.002094, acc: 100%] [G loss: 0.402142, adv: 0.009140, recon: 0.018694, id: 0.007688] time: 18:27:16.270857 \n",
      "[Epoch 455/500] [Batch 100] [D loss: 0.000498, acc: 100%] [G loss: 0.363597, adv: 0.009412, recon: 0.017117, id: 0.000860] time: 18:28:54.244855 \n",
      "[Epoch 456/500] [Batch 0] [D loss: 0.000916, acc: 100%] [G loss: 0.414505, adv: 0.007035, recon: 0.019899, id: 0.000882] time: 18:29:43.783509 \n",
      "[Epoch 456/500] [Batch 100] [D loss: 0.000416, acc: 100%] [G loss: 0.388510, adv: 0.009238, recon: 0.018307, id: 0.001572] time: 18:31:21.907881 \n",
      "[Epoch 457/500] [Batch 0] [D loss: 0.000172, acc: 100%] [G loss: 0.433548, adv: 0.010204, recon: 0.020513, id: 0.000851] time: 18:32:11.477835 \n",
      "[Epoch 457/500] [Batch 100] [D loss: 0.000301, acc: 100%] [G loss: 0.415795, adv: 0.011158, recon: 0.019498, id: 0.000973] time: 18:33:49.535760 \n",
      "[Epoch 458/500] [Batch 0] [D loss: 0.000480, acc: 100%] [G loss: 0.414474, adv: 0.010211, recon: 0.019532, id: 0.000830] time: 18:34:39.056220 \n",
      "[Epoch 458/500] [Batch 100] [D loss: 0.000997, acc: 100%] [G loss: 0.487273, adv: 0.018832, recon: 0.022263, id: 0.000725] time: 18:36:17.240327 \n",
      "[Epoch 459/500] [Batch 0] [D loss: 0.000528, acc: 100%] [G loss: 0.414336, adv: 0.016465, recon: 0.018936, id: 0.000827] time: 18:37:06.770847 \n",
      "[Epoch 459/500] [Batch 100] [D loss: 0.000408, acc: 100%] [G loss: 0.421728, adv: 0.025708, recon: 0.018385, id: 0.000751] time: 18:38:44.855320 \n",
      "[Epoch 460/500] [Batch 0] [D loss: 0.003733, acc: 100%] [G loss: 0.393707, adv: 0.018879, recon: 0.017678, id: 0.000985] time: 18:39:34.331692 \n",
      "[Epoch 460/500] [Batch 100] [D loss: 0.000819, acc: 100%] [G loss: 0.410908, adv: 0.021615, recon: 0.018230, id: 0.001107] time: 18:41:12.495079 \n",
      "[Epoch 461/500] [Batch 0] [D loss: 0.004802, acc:  99%] [G loss: 0.374349, adv: 0.013408, recon: 0.017241, id: 0.000834] time: 18:42:02.176151 \n",
      "[Epoch 461/500] [Batch 100] [D loss: 0.000237, acc: 100%] [G loss: 0.404851, adv: 0.020384, recon: 0.018062, id: 0.000882] time: 18:43:40.149977 \n",
      "[Epoch 462/500] [Batch 0] [D loss: 0.000433, acc: 100%] [G loss: 0.395328, adv: 0.010536, recon: 0.018582, id: 0.000989] time: 18:44:29.546453 \n",
      "[Epoch 462/500] [Batch 100] [D loss: 0.000513, acc: 100%] [G loss: 0.394337, adv: 0.014345, recon: 0.018140, id: 0.001032] time: 18:46:07.482904 \n",
      "[Epoch 463/500] [Batch 0] [D loss: 0.000480, acc: 100%] [G loss: 0.387730, adv: 0.012367, recon: 0.018014, id: 0.001095] time: 18:46:56.933033 \n",
      "[Epoch 463/500] [Batch 100] [D loss: 0.001175, acc: 100%] [G loss: 0.395846, adv: 0.029751, recon: 0.016685, id: 0.000907] time: 18:48:34.776231 \n",
      "[Epoch 464/500] [Batch 0] [D loss: 0.001220, acc: 100%] [G loss: 0.391915, adv: 0.011705, recon: 0.018269, id: 0.000946] time: 18:49:24.212845 \n",
      "[Epoch 464/500] [Batch 100] [D loss: 0.000618, acc: 100%] [G loss: 0.373624, adv: 0.016476, recon: 0.016917, id: 0.000664] time: 18:51:02.081087 \n",
      "[Epoch 465/500] [Batch 0] [D loss: 0.000156, acc: 100%] [G loss: 0.426882, adv: 0.020852, recon: 0.019100, id: 0.000745] time: 18:51:51.534807 \n",
      "[Epoch 465/500] [Batch 100] [D loss: 0.000390, acc: 100%] [G loss: 0.399724, adv: 0.012193, recon: 0.018605, id: 0.001042] time: 18:53:29.413737 \n",
      "[Epoch 466/500] [Batch 0] [D loss: 0.000254, acc: 100%] [G loss: 0.392666, adv: 0.015341, recon: 0.017826, id: 0.003096] time: 18:54:19.014982 \n",
      "[Epoch 466/500] [Batch 100] [D loss: 0.000448, acc: 100%] [G loss: 0.364794, adv: 0.009265, recon: 0.017138, id: 0.001415] time: 18:55:57.336519 \n",
      "[Epoch 467/500] [Batch 0] [D loss: 0.000289, acc: 100%] [G loss: 0.370738, adv: 0.012169, recon: 0.017176, id: 0.000990] time: 18:56:47.078863 \n",
      "[Epoch 467/500] [Batch 100] [D loss: 0.000399, acc: 100%] [G loss: 0.368422, adv: 0.008702, recon: 0.017400, id: 0.001441] time: 18:58:34.892632 \n",
      "[Epoch 468/500] [Batch 0] [D loss: 0.000181, acc: 100%] [G loss: 0.364523, adv: 0.009679, recon: 0.017136, id: 0.000721] time: 18:59:24.254394 \n",
      "[Epoch 468/500] [Batch 100] [D loss: 0.000541, acc: 100%] [G loss: 0.367306, adv: 0.011838, recon: 0.016939, id: 0.000819] time: 19:01:02.252797 \n",
      "[Epoch 469/500] [Batch 0] [D loss: 0.001100, acc: 100%] [G loss: 0.377352, adv: 0.010143, recon: 0.017682, id: 0.001124] time: 19:01:51.749377 \n",
      "[Epoch 469/500] [Batch 100] [D loss: 0.000447, acc: 100%] [G loss: 0.418700, adv: 0.012944, recon: 0.019494, id: 0.000987] time: 19:03:29.945386 \n",
      "[Epoch 470/500] [Batch 0] [D loss: 0.001304, acc: 100%] [G loss: 0.343947, adv: 0.015039, recon: 0.015575, id: 0.000775] time: 19:04:19.519078 \n",
      "[Epoch 470/500] [Batch 100] [D loss: 0.003169, acc: 100%] [G loss: 0.421423, adv: 0.011967, recon: 0.019710, id: 0.000893] time: 19:05:57.705439 \n",
      "[Epoch 471/500] [Batch 0] [D loss: 0.004290, acc: 100%] [G loss: 0.401670, adv: 0.010812, recon: 0.018865, id: 0.000855] time: 19:06:47.425830 \n",
      "[Epoch 471/500] [Batch 100] [D loss: 0.000557, acc: 100%] [G loss: 0.368056, adv: 0.010403, recon: 0.017227, id: 0.000847] time: 19:08:25.531509 \n",
      "[Epoch 472/500] [Batch 0] [D loss: 0.001710, acc: 100%] [G loss: 0.345573, adv: 0.010464, recon: 0.016010, id: 0.003176] time: 19:09:14.922695 \n",
      "[Epoch 472/500] [Batch 100] [D loss: 0.000501, acc: 100%] [G loss: 0.353623, adv: 0.009062, recon: 0.016635, id: 0.000962] time: 19:10:52.716088 \n",
      "[Epoch 473/500] [Batch 0] [D loss: 0.000504, acc: 100%] [G loss: 0.366138, adv: 0.009648, recon: 0.017175, id: 0.000835] time: 19:11:42.152847 \n",
      "[Epoch 473/500] [Batch 100] [D loss: 0.000520, acc: 100%] [G loss: 0.340306, adv: 0.010870, recon: 0.015758, id: 0.000823] time: 19:13:19.902932 \n",
      "[Epoch 474/500] [Batch 0] [D loss: 0.000367, acc: 100%] [G loss: 0.354917, adv: 0.009958, recon: 0.016611, id: 0.000911] time: 19:14:09.283998 \n",
      "[Epoch 474/500] [Batch 100] [D loss: 0.000371, acc: 100%] [G loss: 0.343876, adv: 0.007655, recon: 0.016297, id: 0.000917] time: 19:15:47.176202 \n",
      "[Epoch 475/500] [Batch 0] [D loss: 0.000451, acc: 100%] [G loss: 0.364148, adv: 0.007341, recon: 0.017324, id: 0.000974] time: 19:16:36.481624 \n",
      "[Epoch 475/500] [Batch 100] [D loss: 0.003292, acc: 100%] [G loss: 0.396848, adv: 0.009908, recon: 0.018689, id: 0.001260] time: 19:18:14.246395 \n",
      "[Epoch 476/500] [Batch 0] [D loss: 0.000801, acc: 100%] [G loss: 0.401498, adv: 0.011986, recon: 0.018718, id: 0.000899] time: 19:19:03.729060 \n",
      "[Epoch 476/500] [Batch 100] [D loss: 0.000348, acc: 100%] [G loss: 0.381270, adv: 0.011372, recon: 0.017768, id: 0.000905] time: 19:20:41.649078 \n",
      "[Epoch 477/500] [Batch 0] [D loss: 0.000714, acc: 100%] [G loss: 0.394218, adv: 0.008081, recon: 0.018750, id: 0.000802] time: 19:21:31.066266 \n",
      "[Epoch 477/500] [Batch 100] [D loss: 0.000546, acc: 100%] [G loss: 0.370811, adv: 0.007220, recon: 0.017688, id: 0.000934] time: 19:23:08.954900 \n",
      "[Epoch 478/500] [Batch 0] [D loss: 0.000560, acc: 100%] [G loss: 0.336121, adv: 0.009571, recon: 0.015730, id: 0.000935] time: 19:23:58.360807 \n",
      "[Epoch 478/500] [Batch 100] [D loss: 0.000233, acc: 100%] [G loss: 0.354413, adv: 0.007909, recon: 0.016752, id: 0.000810] time: 19:25:36.227153 \n",
      "[Epoch 479/500] [Batch 0] [D loss: 0.000534, acc: 100%] [G loss: 0.351791, adv: 0.008676, recon: 0.016598, id: 0.000740] time: 19:26:25.567802 \n",
      "[Epoch 479/500] [Batch 100] [D loss: 0.000957, acc: 100%] [G loss: 0.433967, adv: 0.017861, recon: 0.019777, id: 0.000964] time: 19:28:03.347199 \n",
      "[Epoch 480/500] [Batch 0] [D loss: 0.002469, acc: 100%] [G loss: 0.451243, adv: 0.010034, recon: 0.019997, id: 0.001252] time: 19:28:52.795934 \n",
      "[Epoch 480/500] [Batch 100] [D loss: 0.003219, acc: 100%] [G loss: 0.412305, adv: 0.011463, recon: 0.019269, id: 0.001169] time: 19:30:30.515716 \n",
      "[Epoch 481/500] [Batch 0] [D loss: 0.000362, acc: 100%] [G loss: 0.414868, adv: 0.011140, recon: 0.019452, id: 0.001011] time: 19:31:20.102848 \n",
      "[Epoch 481/500] [Batch 100] [D loss: 0.000405, acc: 100%] [G loss: 0.373059, adv: 0.008133, recon: 0.017701, id: 0.000863] time: 19:32:57.788959 \n",
      "[Epoch 482/500] [Batch 0] [D loss: 0.000370, acc: 100%] [G loss: 0.376811, adv: 0.008339, recon: 0.017794, id: 0.001028] time: 19:33:47.122095 \n",
      "[Epoch 482/500] [Batch 100] [D loss: 0.000206, acc: 100%] [G loss: 0.438110, adv: 0.008432, recon: 0.020921, id: 0.000686] time: 19:35:24.774862 \n",
      "[Epoch 483/500] [Batch 0] [D loss: 0.001613, acc: 100%] [G loss: 0.411274, adv: 0.017532, recon: 0.018549, id: 0.001053] time: 19:36:14.100188 \n",
      "[Epoch 483/500] [Batch 100] [D loss: 0.000602, acc: 100%] [G loss: 0.412301, adv: 0.012785, recon: 0.019179, id: 0.000948] time: 19:37:51.866441 \n",
      "[Epoch 484/500] [Batch 0] [D loss: 0.000344, acc: 100%] [G loss: 0.357362, adv: 0.010654, recon: 0.016688, id: 0.000797] time: 19:38:41.108806 \n",
      "[Epoch 484/500] [Batch 100] [D loss: 0.026840, acc:  95%] [G loss: 0.373994, adv: 0.013246, recon: 0.017254, id: 0.000942] time: 19:40:18.711757 \n",
      "[Epoch 485/500] [Batch 0] [D loss: 0.000260, acc: 100%] [G loss: 0.418271, adv: 0.010457, recon: 0.019685, id: 0.000998] time: 19:41:08.028119 \n",
      "[Epoch 485/500] [Batch 100] [D loss: 0.000198, acc: 100%] [G loss: 0.424440, adv: 0.011845, recon: 0.019844, id: 0.000840] time: 19:42:45.669838 \n",
      "[Epoch 486/500] [Batch 0] [D loss: 0.000431, acc: 100%] [G loss: 0.400746, adv: 0.010944, recon: 0.018791, id: 0.000954] time: 19:43:34.967166 \n",
      "[Epoch 486/500] [Batch 100] [D loss: 0.000257, acc: 100%] [G loss: 0.387547, adv: 0.014836, recon: 0.017755, id: 0.000865] time: 19:45:13.051913 \n",
      "[Epoch 487/500] [Batch 0] [D loss: 0.000370, acc: 100%] [G loss: 0.361640, adv: 0.008775, recon: 0.017089, id: 0.000760] time: 19:46:02.779508 \n",
      "[Epoch 487/500] [Batch 100] [D loss: 0.000407, acc: 100%] [G loss: 0.386520, adv: 0.009631, recon: 0.018220, id: 0.000959] time: 19:47:41.229143 \n",
      "[Epoch 488/500] [Batch 0] [D loss: 0.031586, acc:  97%] [G loss: 0.396376, adv: 0.015051, recon: 0.018171, id: 0.000759] time: 19:48:30.956340 \n",
      "[Epoch 488/500] [Batch 100] [D loss: 0.000458, acc: 100%] [G loss: 0.337753, adv: 0.009642, recon: 0.015818, id: 0.000612] time: 19:50:18.583418 \n",
      "[Epoch 489/500] [Batch 0] [D loss: 0.000563, acc: 100%] [G loss: 0.365894, adv: 0.007243, recon: 0.017420, id: 0.000727] time: 19:51:07.953355 \n",
      "[Epoch 489/500] [Batch 100] [D loss: 0.000921, acc: 100%] [G loss: 0.447693, adv: 0.016898, recon: 0.020538, id: 0.000946] time: 19:52:45.681238 \n",
      "[Epoch 490/500] [Batch 0] [D loss: 0.000676, acc: 100%] [G loss: 0.367819, adv: 0.010072, recon: 0.017257, id: 0.000651] time: 19:53:35.199916 \n",
      "[Epoch 490/500] [Batch 100] [D loss: 0.000969, acc: 100%] [G loss: 0.334619, adv: 0.008310, recon: 0.015778, id: 0.000661] time: 19:55:13.210566 \n",
      "[Epoch 491/500] [Batch 0] [D loss: 0.012178, acc:  98%] [G loss: 0.362494, adv: 0.006179, recon: 0.017386, id: 0.000745] time: 19:56:02.899262 \n",
      "[Epoch 491/500] [Batch 100] [D loss: 0.000983, acc: 100%] [G loss: 0.412627, adv: 0.012873, recon: 0.019208, id: 0.000826] time: 19:57:40.998486 \n",
      "[Epoch 492/500] [Batch 0] [D loss: 0.000272, acc: 100%] [G loss: 0.368629, adv: 0.010634, recon: 0.017236, id: 0.000760] time: 19:58:30.558089 \n",
      "[Epoch 492/500] [Batch 100] [D loss: 0.002571, acc: 100%] [G loss: 0.391735, adv: 0.020013, recon: 0.017435, id: 0.001036] time: 20:00:08.628212 \n",
      "[Epoch 493/500] [Batch 0] [D loss: 0.000895, acc: 100%] [G loss: 0.342754, adv: 0.008229, recon: 0.016182, id: 0.000828] time: 20:00:58.039516 \n",
      "[Epoch 493/500] [Batch 100] [D loss: 0.001133, acc: 100%] [G loss: 0.400086, adv: 0.012263, recon: 0.018645, id: 0.000886] time: 20:02:36.046041 \n",
      "[Epoch 494/500] [Batch 0] [D loss: 0.000325, acc: 100%] [G loss: 0.364880, adv: 0.014393, recon: 0.016625, id: 0.000801] time: 20:03:25.509460 \n",
      "[Epoch 494/500] [Batch 100] [D loss: 0.000315, acc: 100%] [G loss: 0.385323, adv: 0.007166, recon: 0.018403, id: 0.000872] time: 20:05:03.583838 \n",
      "[Epoch 495/500] [Batch 0] [D loss: 0.000269, acc: 100%] [G loss: 0.488296, adv: 0.020709, recon: 0.022205, id: 0.000877] time: 20:05:53.146256 \n",
      "[Epoch 495/500] [Batch 100] [D loss: 0.000494, acc: 100%] [G loss: 0.451306, adv: 0.046339, recon: 0.017799, id: 0.001099] time: 20:07:31.075688 \n",
      "[Epoch 496/500] [Batch 0] [D loss: 0.000345, acc: 100%] [G loss: 0.499985, adv: 0.042560, recon: 0.020504, id: 0.000883] time: 20:08:20.652572 \n",
      "[Epoch 496/500] [Batch 100] [D loss: 0.108487, acc:  85%] [G loss: 1.472964, adv: 0.423773, recon: 0.029168, id: 0.030962] time: 20:09:59.125014 \n",
      "[Epoch 497/500] [Batch 0] [D loss: 0.074371, acc:  91%] [G loss: 1.520577, adv: 0.379657, recon: 0.036482, id: 0.025000] time: 20:10:49.001935 \n",
      "[Epoch 497/500] [Batch 100] [D loss: 0.110818, acc:  81%] [G loss: 1.325825, adv: 0.320554, recon: 0.033631, id: 0.007930] time: 20:12:27.604035 \n",
      "[Epoch 498/500] [Batch 0] [D loss: 0.059538, acc:  92%] [G loss: 1.523352, adv: 0.401020, recon: 0.035774, id: 0.001799] time: 20:13:17.322700 \n",
      "[Epoch 498/500] [Batch 100] [D loss: 0.005148, acc: 100%] [G loss: 0.493838, adv: 0.024770, recon: 0.021934, id: 0.001983] time: 20:14:55.916268 \n",
      "[Epoch 499/500] [Batch 0] [D loss: 0.002633, acc: 100%] [G loss: 0.470838, adv: 0.030735, recon: 0.020242, id: 0.001227] time: 20:15:45.686553 \n",
      "[Epoch 499/500] [Batch 100] [D loss: 0.001256, acc: 100%] [G loss: 0.457037, adv: 0.017798, recon: 0.020895, id: 0.001371] time: 20:17:24.133436 \n"
     ]
    }
   ],
   "source": [
    "gan.train(epochs=500, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2149/2149 [00:05<00:00, 418.21it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path1 = r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20200625_sub04\\Recordings'\n",
    "data1 = load_data(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.g_BA.load_weights('saved_model/BA_0490.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in data1:\n",
    "    imgA = i[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_BA.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.close()\n",
    "    cv2.imwrite(os.path.join(r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20200625_sub04\\trans2', 'trans_%04d.png'%count), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'images/trans_images/'\n",
    "img_path = 'images/A/'\n",
    "\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))\n",
    "\n",
    "gan.g_AB.load_weights('saved_model/flip_0831/type1_AB_0110.h5')\n",
    "gan.g_BA.load_weights('saved_model/flip_0831/type1_BA_0110.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.show()\n",
    "#     cv2.imwrite(os.path.join(save_path, 'trans_%04d.png'%i), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/bong/data/depth_train'\n",
    "save_path = '/bong/data/type1'\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "#      plt.imshow(img_trans)\n",
    "#      plt.show()\n",
    "    cv2.imwrite(os.path.join(save_path, 'type1_'+os.path.basename(img_list[i].split('.')[0])+'.png'), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data_file)):\n",
    "    path = glob.glob(os.path.join(data_path, data_file[j]) + '/*.png')\n",
    "    up_file_name = 'type1_%04d_'%j\n",
    "    for i in range(len(path)):\n",
    "        ori = cv2.imread(path[i])\n",
    "        img = ori[:,:256,:]\n",
    "        dep = ori[:,256:,:]\n",
    "        cv2.imwrite(os.path.join(save_img_path, up_file_name + os.path.basename(path[i])), img)\n",
    "        cv2.imwrite(os.path.join(save_dep_path, up_file_name + os.path.basename(path[i])), dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pathA = 'images/A/0001_image_0456.png'\n",
    "img_pathB = 'images/B/0000.png'\n",
    "imgA = cv2.cvtColor(cv2.imread(img_pathA),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "imgB = cv2.cvtColor(cv2.imread(img_pathB),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'saved_model/flip_0831'\n",
    "AB_save_list = sorted(glob.glob(save_path + '/type1_AB_*.h5'))\n",
    "BA_save_list = sorted(glob.glob(save_path + '/type1_BA_*.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "for i in range(len(AB_save_list)):\n",
    "    gan.g_AB.load_weights(AB_save_list[i])\n",
    "    gan.g_BA.load_weights(BA_save_list[i])\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    predictA = gan.g_BA.predict(predictB)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(0.5 * imgA[0,...] + 0.5)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(0.5 * predictB[0,...] + 0.5)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(0.5 * predictA[0,...] + 0.5)\n",
    "    plt.show()\n",
    "    print('iter :', AB_save_list[i], 'loss :' , np.mean(np.abs(predictA - imgA)))\n",
    "    loss.append(np.mean(np.abs(predictA - imgA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(loss).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = [i for i in range(len(loss)) if loss[i] == 0.04742805290815668]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_save_list[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "368cec5af9d55f3645ce893451bf6ff97de02598cc0bc19aa254d796841e01eb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('tensor2': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}