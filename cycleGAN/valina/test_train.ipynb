{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, LeakyReLU, UpSampling2D, Conv2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from instancenormalization import InstanceNormalization\n",
    "\n",
    "from atten_Unet import att_unet \n",
    "import matplotlib.pyplot as plt\n",
    "from data_loader2 import data_loader\n",
    "from data_loader2 import load_data\n",
    "import numpy as np\n",
    "import os, random, sys, datetime\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "ramdom_seed = 5198\n",
    "np.random.seed(ramdom_seed)\n",
    "random.seed(ramdom_seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(ramdom_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 256\n",
    "        self.img_cols = 256\n",
    "        self.channels = 3\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        data_path1 = r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210625\\train\\20210625_sub01\\Recordings'\n",
    "        data_path2 = r'D:\\DATA\\ENDOSCOPY\\yonsei\\normal_crop'\n",
    "        self.batch_size = 20\n",
    "        self.data1 = load_data(data_path1)\n",
    "        self.data2 = load_data(data_path2)\n",
    "        self.data_loader = data_loader(self.data1, self.data2, self.batch_size, aug = True)\n",
    "        self.dataset_name = 'images'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mse', 'mse'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u4)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((self.batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((self.batch_size,) + self.disc_patch)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i in range(150):\n",
    "                imgs_A, imgs_B = next(self.data_loader)\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                if batch_i % 100 ==0:\n",
    "                    print (\"[Epoch %d/%d] [Batch %d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                        % ( epoch, epochs,\n",
    "                                                                            batch_i,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            np.mean(g_loss[1:3]),\n",
    "                                                                            np.mean(g_loss[3:5]),\n",
    "                                                                            np.mean(g_loss[5:6]),\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "            if epoch % 10 == 0:\n",
    "                gan.g_AB.save_weights('saved_model/AB_%04d.h5'%epoch)\n",
    "                gan.g_BA.save_weights('saved_model/BA_%04d.h5'%epoch)\n",
    "        \n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3\n",
    "\n",
    "        imgs_A, imgs_B = next(self.data_loader)\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A[0:1])\n",
    "        fake_A = self.g_BA.predict(imgs_B[0:1])\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A[0:1], fake_B, reconstr_A, imgs_B[0:1], fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.subplot(231)\n",
    "        plt.imshow(gen_imgs[0])\n",
    "        plt.subplot(232)\n",
    "        plt.imshow(gen_imgs[1])\n",
    "        plt.subplot(233)\n",
    "        plt.imshow(gen_imgs[2])\n",
    "        plt.subplot(234)\n",
    "        plt.imshow(gen_imgs[3])\n",
    "        plt.subplot(235)\n",
    "        plt.imshow(gen_imgs[4])\n",
    "        plt.subplot(236)\n",
    "        plt.imshow(gen_imgs[5])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"images/%s/%04d_%04d.png\" % (self.dataset_name, epoch, batch_i))\n",
    "        plt.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1569/1569 [00:03<00:00, 446.28it/s]\n",
      "100%|██████████| 685/685 [00:06<00:00, 107.97it/s]\n"
     ]
    }
   ],
   "source": [
    "gan = CycleGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0/300] [Batch 0] [D loss: 7.365218, acc:  59%] [G loss: 414.569305, adv: 199.979980, recon: 0.659716, id: 0.490723] time: 0:00:18.513377 \n",
      "[Epoch 0/300] [Batch 100] [D loss: 0.328485, acc:  55%] [G loss: 4.946307, adv: 1.100113, recon: 0.130265, id: 0.050739] time: 0:02:28.288400 \n",
      "[Epoch 1/300] [Batch 0] [D loss: 0.404345, acc:  36%] [G loss: 3.902129, adv: 0.621120, recon: 0.120941, id: 0.116379] time: 0:03:33.584006 \n",
      "[Epoch 1/300] [Batch 100] [D loss: 0.288536, acc:  52%] [G loss: 2.876863, adv: 0.553929, recon: 0.077193, id: 0.115049] time: 0:05:43.856139 \n",
      "[Epoch 2/300] [Batch 0] [D loss: 0.305184, acc:  40%] [G loss: 2.968483, adv: 0.511915, recon: 0.084420, id: 0.133933] time: 0:06:49.307005 \n",
      "[Epoch 2/300] [Batch 100] [D loss: 0.246000, acc:  55%] [G loss: 2.523629, adv: 0.464355, recon: 0.068246, id: 0.124398] time: 0:08:50.915411 \n",
      "[Epoch 3/300] [Batch 0] [D loss: 0.241209, acc:  61%] [G loss: 2.779799, adv: 0.537192, recon: 0.073627, id: 0.110843] time: 0:09:52.580775 \n",
      "[Epoch 3/300] [Batch 100] [D loss: 0.222233, acc:  62%] [G loss: 3.294522, adv: 0.652407, recon: 0.089312, id: 0.094440] time: 0:11:53.976093 \n",
      "[Epoch 4/300] [Batch 0] [D loss: 0.219784, acc:  64%] [G loss: 2.977596, adv: 0.653752, recon: 0.073365, id: 0.097311] time: 0:12:55.383702 \n",
      "[Epoch 4/300] [Batch 100] [D loss: 0.231831, acc:  66%] [G loss: 3.309900, adv: 0.734165, recon: 0.081123, id: 0.113683] time: 0:14:57.574479 \n",
      "[Epoch 5/300] [Batch 0] [D loss: 0.172501, acc:  76%] [G loss: 3.042488, adv: 0.742517, recon: 0.067250, id: 0.105162] time: 0:15:58.600454 \n",
      "[Epoch 5/300] [Batch 100] [D loss: 0.237014, acc:  64%] [G loss: 2.309124, adv: 0.535387, recon: 0.052575, id: 0.089027] time: 0:17:59.778017 \n",
      "[Epoch 6/300] [Batch 0] [D loss: 0.215013, acc:  68%] [G loss: 2.778534, adv: 0.746448, recon: 0.054177, id: 0.089949] time: 0:19:01.180337 \n",
      "[Epoch 6/300] [Batch 100] [D loss: 0.157329, acc:  78%] [G loss: 3.308393, adv: 0.766333, recon: 0.076019, id: 0.124558] time: 0:21:03.746733 \n",
      "[Epoch 7/300] [Batch 0] [D loss: 0.181912, acc:  73%] [G loss: 3.393353, adv: 0.764418, recon: 0.082140, id: 0.084930] time: 0:22:05.212065 \n",
      "[Epoch 7/300] [Batch 100] [D loss: 0.210137, acc:  68%] [G loss: 2.809649, adv: 0.682723, recon: 0.058483, id: 0.163444] time: 0:24:07.108088 \n",
      "[Epoch 8/300] [Batch 0] [D loss: 0.239131, acc:  65%] [G loss: 3.104402, adv: 0.789720, recon: 0.063502, id: 0.116449] time: 0:25:09.016718 \n",
      "[Epoch 8/300] [Batch 100] [D loss: 0.113847, acc:  84%] [G loss: 3.190766, adv: 0.813412, recon: 0.067597, id: 0.092120] time: 0:27:10.227049 \n",
      "[Epoch 9/300] [Batch 0] [D loss: 0.084749, acc:  90%] [G loss: 3.175340, adv: 0.810971, recon: 0.068973, id: 0.057801] time: 0:28:11.306994 \n",
      "[Epoch 9/300] [Batch 100] [D loss: 0.099059, acc:  92%] [G loss: 3.114608, adv: 0.854087, recon: 0.059693, id: 0.078460] time: 0:30:14.040008 \n",
      "[Epoch 10/300] [Batch 0] [D loss: 0.097344, acc:  92%] [G loss: 3.242217, adv: 0.850822, recon: 0.065020, id: 0.133803] time: 0:31:15.514252 \n",
      "[Epoch 10/300] [Batch 100] [D loss: 0.138087, acc:  84%] [G loss: 3.437534, adv: 0.957226, recon: 0.065240, id: 0.118986] time: 0:33:17.413636 \n",
      "[Epoch 11/300] [Batch 0] [D loss: 0.186936, acc:  72%] [G loss: 2.594713, adv: 0.651696, recon: 0.052249, id: 0.147030] time: 0:34:18.934052 \n",
      "[Epoch 11/300] [Batch 100] [D loss: 0.198090, acc:  69%] [G loss: 2.875672, adv: 0.682088, recon: 0.065109, id: 0.098391] time: 0:36:22.060220 \n",
      "[Epoch 12/300] [Batch 0] [D loss: 0.237516, acc:  64%] [G loss: 2.492334, adv: 0.585931, recon: 0.057966, id: 0.084157] time: 0:37:23.708489 \n",
      "[Epoch 12/300] [Batch 100] [D loss: 0.225978, acc:  61%] [G loss: 2.514600, adv: 0.581868, recon: 0.058470, id: 0.114739] time: 0:39:25.783527 \n",
      "[Epoch 13/300] [Batch 0] [D loss: 0.183061, acc:  70%] [G loss: 2.393209, adv: 0.614841, recon: 0.048785, id: 0.122606] time: 0:40:27.228571 \n",
      "[Epoch 13/300] [Batch 100] [D loss: 0.255307, acc:  53%] [G loss: 2.346191, adv: 0.521090, recon: 0.059153, id: 0.060378] time: 0:42:30.018064 \n",
      "[Epoch 14/300] [Batch 0] [D loss: 0.212260, acc:  65%] [G loss: 2.306564, adv: 0.553253, recon: 0.052156, id: 0.082303] time: 0:43:31.510063 \n",
      "[Epoch 14/300] [Batch 100] [D loss: 0.213470, acc:  64%] [G loss: 2.222529, adv: 0.523255, recon: 0.049418, id: 0.121715] time: 0:45:33.392536 \n",
      "[Epoch 15/300] [Batch 0] [D loss: 0.263651, acc:  53%] [G loss: 2.015759, adv: 0.456968, recon: 0.046145, id: 0.079546] time: 0:46:34.737391 \n",
      "[Epoch 15/300] [Batch 100] [D loss: 0.184502, acc:  72%] [G loss: 2.378182, adv: 0.592318, recon: 0.050006, id: 0.110454] time: 0:48:36.508203 \n",
      "[Epoch 16/300] [Batch 0] [D loss: 0.184698, acc:  69%] [G loss: 2.398122, adv: 0.627839, recon: 0.051295, id: 0.071006] time: 0:49:37.521639 \n",
      "[Epoch 16/300] [Batch 100] [D loss: 0.253951, acc:  54%] [G loss: 2.327341, adv: 0.645911, recon: 0.047837, id: 0.046048] time: 0:51:39.239238 \n",
      "[Epoch 17/300] [Batch 0] [D loss: 0.262774, acc:  54%] [G loss: 2.299099, adv: 0.583925, recon: 0.049941, id: 0.056795] time: 0:52:40.571144 \n",
      "[Epoch 17/300] [Batch 100] [D loss: 0.248355, acc:  59%] [G loss: 2.257990, adv: 0.623491, recon: 0.044427, id: 0.052050] time: 0:54:43.179310 \n",
      "[Epoch 18/300] [Batch 0] [D loss: 0.185502, acc:  72%] [G loss: 2.350829, adv: 0.604412, recon: 0.049190, id: 0.080958] time: 0:55:44.495932 \n",
      "[Epoch 18/300] [Batch 100] [D loss: 0.203632, acc:  66%] [G loss: 2.337746, adv: 0.595835, recon: 0.047396, id: 0.115749] time: 0:57:45.717018 \n",
      "[Epoch 19/300] [Batch 0] [D loss: 0.148802, acc:  77%] [G loss: 2.436706, adv: 0.639910, recon: 0.052235, id: 0.042505] time: 0:58:46.655895 \n",
      "[Epoch 19/300] [Batch 100] [D loss: 0.145293, acc:  78%] [G loss: 2.478358, adv: 0.706683, recon: 0.046824, id: 0.055367] time: 1:00:48.748752 \n",
      "[Epoch 20/300] [Batch 0] [D loss: 0.166904, acc:  72%] [G loss: 2.663562, adv: 0.641308, recon: 0.062548, id: 0.078785] time: 1:01:50.166728 \n",
      "[Epoch 20/300] [Batch 100] [D loss: 0.160130, acc:  73%] [G loss: 2.419839, adv: 0.711136, recon: 0.041900, id: 0.097810] time: 1:03:52.112859 \n",
      "[Epoch 21/300] [Batch 0] [D loss: 0.183593, acc:  72%] [G loss: 2.880159, adv: 0.734026, recon: 0.064103, id: 0.063502] time: 1:04:53.650935 \n",
      "[Epoch 21/300] [Batch 100] [D loss: 0.177237, acc:  71%] [G loss: 2.720068, adv: 0.710528, recon: 0.057929, id: 0.077591] time: 1:06:56.518851 \n",
      "[Epoch 22/300] [Batch 0] [D loss: 0.133218, acc:  82%] [G loss: 2.695365, adv: 0.783773, recon: 0.050952, id: 0.030956] time: 1:07:57.942967 \n",
      "[Epoch 22/300] [Batch 100] [D loss: 0.154761, acc:  78%] [G loss: 2.594166, adv: 0.721271, recon: 0.049448, id: 0.086444] time: 1:09:59.757922 \n",
      "[Epoch 23/300] [Batch 0] [D loss: 0.134071, acc:  81%] [G loss: 2.542455, adv: 0.696635, recon: 0.048672, id: 0.085385] time: 1:11:01.202019 \n",
      "[Epoch 23/300] [Batch 100] [D loss: 0.114351, acc:  86%] [G loss: 2.915801, adv: 0.854401, recon: 0.051237, id: 0.080849] time: 1:13:03.037915 \n",
      "[Epoch 24/300] [Batch 0] [D loss: 0.115687, acc:  86%] [G loss: 2.940492, adv: 0.927100, recon: 0.049049, id: 0.046493] time: 1:14:05.457739 \n",
      "[Epoch 24/300] [Batch 100] [D loss: 0.115747, acc:  84%] [G loss: 3.375772, adv: 0.953942, recon: 0.066692, id: 0.036566] time: 1:16:07.527420 \n",
      "[Epoch 25/300] [Batch 0] [D loss: 0.191532, acc:  70%] [G loss: 2.816236, adv: 0.785595, recon: 0.058433, id: 0.050280] time: 1:17:08.936415 \n",
      "[Epoch 25/300] [Batch 100] [D loss: 0.184961, acc:  69%] [G loss: 2.960875, adv: 0.752238, recon: 0.062486, id: 0.147260] time: 1:19:10.785481 \n",
      "[Epoch 26/300] [Batch 0] [D loss: 0.077757, acc:  93%] [G loss: 3.118299, adv: 0.997613, recon: 0.050602, id: 0.060655] time: 1:20:12.211668 \n",
      "[Epoch 26/300] [Batch 100] [D loss: 0.090582, acc:  92%] [G loss: 3.206451, adv: 0.867841, recon: 0.065389, id: 0.098452] time: 1:22:15.110067 \n",
      "[Epoch 27/300] [Batch 0] [D loss: 0.124211, acc:  85%] [G loss: 2.863655, adv: 0.828445, recon: 0.056583, id: 0.028504] time: 1:23:16.574743 \n",
      "[Epoch 27/300] [Batch 100] [D loss: 0.095572, acc:  89%] [G loss: 3.086872, adv: 0.879713, recon: 0.058364, id: 0.097742] time: 1:25:18.387413 \n",
      "[Epoch 28/300] [Batch 0] [D loss: 0.131087, acc:  82%] [G loss: 3.439022, adv: 1.035973, recon: 0.057117, id: 0.139345] time: 1:26:19.837985 \n",
      "[Epoch 28/300] [Batch 100] [D loss: 0.164754, acc:  75%] [G loss: 3.116581, adv: 0.822597, recon: 0.067993, id: 0.068325] time: 1:28:22.867003 \n",
      "[Epoch 29/300] [Batch 0] [D loss: 0.288984, acc:  46%] [G loss: 2.271122, adv: 0.611699, recon: 0.044927, id: 0.094742] time: 1:29:24.317558 \n",
      "[Epoch 29/300] [Batch 100] [D loss: 0.186431, acc:  56%] [G loss: 2.257761, adv: 0.572471, recon: 0.052856, id: 0.012061] time: 1:31:26.142353 \n",
      "[Epoch 30/300] [Batch 0] [D loss: 0.184940, acc:  62%] [G loss: 2.276846, adv: 0.574133, recon: 0.051213, id: 0.048105] time: 1:32:27.554194 \n",
      "[Epoch 30/300] [Batch 100] [D loss: 0.205651, acc:  59%] [G loss: 2.224471, adv: 0.580904, recon: 0.047032, id: 0.035412] time: 1:34:29.469637 \n",
      "[Epoch 31/300] [Batch 0] [D loss: 0.211790, acc:  56%] [G loss: 2.189737, adv: 0.575463, recon: 0.048517, id: 0.023630] time: 1:35:30.975372 \n",
      "[Epoch 31/300] [Batch 100] [D loss: 0.235603, acc:  57%] [G loss: 2.183662, adv: 0.507788, recon: 0.053454, id: 0.016345] time: 1:37:33.939708 \n",
      "[Epoch 32/300] [Batch 0] [D loss: 0.235994, acc:  58%] [G loss: 1.614816, adv: 0.368359, recon: 0.039045, id: 0.010083] time: 1:38:35.312140 \n",
      "[Epoch 32/300] [Batch 100] [D loss: 0.241922, acc:  55%] [G loss: 2.093792, adv: 0.493923, recon: 0.052801, id: 0.008915] time: 1:40:37.090658 \n",
      "[Epoch 33/300] [Batch 0] [D loss: 0.244007, acc:  53%] [G loss: 1.819879, adv: 0.390108, recon: 0.044438, id: 0.114312] time: 1:41:38.437693 \n",
      "[Epoch 33/300] [Batch 100] [D loss: 0.188683, acc:  70%] [G loss: 2.205286, adv: 0.590383, recon: 0.048122, id: 0.022875] time: 1:43:40.144921 \n",
      "[Epoch 34/300] [Batch 0] [D loss: 0.263733, acc:  48%] [G loss: 1.625397, adv: 0.320767, recon: 0.040207, id: 0.131788] time: 1:44:42.274142 \n",
      "[Epoch 34/300] [Batch 100] [D loss: 0.271314, acc:  41%] [G loss: 1.732424, adv: 0.286359, recon: 0.053967, id: 0.065042] time: 1:46:44.101170 \n",
      "[Epoch 35/300] [Batch 0] [D loss: 0.272656, acc:  43%] [G loss: 1.552365, adv: 0.299292, recon: 0.044204, id: 0.018977] time: 1:47:45.562184 \n",
      "[Epoch 35/300] [Batch 100] [D loss: 0.254464, acc:  52%] [G loss: 1.598329, adv: 0.324386, recon: 0.043803, id: 0.004802] time: 1:49:47.448280 \n",
      "[Epoch 36/300] [Batch 0] [D loss: 0.242794, acc:  58%] [G loss: 1.543448, adv: 0.329557, recon: 0.041191, id: 0.014676] time: 1:50:48.826667 \n",
      "[Epoch 36/300] [Batch 100] [D loss: 0.244449, acc:  56%] [G loss: 1.522713, adv: 0.329422, recon: 0.039218, id: 0.063127] time: 1:52:52.006179 \n",
      "[Epoch 37/300] [Batch 0] [D loss: 0.244581, acc:  53%] [G loss: 1.719606, adv: 0.354147, recon: 0.043469, id: 0.022955] time: 1:53:53.520683 \n",
      "[Epoch 37/300] [Batch 100] [D loss: 0.226421, acc:  61%] [G loss: 1.503092, adv: 0.400745, recon: 0.032578, id: 0.020434] time: 1:55:55.393912 \n",
      "[Epoch 38/300] [Batch 0] [D loss: 0.230402, acc:  61%] [G loss: 1.760557, adv: 0.435301, recon: 0.043010, id: 0.013418] time: 1:57:03.101166 \n",
      "[Epoch 38/300] [Batch 100] [D loss: 0.249110, acc:  54%] [G loss: 1.902583, adv: 0.397057, recon: 0.052475, id: 0.028470] time: 1:59:05.981887 \n",
      "[Epoch 39/300] [Batch 0] [D loss: 0.238916, acc:  56%] [G loss: 1.804974, adv: 0.425894, recon: 0.046541, id: 0.005202] time: 2:00:07.138546 \n",
      "[Epoch 39/300] [Batch 100] [D loss: 0.210891, acc:  68%] [G loss: 2.151582, adv: 0.508996, recon: 0.053155, id: 0.003897] time: 2:02:09.961000 \n",
      "[Epoch 40/300] [Batch 0] [D loss: 0.207544, acc:  67%] [G loss: 2.161489, adv: 0.580617, recon: 0.045213, id: 0.012826] time: 2:03:11.502000 \n",
      "[Epoch 40/300] [Batch 100] [D loss: 0.226726, acc:  64%] [G loss: 1.947652, adv: 0.477375, recon: 0.048185, id: 0.007159] time: 2:05:13.731738 \n",
      "[Epoch 41/300] [Batch 0] [D loss: 0.199522, acc:  72%] [G loss: 2.054010, adv: 0.528678, recon: 0.048253, id: 0.006864] time: 2:06:15.425755 \n",
      "[Epoch 41/300] [Batch 100] [D loss: 0.160619, acc:  78%] [G loss: 2.088526, adv: 0.603619, recon: 0.042690, id: 0.003721] time: 2:08:17.571230 \n",
      "[Epoch 42/300] [Batch 0] [D loss: 0.212351, acc:  65%] [G loss: 2.165166, adv: 0.532228, recon: 0.053131, id: 0.014097] time: 2:09:19.165154 \n",
      "[Epoch 42/300] [Batch 100] [D loss: 0.194590, acc:  72%] [G loss: 2.236633, adv: 0.566628, recon: 0.053125, id: 0.004530] time: 2:11:22.594363 \n",
      "[Epoch 43/300] [Batch 0] [D loss: 0.225124, acc:  63%] [G loss: 2.057378, adv: 0.534982, recon: 0.047819, id: 0.005181] time: 2:12:24.228002 \n",
      "[Epoch 43/300] [Batch 100] [D loss: 0.176672, acc:  75%] [G loss: 2.462069, adv: 0.711527, recon: 0.048865, id: 0.014643] time: 2:14:26.313757 \n",
      "[Epoch 44/300] [Batch 0] [D loss: 0.234921, acc:  59%] [G loss: 2.143664, adv: 0.527992, recon: 0.052023, id: 0.017285] time: 2:15:27.883561 \n",
      "[Epoch 44/300] [Batch 100] [D loss: 0.210696, acc:  66%] [G loss: 2.110645, adv: 0.543559, recon: 0.049915, id: 0.003518] time: 2:17:29.866452 \n",
      "[Epoch 45/300] [Batch 0] [D loss: 0.180427, acc:  74%] [G loss: 2.194905, adv: 0.568025, recon: 0.049529, id: 0.014748] time: 2:18:31.432924 \n",
      "[Epoch 45/300] [Batch 100] [D loss: 0.185048, acc:  72%] [G loss: 2.373003, adv: 0.666890, recon: 0.048339, id: 0.050596] time: 2:20:34.873124 \n",
      "[Epoch 46/300] [Batch 0] [D loss: 0.220370, acc:  65%] [G loss: 2.520257, adv: 0.710489, recon: 0.053559, id: 0.009033] time: 2:21:36.489488 \n",
      "[Epoch 46/300] [Batch 100] [D loss: 0.214673, acc:  66%] [G loss: 2.105383, adv: 0.555630, recon: 0.047610, id: 0.003575] time: 2:23:38.542100 \n",
      "[Epoch 47/300] [Batch 0] [D loss: 0.197770, acc:  70%] [G loss: 2.346879, adv: 0.728936, recon: 0.043800, id: 0.003685] time: 2:24:40.142597 \n",
      "[Epoch 47/300] [Batch 100] [D loss: 0.193790, acc:  72%] [G loss: 2.800322, adv: 0.815954, recon: 0.052224, id: 0.072712] time: 2:26:42.132589 \n",
      "[Epoch 48/300] [Batch 0] [D loss: 0.184523, acc:  72%] [G loss: 2.136452, adv: 0.601720, recon: 0.042804, id: 0.067094] time: 2:27:43.706589 \n",
      "[Epoch 48/300] [Batch 100] [D loss: 0.219285, acc:  63%] [G loss: 2.221990, adv: 0.606979, recon: 0.049615, id: 0.007250] time: 2:29:45.747936 \n",
      "[Epoch 49/300] [Batch 0] [D loss: 0.155703, acc:  80%] [G loss: 2.347207, adv: 0.705115, recon: 0.045863, id: 0.008329] time: 2:30:48.789746 \n",
      "[Epoch 49/300] [Batch 100] [D loss: 0.185213, acc:  73%] [G loss: 2.459339, adv: 0.655593, recon: 0.055245, id: 0.027304] time: 2:32:50.906930 \n",
      "[Epoch 50/300] [Batch 0] [D loss: 0.195162, acc:  70%] [G loss: 2.412049, adv: 0.668202, recon: 0.050983, id: 0.016278] time: 2:33:52.498665 \n",
      "[Epoch 50/300] [Batch 100] [D loss: 0.179823, acc:  75%] [G loss: 2.151900, adv: 0.634379, recon: 0.042841, id: 0.008191] time: 2:35:54.543154 \n",
      "[Epoch 51/300] [Batch 0] [D loss: 0.175566, acc:  75%] [G loss: 2.399475, adv: 0.735907, recon: 0.044678, id: 0.018372] time: 2:36:56.173891 \n",
      "[Epoch 51/300] [Batch 100] [D loss: 0.173720, acc:  77%] [G loss: 2.337718, adv: 0.699858, recon: 0.045695, id: 0.003257] time: 2:38:58.306024 \n",
      "[Epoch 52/300] [Batch 0] [D loss: 0.217974, acc:  64%] [G loss: 2.702976, adv: 0.806736, recon: 0.052625, id: 0.019322] time: 2:39:59.899092 \n",
      "[Epoch 52/300] [Batch 100] [D loss: 0.211014, acc:  66%] [G loss: 2.402436, adv: 0.619562, recon: 0.056882, id: 0.003524] time: 2:42:03.553451 \n",
      "[Epoch 53/300] [Batch 0] [D loss: 0.219811, acc:  64%] [G loss: 2.271867, adv: 0.659472, recon: 0.044982, id: 0.019381] time: 2:43:05.144748 \n",
      "[Epoch 53/300] [Batch 100] [D loss: 0.260366, acc:  48%] [G loss: 1.520288, adv: 0.377028, recon: 0.037023, id: 0.011158] time: 2:45:07.386926 \n",
      "[Epoch 54/300] [Batch 0] [D loss: 0.254357, acc:  50%] [G loss: 1.600678, adv: 0.382760, recon: 0.040850, id: 0.003548] time: 2:46:08.974444 \n",
      "[Epoch 54/300] [Batch 100] [D loss: 0.244059, acc:  51%] [G loss: 1.710409, adv: 0.392077, recon: 0.044916, id: 0.013159] time: 2:48:11.258303 \n",
      "[Epoch 55/300] [Batch 0] [D loss: 0.260445, acc:  51%] [G loss: 1.506699, adv: 0.361569, recon: 0.037609, id: 0.003126] time: 2:49:12.785076 \n",
      "[Epoch 55/300] [Batch 100] [D loss: 0.224673, acc:  60%] [G loss: 1.706516, adv: 0.413552, recon: 0.042699, id: 0.015855] time: 2:51:16.536500 \n",
      "[Epoch 56/300] [Batch 0] [D loss: 0.235382, acc:  56%] [G loss: 1.536967, adv: 0.410088, recon: 0.035088, id: 0.005073] time: 2:52:18.108287 \n",
      "[Epoch 56/300] [Batch 100] [D loss: 0.236176, acc:  59%] [G loss: 1.667286, adv: 0.351941, recon: 0.047399, id: 0.005028] time: 2:54:20.315412 \n",
      "[Epoch 57/300] [Batch 0] [D loss: 0.242843, acc:  58%] [G loss: 1.485294, adv: 0.359026, recon: 0.037469, id: 0.006316] time: 2:55:21.832326 \n",
      "[Epoch 57/300] [Batch 100] [D loss: 0.234229, acc:  59%] [G loss: 1.861455, adv: 0.442129, recon: 0.046200, id: 0.044384] time: 2:57:23.905713 \n",
      "[Epoch 58/300] [Batch 0] [D loss: 0.233636, acc:  58%] [G loss: 1.805405, adv: 0.456892, recon: 0.041496, id: 0.048720] time: 2:58:25.415284 \n",
      "[Epoch 58/300] [Batch 100] [D loss: 0.177410, acc:  76%] [G loss: 2.323182, adv: 0.595378, recon: 0.054959, id: 0.025438] time: 3:00:27.484848 \n",
      "[Epoch 59/300] [Batch 0] [D loss: 0.212534, acc:  65%] [G loss: 2.307215, adv: 0.701398, recon: 0.041967, id: 0.011568] time: 3:01:28.998811 \n",
      "[Epoch 59/300] [Batch 100] [D loss: 0.170620, acc:  74%] [G loss: 2.945957, adv: 0.929852, recon: 0.052216, id: 0.015322] time: 3:03:32.100839 \n",
      "[Epoch 60/300] [Batch 0] [D loss: 0.147892, acc:  82%] [G loss: 2.790682, adv: 0.859462, recon: 0.050495, id: 0.010539] time: 3:04:33.383449 \n",
      "[Epoch 60/300] [Batch 100] [D loss: 0.196172, acc:  69%] [G loss: 2.330074, adv: 0.653638, recon: 0.048654, id: 0.035528] time: 3:06:34.889788 \n",
      "[Epoch 61/300] [Batch 0] [D loss: 0.150246, acc:  79%] [G loss: 2.684330, adv: 0.810533, recon: 0.050708, id: 0.039083] time: 3:07:36.233852 \n",
      "[Epoch 61/300] [Batch 100] [D loss: 0.163599, acc:  78%] [G loss: 2.897430, adv: 0.944483, recon: 0.049292, id: 0.010142] time: 3:09:37.617507 \n",
      "[Epoch 62/300] [Batch 0] [D loss: 0.115932, acc:  86%] [G loss: 2.761132, adv: 0.914222, recon: 0.045809, id: 0.003880] time: 3:10:38.832584 \n",
      "[Epoch 62/300] [Batch 100] [D loss: 0.136413, acc:  82%] [G loss: 2.675814, adv: 0.876807, recon: 0.045083, id: 0.009722] time: 3:12:40.225229 \n",
      "[Epoch 63/300] [Batch 0] [D loss: 0.136055, acc:  82%] [G loss: 2.530319, adv: 0.806964, recon: 0.044981, id: 0.004460] time: 3:13:43.182388 \n",
      "[Epoch 63/300] [Batch 100] [D loss: 0.163235, acc:  77%] [G loss: 2.587364, adv: 0.751109, recon: 0.053134, id: 0.005255] time: 3:15:44.601224 \n",
      "[Epoch 64/300] [Batch 0] [D loss: 0.121179, acc:  85%] [G loss: 2.879475, adv: 0.884199, recon: 0.054820, id: 0.006295] time: 3:16:45.796339 \n",
      "[Epoch 64/300] [Batch 100] [D loss: 0.102038, acc:  89%] [G loss: 2.719059, adv: 0.890552, recon: 0.045048, id: 0.026649] time: 3:18:47.275389 \n",
      "[Epoch 65/300] [Batch 0] [D loss: 0.126648, acc:  84%] [G loss: 2.994466, adv: 0.968077, recon: 0.051322, id: 0.021521] time: 3:19:48.538496 \n",
      "[Epoch 65/300] [Batch 100] [D loss: 0.120368, acc:  84%] [G loss: 2.726108, adv: 0.909431, recon: 0.044726, id: 0.005705] time: 3:21:49.900676 \n",
      "[Epoch 66/300] [Batch 0] [D loss: 0.252068, acc:  60%] [G loss: 2.793722, adv: 0.915017, recon: 0.047358, id: 0.007049] time: 3:22:51.151071 \n",
      "[Epoch 66/300] [Batch 100] [D loss: 0.142829, acc:  89%] [G loss: 3.248853, adv: 1.081807, recon: 0.052253, id: 0.030508] time: 3:24:52.636586 \n",
      "[Epoch 67/300] [Batch 0] [D loss: 0.132810, acc:  84%] [G loss: 2.895305, adv: 0.884919, recon: 0.055140, id: 0.004356] time: 3:25:55.737522 \n",
      "[Epoch 67/300] [Batch 100] [D loss: 0.098411, acc:  90%] [G loss: 2.805826, adv: 0.911958, recon: 0.044391, id: 0.082936] time: 3:27:57.323665 \n",
      "[Epoch 68/300] [Batch 0] [D loss: 0.143247, acc:  79%] [G loss: 2.727178, adv: 0.871005, recon: 0.047759, id: 0.017904] time: 3:28:58.590601 \n",
      "[Epoch 68/300] [Batch 100] [D loss: 0.105347, acc:  91%] [G loss: 3.271075, adv: 1.156486, recon: 0.046726, id: 0.009693] time: 3:31:00.125805 \n",
      "[Epoch 69/300] [Batch 0] [D loss: 0.148828, acc:  82%] [G loss: 2.858707, adv: 0.985039, recon: 0.042661, id: 0.019782] time: 3:32:01.334621 \n",
      "[Epoch 69/300] [Batch 100] [D loss: 0.108566, acc:  87%] [G loss: 2.823926, adv: 0.984547, recon: 0.041729, id: 0.004131] time: 3:34:02.688276 \n",
      "[Epoch 70/300] [Batch 0] [D loss: 0.137005, acc:  82%] [G loss: 2.899787, adv: 0.926538, recon: 0.050403, id: 0.025491] time: 3:35:03.867371 \n",
      "[Epoch 70/300] [Batch 100] [D loss: 0.108721, acc:  89%] [G loss: 2.767938, adv: 0.944497, recon: 0.043072, id: 0.004728] time: 3:37:05.788306 \n",
      "[Epoch 71/300] [Batch 0] [D loss: 0.134272, acc:  81%] [G loss: 2.801741, adv: 0.931639, recon: 0.046004, id: 0.008417] time: 3:38:09.784060 \n",
      "[Epoch 71/300] [Batch 100] [D loss: 0.150332, acc:  80%] [G loss: 2.900038, adv: 0.869596, recon: 0.054179, id: 0.005625] time: 3:40:11.178474 \n",
      "[Epoch 72/300] [Batch 0] [D loss: 0.105111, acc:  86%] [G loss: 3.063892, adv: 1.070035, recon: 0.045280, id: 0.010609] time: 3:41:12.222409 \n",
      "[Epoch 72/300] [Batch 100] [D loss: 0.120353, acc:  85%] [G loss: 3.099044, adv: 0.983159, recon: 0.054792, id: 0.020244] time: 3:43:13.322384 \n",
      "[Epoch 73/300] [Batch 0] [D loss: 0.137838, acc:  83%] [G loss: 2.690388, adv: 0.881843, recon: 0.045670, id: 0.005229] time: 3:44:14.492103 \n",
      "[Epoch 73/300] [Batch 100] [D loss: 0.174146, acc:  75%] [G loss: 2.942631, adv: 1.000070, recon: 0.046228, id: 0.006180] time: 3:46:16.795703 \n",
      "[Epoch 74/300] [Batch 0] [D loss: 0.117048, acc:  86%] [G loss: 2.949812, adv: 0.964830, recon: 0.050264, id: 0.004198] time: 3:47:18.342941 \n",
      "[Epoch 74/300] [Batch 100] [D loss: 0.110643, acc:  86%] [G loss: 3.288683, adv: 1.097114, recon: 0.052297, id: 0.037980] time: 3:49:20.464788 \n",
      "[Epoch 75/300] [Batch 0] [D loss: 0.158845, acc:  77%] [G loss: 2.796524, adv: 0.866495, recon: 0.050876, id: 0.028491] time: 3:50:22.046602 \n",
      "[Epoch 75/300] [Batch 100] [D loss: 0.211600, acc:  69%] [G loss: 2.418479, adv: 0.770148, recon: 0.042893, id: 0.007105] time: 3:52:25.602743 \n",
      "[Epoch 76/300] [Batch 0] [D loss: 0.097268, acc:  89%] [G loss: 2.989143, adv: 0.955931, recon: 0.051395, id: 0.033988] time: 3:53:26.849816 \n",
      "[Epoch 76/300] [Batch 100] [D loss: 0.148670, acc:  79%] [G loss: 3.304214, adv: 1.125448, recon: 0.051148, id: 0.021297] time: 3:55:28.423772 \n",
      "[Epoch 77/300] [Batch 0] [D loss: 0.125674, acc:  85%] [G loss: 2.939481, adv: 1.010644, recon: 0.044813, id: 0.014428] time: 3:56:29.748803 \n",
      "[Epoch 77/300] [Batch 100] [D loss: 0.117153, acc:  86%] [G loss: 3.086000, adv: 1.035089, recon: 0.049556, id: 0.011685] time: 3:58:31.361273 \n",
      "[Epoch 78/300] [Batch 0] [D loss: 0.196888, acc:  72%] [G loss: 3.092125, adv: 0.983563, recon: 0.053433, id: 0.009774] time: 3:59:32.643115 \n",
      "[Epoch 78/300] [Batch 100] [D loss: 0.133743, acc:  83%] [G loss: 3.348426, adv: 1.140350, recon: 0.052159, id: 0.015553] time: 4:01:34.155936 \n",
      "[Epoch 79/300] [Batch 0] [D loss: 0.087964, acc:  92%] [G loss: 3.041593, adv: 1.022605, recon: 0.048538, id: 0.017434] time: 4:02:35.431983 \n",
      "[Epoch 79/300] [Batch 100] [D loss: 0.122027, acc:  84%] [G loss: 3.372287, adv: 1.190139, recon: 0.048380, id: 0.008133] time: 4:04:39.653205 \n",
      "[Epoch 80/300] [Batch 0] [D loss: 0.115802, acc:  87%] [G loss: 2.763577, adv: 0.898343, recon: 0.047341, id: 0.013872] time: 4:05:40.889207 \n",
      "[Epoch 80/300] [Batch 100] [D loss: 0.143827, acc:  81%] [G loss: 2.994150, adv: 1.014491, recon: 0.047605, id: 0.004190] time: 4:07:42.574305 \n",
      "[Epoch 81/300] [Batch 0] [D loss: 0.161195, acc:  81%] [G loss: 3.316450, adv: 1.152072, recon: 0.049130, id: 0.011610] time: 4:08:44.145852 \n",
      "[Epoch 81/300] [Batch 100] [D loss: 0.098222, acc:  89%] [G loss: 3.761825, adv: 1.335431, recon: 0.047902, id: 0.069341] time: 4:10:45.798717 \n",
      "[Epoch 82/300] [Batch 0] [D loss: 0.202168, acc:  66%] [G loss: 2.461802, adv: 0.761337, recon: 0.045639, id: 0.017163] time: 4:11:47.086342 \n",
      "[Epoch 82/300] [Batch 100] [D loss: 0.140685, acc:  80%] [G loss: 2.558952, adv: 0.834327, recon: 0.043600, id: 0.009496] time: 4:13:48.607667 \n",
      "[Epoch 83/300] [Batch 0] [D loss: 0.099121, acc:  89%] [G loss: 3.041368, adv: 0.974708, recon: 0.053936, id: 0.006242] time: 4:14:49.933226 \n",
      "[Epoch 83/300] [Batch 100] [D loss: 0.161550, acc:  76%] [G loss: 3.227828, adv: 0.976842, recon: 0.062761, id: 0.011753] time: 4:16:51.523785 \n",
      "[Epoch 84/300] [Batch 0] [D loss: 0.152567, acc:  81%] [G loss: 3.102275, adv: 1.014722, recon: 0.052614, id: 0.013950] time: 4:17:53.160654 \n",
      "[Epoch 84/300] [Batch 100] [D loss: 0.168772, acc:  75%] [G loss: 2.736216, adv: 0.867685, recon: 0.049146, id: 0.009871] time: 4:19:56.888541 \n",
      "[Epoch 85/300] [Batch 0] [D loss: 0.155432, acc:  78%] [G loss: 2.979661, adv: 0.988146, recon: 0.049618, id: 0.005219] time: 4:20:58.198588 \n",
      "[Epoch 85/300] [Batch 100] [D loss: 0.115982, acc:  85%] [G loss: 2.982376, adv: 0.994880, recon: 0.048864, id: 0.008311] time: 4:22:59.880798 \n",
      "[Epoch 86/300] [Batch 0] [D loss: 0.142876, acc:  82%] [G loss: 2.900354, adv: 0.955429, recon: 0.048601, id: 0.010656] time: 4:24:01.249742 \n",
      "[Epoch 86/300] [Batch 100] [D loss: 0.120221, acc:  84%] [G loss: 3.110605, adv: 1.039372, recon: 0.044834, id: 0.128485] time: 4:26:02.889080 \n",
      "[Epoch 87/300] [Batch 0] [D loss: 0.178024, acc:  70%] [G loss: 3.208810, adv: 1.100069, recon: 0.047647, id: 0.050078] time: 4:27:04.137222 \n",
      "[Epoch 87/300] [Batch 100] [D loss: 0.105055, acc:  88%] [G loss: 3.033458, adv: 1.049214, recon: 0.045606, id: 0.016583] time: 4:29:05.756383 \n",
      "[Epoch 88/300] [Batch 0] [D loss: 0.110022, acc:  87%] [G loss: 3.163547, adv: 1.019260, recon: 0.055090, id: 0.013345] time: 4:30:07.116212 \n",
      "[Epoch 88/300] [Batch 100] [D loss: 0.119388, acc:  84%] [G loss: 3.044076, adv: 0.976321, recon: 0.054035, id: 0.003570] time: 4:32:09.129748 \n",
      "[Epoch 89/300] [Batch 0] [D loss: 0.121746, acc:  85%] [G loss: 2.968703, adv: 0.997277, recon: 0.048028, id: 0.005141] time: 4:33:12.800697 \n",
      "[Epoch 89/300] [Batch 100] [D loss: 0.150701, acc:  78%] [G loss: 3.060698, adv: 1.048016, recon: 0.047605, id: 0.006494] time: 4:35:14.708672 \n",
      "[Epoch 90/300] [Batch 0] [D loss: 0.123511, acc:  86%] [G loss: 3.147056, adv: 0.915265, recon: 0.062992, id: 0.034251] time: 4:36:16.482549 \n",
      "[Epoch 90/300] [Batch 100] [D loss: 0.127749, acc:  85%] [G loss: 3.011070, adv: 1.017468, recon: 0.047839, id: 0.011986] time: 4:38:18.995395 \n",
      "[Epoch 91/300] [Batch 0] [D loss: 0.261415, acc:  52%] [G loss: 2.292613, adv: 0.684605, recon: 0.045425, id: 0.005859] time: 4:39:21.262239 \n",
      "[Epoch 91/300] [Batch 100] [D loss: 0.168731, acc:  71%] [G loss: 2.355534, adv: 0.757768, recon: 0.041167, id: 0.011152] time: 4:41:23.705645 \n",
      "[Epoch 92/300] [Batch 0] [D loss: 0.178395, acc:  69%] [G loss: 2.183357, adv: 0.635950, recon: 0.042508, id: 0.006499] time: 4:42:25.439657 \n",
      "[Epoch 92/300] [Batch 100] [D loss: 0.230086, acc:  62%] [G loss: 2.175723, adv: 0.602336, recon: 0.045795, id: 0.050300] time: 4:44:27.773509 \n",
      "[Epoch 93/300] [Batch 0] [D loss: 0.185224, acc:  71%] [G loss: 2.620209, adv: 0.842678, recon: 0.045522, id: 0.008721] time: 4:45:29.517948 \n",
      "[Epoch 93/300] [Batch 100] [D loss: 0.098934, acc:  90%] [G loss: 2.852402, adv: 1.009422, recon: 0.040040, id: 0.009408] time: 4:47:32.212797 \n",
      "[Epoch 94/300] [Batch 0] [D loss: 0.204177, acc:  68%] [G loss: 2.843322, adv: 0.861257, recon: 0.050272, id: 0.108048] time: 4:48:36.259248 \n",
      "[Epoch 94/300] [Batch 100] [D loss: 0.099218, acc:  90%] [G loss: 3.230530, adv: 1.071394, recon: 0.051460, id: 0.006719] time: 4:50:38.683597 \n",
      "[Epoch 95/300] [Batch 0] [D loss: 0.189241, acc:  71%] [G loss: 3.052031, adv: 1.043226, recon: 0.046680, id: 0.026941] time: 4:51:40.427508 \n",
      "[Epoch 95/300] [Batch 100] [D loss: 0.135074, acc:  82%] [G loss: 2.994721, adv: 1.005094, recon: 0.047836, id: 0.010525] time: 4:53:42.979032 \n",
      "[Epoch 96/300] [Batch 0] [D loss: 0.125828, acc:  86%] [G loss: 3.137321, adv: 0.994892, recon: 0.056597, id: 0.009555] time: 4:54:44.763032 \n",
      "[Epoch 96/300] [Batch 100] [D loss: 0.191461, acc:  71%] [G loss: 3.092974, adv: 0.978018, recon: 0.055446, id: 0.012452] time: 4:56:47.344275 \n",
      "[Epoch 97/300] [Batch 0] [D loss: 0.147072, acc:  80%] [G loss: 3.019765, adv: 1.018354, recon: 0.047226, id: 0.033078] time: 4:57:49.102169 \n",
      "[Epoch 97/300] [Batch 100] [D loss: 0.150265, acc:  80%] [G loss: 2.959393, adv: 0.951352, recon: 0.051595, id: 0.005504] time: 4:59:51.581986 \n",
      "[Epoch 98/300] [Batch 0] [D loss: 0.208698, acc:  68%] [G loss: 3.086781, adv: 1.003621, recon: 0.052482, id: 0.023809] time: 5:00:53.275295 \n",
      "[Epoch 98/300] [Batch 100] [D loss: 0.164217, acc:  76%] [G loss: 3.131934, adv: 1.010866, recon: 0.054666, id: 0.003569] time: 5:02:55.885518 \n",
      "[Epoch 99/300] [Batch 0] [D loss: 0.164127, acc:  76%] [G loss: 3.223124, adv: 1.057962, recon: 0.054229, id: 0.016372] time: 5:04:00.144538 \n",
      "[Epoch 99/300] [Batch 100] [D loss: 0.126010, acc:  84%] [G loss: 3.096851, adv: 1.022554, recon: 0.051302, id: 0.019578] time: 5:06:02.548022 \n",
      "[Epoch 100/300] [Batch 0] [D loss: 0.097581, acc:  88%] [G loss: 3.181524, adv: 1.043107, recon: 0.053655, id: 0.008594] time: 5:07:04.318267 \n",
      "[Epoch 100/300] [Batch 100] [D loss: 0.182293, acc:  72%] [G loss: 3.164021, adv: 0.986124, recon: 0.058251, id: 0.011261] time: 5:09:06.786911 \n",
      "[Epoch 101/300] [Batch 0] [D loss: 0.242113, acc:  48%] [G loss: 2.234622, adv: 0.660649, recon: 0.044170, id: 0.022180] time: 5:10:08.659142 \n",
      "[Epoch 101/300] [Batch 100] [D loss: 0.194893, acc:  67%] [G loss: 2.135643, adv: 0.633693, recon: 0.042641, id: 0.007169] time: 5:12:11.071146 \n",
      "[Epoch 102/300] [Batch 0] [D loss: 0.259583, acc:  52%] [G loss: 2.416040, adv: 0.690500, recon: 0.051203, id: 0.005486] time: 5:13:12.762752 \n",
      "[Epoch 102/300] [Batch 100] [D loss: 0.180176, acc:  71%] [G loss: 2.256097, adv: 0.643633, recon: 0.047542, id: 0.011510] time: 5:15:15.293631 \n",
      "[Epoch 103/300] [Batch 0] [D loss: 0.187727, acc:  72%] [G loss: 2.569952, adv: 0.838146, recon: 0.042854, id: 0.030797] time: 5:16:17.073949 \n",
      "[Epoch 103/300] [Batch 100] [D loss: 0.166798, acc:  76%] [G loss: 2.543145, adv: 0.796610, recon: 0.046666, id: 0.011761] time: 5:18:19.400716 \n",
      "[Epoch 104/300] [Batch 0] [D loss: 0.181144, acc:  75%] [G loss: 2.709029, adv: 0.886709, recon: 0.045820, id: 0.011092] time: 5:19:21.282279 \n",
      "[Epoch 104/300] [Batch 100] [D loss: 0.149886, acc:  79%] [G loss: 2.962554, adv: 1.009229, recon: 0.046420, id: 0.009075] time: 5:21:26.346052 \n",
      "[Epoch 105/300] [Batch 0] [D loss: 0.187133, acc:  70%] [G loss: 2.986022, adv: 0.975417, recon: 0.051137, id: 0.006745] time: 5:22:28.066284 \n",
      "[Epoch 105/300] [Batch 100] [D loss: 0.143267, acc:  80%] [G loss: 2.882450, adv: 0.969915, recon: 0.046256, id: 0.010647] time: 5:24:30.536466 \n",
      "[Epoch 106/300] [Batch 0] [D loss: 0.281054, acc:  45%] [G loss: 1.964368, adv: 0.559605, recon: 0.041250, id: 0.009846] time: 5:25:32.267462 \n",
      "[Epoch 106/300] [Batch 100] [D loss: 0.154672, acc:  71%] [G loss: 2.323666, adv: 0.718544, recon: 0.043801, id: 0.004668] time: 5:27:34.836010 \n",
      "[Epoch 107/300] [Batch 0] [D loss: 0.171756, acc:  70%] [G loss: 2.132919, adv: 0.667051, recon: 0.039011, id: 0.013044] time: 5:28:36.614025 \n",
      "[Epoch 107/300] [Batch 100] [D loss: 0.156554, acc:  73%] [G loss: 2.119038, adv: 0.653123, recon: 0.039691, id: 0.012194] time: 5:30:39.104212 \n",
      "[Epoch 108/300] [Batch 0] [D loss: 0.209313, acc:  62%] [G loss: 2.222885, adv: 0.640775, recon: 0.045714, id: 0.021492] time: 5:31:40.915023 \n",
      "[Epoch 108/300] [Batch 100] [D loss: 0.192584, acc:  69%] [G loss: 2.427737, adv: 0.805720, recon: 0.040086, id: 0.009288] time: 5:33:43.398482 \n",
      "[Epoch 109/300] [Batch 0] [D loss: 0.171133, acc:  71%] [G loss: 2.292922, adv: 0.690232, recon: 0.044605, id: 0.014431] time: 5:34:45.120579 \n",
      "[Epoch 109/300] [Batch 100] [D loss: 0.163317, acc:  74%] [G loss: 2.168442, adv: 0.702975, recon: 0.037391, id: 0.008267] time: 5:36:47.247523 \n",
      "[Epoch 110/300] [Batch 0] [D loss: 0.165554, acc:  75%] [G loss: 2.202427, adv: 0.683120, recon: 0.040947, id: 0.003365] time: 5:37:51.105363 \n",
      "[Epoch 110/300] [Batch 100] [D loss: 0.177710, acc:  73%] [G loss: 2.086298, adv: 0.674591, recon: 0.036394, id: 0.003614] time: 5:39:52.678034 \n",
      "[Epoch 111/300] [Batch 0] [D loss: 0.175530, acc:  72%] [G loss: 2.215934, adv: 0.712635, recon: 0.038965, id: 0.003057] time: 5:40:54.170625 \n",
      "[Epoch 111/300] [Batch 100] [D loss: 0.162084, acc:  73%] [G loss: 2.064090, adv: 0.645252, recon: 0.037682, id: 0.010383] time: 5:42:55.938717 \n",
      "[Epoch 112/300] [Batch 0] [D loss: 0.160046, acc:  76%] [G loss: 2.382081, adv: 0.825100, recon: 0.036137, id: 0.002461] time: 5:43:57.307091 \n",
      "[Epoch 112/300] [Batch 100] [D loss: 0.199059, acc:  66%] [G loss: 2.146242, adv: 0.717242, recon: 0.035022, id: 0.004764] time: 5:45:59.006475 \n",
      "[Epoch 113/300] [Batch 0] [D loss: 0.196981, acc:  65%] [G loss: 2.551062, adv: 0.896218, recon: 0.037247, id: 0.002591] time: 5:47:00.345869 \n",
      "[Epoch 113/300] [Batch 100] [D loss: 0.234436, acc:  62%] [G loss: 2.025842, adv: 0.612929, recon: 0.039267, id: 0.007086] time: 5:49:01.991377 \n",
      "[Epoch 114/300] [Batch 0] [D loss: 0.138587, acc:  81%] [G loss: 2.086414, adv: 0.655090, recon: 0.037966, id: 0.003043] time: 5:50:03.338894 \n",
      "[Epoch 114/300] [Batch 100] [D loss: 0.139710, acc:  82%] [G loss: 2.554153, adv: 0.785361, recon: 0.048522, id: 0.004215] time: 5:52:04.993098 \n",
      "[Epoch 115/300] [Batch 0] [D loss: 0.170085, acc:  72%] [G loss: 2.375067, adv: 0.698797, recon: 0.048215, id: 0.002697] time: 5:53:06.414487 \n",
      "[Epoch 115/300] [Batch 100] [D loss: 0.170182, acc:  72%] [G loss: 2.145807, adv: 0.700661, recon: 0.036828, id: 0.002317] time: 5:55:11.243506 \n",
      "[Epoch 116/300] [Batch 0] [D loss: 0.207691, acc:  66%] [G loss: 2.529676, adv: 0.903509, recon: 0.035210, id: 0.013451] time: 5:56:12.614279 \n",
      "[Epoch 116/300] [Batch 100] [D loss: 0.133629, acc:  83%] [G loss: 2.367238, adv: 0.817234, recon: 0.036141, id: 0.002887] time: 5:58:14.394409 \n",
      "[Epoch 117/300] [Batch 0] [D loss: 0.164108, acc:  73%] [G loss: 2.185158, adv: 0.723580, recon: 0.036344, id: 0.002729] time: 5:59:15.818816 \n",
      "[Epoch 117/300] [Batch 100] [D loss: 0.168662, acc:  74%] [G loss: 2.394852, adv: 0.793369, recon: 0.039932, id: 0.003548] time: 6:01:17.599794 \n",
      "[Epoch 118/300] [Batch 0] [D loss: 0.169356, acc:  73%] [G loss: 2.166976, adv: 0.705839, recon: 0.037342, id: 0.002228] time: 6:02:19.098303 \n",
      "[Epoch 118/300] [Batch 100] [D loss: 0.161529, acc:  79%] [G loss: 2.196370, adv: 0.728159, recon: 0.036510, id: 0.004766] time: 6:04:20.915635 \n",
      "[Epoch 119/300] [Batch 0] [D loss: 0.220057, acc:  65%] [G loss: 2.408659, adv: 0.809072, recon: 0.038960, id: 0.005328] time: 6:05:22.337418 \n",
      "[Epoch 119/300] [Batch 100] [D loss: 0.173096, acc:  73%] [G loss: 2.319933, adv: 0.729469, recon: 0.042240, id: 0.004143] time: 6:07:24.112679 \n",
      "[Epoch 120/300] [Batch 0] [D loss: 0.192472, acc:  70%] [G loss: 2.318233, adv: 0.748232, recon: 0.039658, id: 0.002589] time: 6:08:25.445482 \n",
      "[Epoch 120/300] [Batch 100] [D loss: 0.207304, acc:  69%] [G loss: 2.867579, adv: 1.018585, recon: 0.038831, id: 0.002705] time: 6:10:27.148139 \n",
      "[Epoch 121/300] [Batch 0] [D loss: 0.169434, acc:  77%] [G loss: 2.368651, adv: 0.783335, recon: 0.039690, id: 0.002354] time: 6:11:28.577513 \n",
      "[Epoch 121/300] [Batch 100] [D loss: 0.127708, acc:  83%] [G loss: 2.286129, adv: 0.740071, recon: 0.039748, id: 0.004699] time: 6:13:33.561758 \n",
      "[Epoch 122/300] [Batch 0] [D loss: 0.169573, acc:  75%] [G loss: 2.337137, adv: 0.744765, recon: 0.041594, id: 0.005675] time: 6:14:34.971525 \n",
      "[Epoch 122/300] [Batch 100] [D loss: 0.158444, acc:  77%] [G loss: 2.537261, adv: 0.805365, recon: 0.041046, id: 0.097651] time: 6:16:36.841524 \n",
      "[Epoch 123/300] [Batch 0] [D loss: 0.158060, acc:  78%] [G loss: 2.392125, adv: 0.787182, recon: 0.040209, id: 0.003751] time: 6:17:38.217276 \n",
      "[Epoch 123/300] [Batch 100] [D loss: 0.194668, acc:  71%] [G loss: 2.422076, adv: 0.803351, recon: 0.040277, id: 0.002867] time: 6:19:40.278789 \n",
      "[Epoch 124/300] [Batch 0] [D loss: 0.145187, acc:  78%] [G loss: 2.550878, adv: 0.847703, recon: 0.042368, id: 0.002560] time: 6:20:41.703131 \n",
      "[Epoch 124/300] [Batch 100] [D loss: 0.153931, acc:  76%] [G loss: 2.426244, adv: 0.830013, recon: 0.037762, id: 0.005321] time: 6:22:43.655932 \n",
      "[Epoch 125/300] [Batch 0] [D loss: 0.181079, acc:  73%] [G loss: 2.180610, adv: 0.685183, recon: 0.040006, id: 0.003908] time: 6:23:45.023407 \n",
      "[Epoch 125/300] [Batch 100] [D loss: 0.173447, acc:  74%] [G loss: 2.454246, adv: 0.814637, recon: 0.040873, id: 0.002205] time: 6:25:46.903201 \n",
      "[Epoch 126/300] [Batch 0] [D loss: 0.165174, acc:  76%] [G loss: 2.456025, adv: 0.812623, recon: 0.040997, id: 0.005621] time: 6:26:48.308345 \n",
      "[Epoch 126/300] [Batch 100] [D loss: 0.159182, acc:  78%] [G loss: 2.618334, adv: 0.863904, recon: 0.044116, id: 0.002143] time: 6:28:50.112597 \n",
      "[Epoch 127/300] [Batch 0] [D loss: 0.133526, acc:  82%] [G loss: 2.608964, adv: 0.869008, recon: 0.043147, id: 0.002367] time: 6:29:51.495974 \n",
      "[Epoch 127/300] [Batch 100] [D loss: 0.222883, acc:  62%] [G loss: 2.770196, adv: 0.993354, recon: 0.038746, id: 0.002784] time: 6:31:53.873246 \n",
      "[Epoch 128/300] [Batch 0] [D loss: 0.155295, acc:  80%] [G loss: 2.879793, adv: 0.985596, recon: 0.044203, id: 0.013421] time: 6:32:58.080319 \n",
      "[Epoch 128/300] [Batch 100] [D loss: 0.165047, acc:  77%] [G loss: 2.570754, adv: 0.834540, recon: 0.044371, id: 0.002819] time: 6:35:00.022095 \n",
      "[Epoch 129/300] [Batch 0] [D loss: 0.148793, acc:  79%] [G loss: 2.673668, adv: 0.840284, recon: 0.046247, id: 0.003487] time: 6:36:01.928627 \n",
      "[Epoch 129/300] [Batch 100] [D loss: 0.162859, acc:  77%] [G loss: 2.532693, adv: 0.820029, recon: 0.044073, id: 0.005639] time: 6:38:04.755695 \n",
      "[Epoch 130/300] [Batch 0] [D loss: 0.149525, acc:  78%] [G loss: 2.411409, adv: 0.779710, recon: 0.042041, id: 0.004667] time: 6:39:06.608853 \n",
      "[Epoch 130/300] [Batch 100] [D loss: 0.121659, acc:  86%] [G loss: 2.744445, adv: 0.862235, recon: 0.048856, id: 0.037144] time: 6:41:09.274644 \n",
      "[Epoch 131/300] [Batch 0] [D loss: 0.175184, acc:  75%] [G loss: 2.590048, adv: 0.902991, recon: 0.038718, id: 0.004280] time: 6:42:11.250227 \n",
      "[Epoch 131/300] [Batch 100] [D loss: 0.151888, acc:  77%] [G loss: 2.584618, adv: 0.797538, recon: 0.048638, id: 0.007057] time: 6:44:13.984744 \n",
      "[Epoch 132/300] [Batch 0] [D loss: 0.180537, acc:  74%] [G loss: 2.778809, adv: 0.921051, recon: 0.046223, id: 0.006310] time: 6:45:15.817048 \n",
      "[Epoch 132/300] [Batch 100] [D loss: 0.157728, acc:  77%] [G loss: 2.574074, adv: 0.831715, recon: 0.045022, id: 0.003169] time: 6:47:18.468122 \n",
      "[Epoch 133/300] [Batch 0] [D loss: 0.148316, acc:  79%] [G loss: 3.671761, adv: 1.316824, recon: 0.051105, id: 0.009681] time: 6:48:20.269658 \n",
      "[Epoch 133/300] [Batch 100] [D loss: 0.153971, acc:  78%] [G loss: 2.326470, adv: 0.753117, recon: 0.040534, id: 0.003773] time: 6:50:22.867203 \n",
      "[Epoch 134/300] [Batch 0] [D loss: 0.189665, acc:  71%] [G loss: 2.558036, adv: 0.770822, recon: 0.050324, id: 0.003851] time: 6:51:24.890298 \n",
      "[Epoch 134/300] [Batch 100] [D loss: 0.154553, acc:  81%] [G loss: 2.720682, adv: 0.904051, recon: 0.045182, id: 0.003188] time: 6:53:30.619744 \n",
      "[Epoch 135/300] [Batch 0] [D loss: 0.122328, acc:  87%] [G loss: 2.339815, adv: 0.746727, recon: 0.041881, id: 0.002575] time: 6:54:32.497864 \n",
      "[Epoch 135/300] [Batch 100] [D loss: 0.183041, acc:  73%] [G loss: 2.586579, adv: 0.814788, recon: 0.046758, id: 0.014620] time: 6:56:35.235927 \n",
      "[Epoch 136/300] [Batch 0] [D loss: 0.185442, acc:  74%] [G loss: 2.907774, adv: 0.971075, recon: 0.047699, id: 0.006310] time: 6:57:37.162330 \n",
      "[Epoch 136/300] [Batch 100] [D loss: 0.174527, acc:  74%] [G loss: 2.728229, adv: 0.941169, recon: 0.041901, id: 0.002680] time: 6:59:39.978953 \n",
      "[Epoch 137/300] [Batch 0] [D loss: 0.134837, acc:  82%] [G loss: 2.596625, adv: 0.844972, recon: 0.044813, id: 0.005151] time: 7:00:41.798787 \n",
      "[Epoch 137/300] [Batch 100] [D loss: 0.163452, acc:  77%] [G loss: 2.577350, adv: 0.842792, recon: 0.043697, id: 0.003010] time: 7:02:44.486847 \n",
      "[Epoch 138/300] [Batch 0] [D loss: 0.177113, acc:  73%] [G loss: 2.535388, adv: 0.792047, recon: 0.047136, id: 0.004030] time: 7:03:46.385935 \n",
      "[Epoch 138/300] [Batch 100] [D loss: 0.161532, acc:  76%] [G loss: 2.693047, adv: 0.867502, recon: 0.047211, id: 0.006613] time: 7:05:49.022042 \n",
      "[Epoch 139/300] [Batch 0] [D loss: 0.113809, acc:  86%] [G loss: 3.090601, adv: 1.026419, recon: 0.051345, id: 0.004426] time: 7:06:50.782404 \n",
      "[Epoch 139/300] [Batch 100] [D loss: 0.135302, acc:  81%] [G loss: 2.945504, adv: 1.004049, recon: 0.046376, id: 0.005165] time: 7:08:53.442658 \n",
      "[Epoch 140/300] [Batch 0] [D loss: 0.142729, acc:  81%] [G loss: 2.781943, adv: 0.922569, recon: 0.046372, id: 0.004715] time: 7:09:55.205655 \n",
      "[Epoch 140/300] [Batch 100] [D loss: 0.176807, acc:  76%] [G loss: 2.775589, adv: 0.891778, recon: 0.046037, id: 0.064030] time: 7:11:57.960319 \n",
      "[Epoch 141/300] [Batch 0] [D loss: 0.135356, acc:  81%] [G loss: 2.898324, adv: 0.998377, recon: 0.044534, id: 0.005368] time: 7:13:03.201088 \n",
      "[Epoch 141/300] [Batch 100] [D loss: 0.163572, acc:  76%] [G loss: 3.090887, adv: 1.123607, recon: 0.041638, id: 0.005649] time: 7:15:05.782416 \n",
      "[Epoch 142/300] [Batch 0] [D loss: 0.118603, acc:  86%] [G loss: 2.660962, adv: 0.864402, recon: 0.046114, id: 0.004384] time: 7:16:07.688194 \n",
      "[Epoch 142/300] [Batch 100] [D loss: 0.195000, acc:  69%] [G loss: 3.185683, adv: 1.109428, recon: 0.047012, id: 0.017745] time: 7:18:10.462227 \n",
      "[Epoch 143/300] [Batch 0] [D loss: 0.157111, acc:  76%] [G loss: 2.616625, adv: 0.888342, recon: 0.041560, id: 0.003612] time: 7:19:12.375948 \n",
      "[Epoch 143/300] [Batch 100] [D loss: 0.147092, acc:  81%] [G loss: 2.572328, adv: 0.838322, recon: 0.044327, id: 0.003628] time: 7:21:15.157860 \n",
      "[Epoch 144/300] [Batch 0] [D loss: 0.225829, acc:  64%] [G loss: 2.839894, adv: 0.949938, recon: 0.046511, id: 0.002517] time: 7:22:17.104314 \n",
      "[Epoch 144/300] [Batch 100] [D loss: 0.150899, acc:  80%] [G loss: 2.831179, adv: 0.969460, recon: 0.044052, id: 0.004503] time: 7:24:19.860711 \n",
      "[Epoch 145/300] [Batch 0] [D loss: 0.157362, acc:  78%] [G loss: 2.862126, adv: 0.970049, recon: 0.045486, id: 0.008092] time: 7:25:21.749021 \n",
      "[Epoch 145/300] [Batch 100] [D loss: 0.147942, acc:  80%] [G loss: 2.660774, adv: 0.802169, recon: 0.052363, id: 0.004067] time: 7:27:24.498788 \n",
      "[Epoch 146/300] [Batch 0] [D loss: 0.145414, acc:  80%] [G loss: 2.653173, adv: 0.909619, recon: 0.041202, id: 0.003034] time: 7:28:26.318331 \n",
      "[Epoch 146/300] [Batch 100] [D loss: 0.129819, acc:  83%] [G loss: 2.807372, adv: 0.919293, recon: 0.047931, id: 0.003393] time: 7:30:29.045580 \n",
      "[Epoch 147/300] [Batch 0] [D loss: 0.154438, acc:  78%] [G loss: 2.779836, adv: 0.949032, recon: 0.043661, id: 0.003295] time: 7:31:30.977907 \n",
      "[Epoch 147/300] [Batch 100] [D loss: 0.114502, acc:  87%] [G loss: 3.147547, adv: 1.056273, recon: 0.050900, id: 0.004995] time: 7:33:34.021627 \n",
      "[Epoch 148/300] [Batch 0] [D loss: 0.118626, acc:  86%] [G loss: 2.864517, adv: 0.985681, recon: 0.044039, id: 0.005932] time: 7:34:39.310913 \n",
      "[Epoch 148/300] [Batch 100] [D loss: 0.129232, acc:  83%] [G loss: 2.683987, adv: 0.832161, recon: 0.050025, id: 0.009365] time: 7:36:41.231932 \n",
      "[Epoch 149/300] [Batch 0] [D loss: 0.143297, acc:  81%] [G loss: 3.105207, adv: 0.998123, recon: 0.054944, id: 0.004178] time: 7:37:42.778256 \n",
      "[Epoch 149/300] [Batch 100] [D loss: 0.118124, acc:  85%] [G loss: 2.851452, adv: 0.986134, recon: 0.043410, id: 0.004859] time: 7:39:44.847189 \n",
      "[Epoch 150/300] [Batch 0] [D loss: 0.130305, acc:  83%] [G loss: 2.809560, adv: 0.913651, recon: 0.048407, id: 0.007828] time: 7:40:46.352967 \n",
      "[Epoch 150/300] [Batch 100] [D loss: 0.144258, acc:  82%] [G loss: 3.173917, adv: 1.108213, recon: 0.047360, id: 0.003906] time: 7:42:48.598075 \n",
      "[Epoch 151/300] [Batch 0] [D loss: 0.156561, acc:  76%] [G loss: 2.809746, adv: 0.967787, recon: 0.043208, id: 0.005240] time: 7:43:50.232115 \n",
      "[Epoch 151/300] [Batch 100] [D loss: 0.155198, acc:  77%] [G loss: 3.086177, adv: 1.059552, recon: 0.047389, id: 0.007178] time: 7:45:52.355397 \n",
      "[Epoch 152/300] [Batch 0] [D loss: 0.131433, acc:  82%] [G loss: 3.024361, adv: 1.002575, recon: 0.050477, id: 0.003972] time: 7:46:53.885221 \n",
      "[Epoch 152/300] [Batch 100] [D loss: 0.110462, acc:  86%] [G loss: 2.707951, adv: 0.891813, recon: 0.045767, id: 0.003858] time: 7:48:55.929460 \n",
      "[Epoch 153/300] [Batch 0] [D loss: 0.109464, acc:  87%] [G loss: 2.885862, adv: 0.995076, recon: 0.044198, id: 0.004889] time: 7:49:57.404400 \n",
      "[Epoch 153/300] [Batch 100] [D loss: 0.099228, acc:  89%] [G loss: 2.843937, adv: 0.972088, recon: 0.044486, id: 0.004772] time: 7:51:59.396706 \n",
      "[Epoch 154/300] [Batch 0] [D loss: 0.142749, acc:  80%] [G loss: 2.831699, adv: 0.977364, recon: 0.043262, id: 0.004660] time: 7:53:00.920184 \n",
      "[Epoch 154/300] [Batch 100] [D loss: 0.110126, acc:  86%] [G loss: 2.841644, adv: 0.926330, recon: 0.048516, id: 0.012820] time: 7:55:03.395967 \n",
      "[Epoch 155/300] [Batch 0] [D loss: 0.154344, acc:  79%] [G loss: 2.827034, adv: 0.895395, recon: 0.051064, id: 0.009445] time: 7:56:05.353371 \n",
      "[Epoch 155/300] [Batch 100] [D loss: 0.127505, acc:  84%] [G loss: 2.856243, adv: 0.990664, recon: 0.043292, id: 0.003984] time: 7:58:10.690971 \n",
      "[Epoch 156/300] [Batch 0] [D loss: 0.141752, acc:  82%] [G loss: 3.024421, adv: 0.983201, recon: 0.052357, id: 0.005911] time: 7:59:12.250316 \n",
      "[Epoch 156/300] [Batch 100] [D loss: 0.139265, acc:  83%] [G loss: 3.178129, adv: 1.100430, recon: 0.047495, id: 0.021656] time: 8:01:14.231212 \n",
      "[Epoch 157/300] [Batch 0] [D loss: 0.155090, acc:  80%] [G loss: 2.868162, adv: 0.972161, recon: 0.045403, id: 0.010877] time: 8:02:15.793675 \n",
      "[Epoch 157/300] [Batch 100] [D loss: 0.208809, acc:  68%] [G loss: 2.868165, adv: 0.982327, recon: 0.044596, id: 0.005116] time: 8:04:17.899462 \n",
      "[Epoch 158/300] [Batch 0] [D loss: 0.180997, acc:  75%] [G loss: 2.619098, adv: 0.895659, recon: 0.040803, id: 0.006195] time: 8:05:19.398608 \n",
      "[Epoch 158/300] [Batch 100] [D loss: 0.145356, acc:  81%] [G loss: 2.699436, adv: 0.913032, recon: 0.043243, id: 0.003871] time: 8:07:21.470596 \n",
      "[Epoch 159/300] [Batch 0] [D loss: 0.112541, acc:  86%] [G loss: 3.047923, adv: 1.031589, recon: 0.048542, id: 0.007978] time: 8:08:23.040544 \n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-b9c11522372c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-49831509d780>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epochs, sample_interval)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                 \u001b[0mimgs_A\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimgs_B\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m                 \u001b[1;31m# ----------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m                 \u001b[1;31m#  Train Discriminators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HW-steve\\Documents\\GitHub\\endo_recon\\cycleGAN\\valina\\data_loader2.py\u001b[0m in \u001b[0;36mdata_loader\u001b[1;34m(data1, data2, batch, aug)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maug\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mdata1_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_aug_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata1_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m127.5\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mdata2_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage_aug_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata2_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m127.5\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mdata1_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata1_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\HW-steve\\Documents\\GitHub\\endo_recon\\cycleGAN\\valina\\data_loader2.py\u001b[0m in \u001b[0;36mimage_aug_batch\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.90\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         rotate=(-5, 5), mode='edge')])\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0mimages_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimages_aug\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[1;34m\"\"\"Alias for :func:`~imgaug.augmenters.meta.Augmenter.augment`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxtasksperchild\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment\u001b[1;34m(self, return_batch, hooks, **kwargs)\u001b[0m\n\u001b[0;32m   1977\u001b[0m         )\n\u001b[0;32m   1978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1979\u001b[1;33m         \u001b[0mbatch_aug\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maugment_batch_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1981\u001b[0m         \u001b[1;31m# return either batch or tuple of augmentables, depending on what\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                     hooks=hooks)\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   3125\u001b[0m                     \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3126\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3127\u001b[1;33m                     \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3128\u001b[0m                 )\n\u001b[0;32m   3129\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\meta.py\u001b[0m in \u001b[0;36maugment_batch_\u001b[1;34m(self, batch, parents, hooks)\u001b[0m\n\u001b[0;32m    643\u001b[0m                     \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                     \u001b[0mparents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparents\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m                     hooks=hooks)\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m         \u001b[1;31m# revert augmentables being set to None for non-activated augmenters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\geometric.py\u001b[0m in \u001b[0;36m_augment_batch_\u001b[1;34m(self, batch, random_state, parents, hooks)\u001b[0m\n\u001b[0;32m   1335\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m             batch.images = self._augment_images_by_samples(batch.images,\n\u001b[1;32m-> 1337\u001b[1;33m                                                            samples)\n\u001b[0m\u001b[0;32m   1338\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmaps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\geometric.py\u001b[0m in \u001b[0;36m_augment_images_by_samples\u001b[1;34m(self, images, samples, image_shapes, return_matrices)\u001b[0m\n\u001b[0;32m   1408\u001b[0m                     \u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m                     \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1410\u001b[1;33m                     output_shape=output_shape, backend=self.backend)\n\u001b[0m\u001b[0;32m   1411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1412\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage_warped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\geometric.py\u001b[0m in \u001b[0;36m_warp_affine_arr\u001b[1;34m(arr, matrix, order, mode, cval, output_shape, backend)\u001b[0m\n\u001b[0;32m    207\u001b[0m             \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m             \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m             \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         )\n\u001b[0;32m    211\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimage_warped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\tensor2\\lib\\site-packages\\imgaug\\augmenters\\geometric.py\u001b[0m in \u001b[0;36m_warp_affine_arr_cv2\u001b[1;34m(arr, matrix, cval, mode, order, output_shape)\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mborderMode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[0mborderValue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcval\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         )\n\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gan.train(epochs=300, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2149/2149 [00:05<00:00, 418.21it/s]\n"
     ]
    }
   ],
   "source": [
    "data_path1 = r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20200625_sub04\\Recordings'\n",
    "data1 = load_data(data_path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 17172/17172 [12:06<00:00, 23.63it/s]\n"
     ]
    }
   ],
   "source": [
    "all_data = []\n",
    "data_list = glob.glob(r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20210625\\*' + '/*/Recordings/*.png')\n",
    "for tmp in tqdm.tqdm(data_list): \n",
    "    img = cv2.cvtColor(cv2.imread(tmp),cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (256,256))/127.5 - 1\n",
    "    predictB = gan.g_AB.predict(img[np.newaxis,...])\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    cv2.imwrite(os.path.join(tmp.replace('Recordings', 'trans2')), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529, 256, 256, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.g_BA.load_weights('saved_model/BA_0490.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in data1:\n",
    "    imgA = i[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_BA.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.close()\n",
    "    cv2.imwrite(os.path.join(r'D:\\DATA\\ENDOSCOPY\\00_vrcaps_recoding_data_simulation\\20200625_sub04\\trans2', 'trans_%04d.png'%count), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'images/trans_images/'\n",
    "img_path = 'images/A/'\n",
    "\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))\n",
    "\n",
    "gan.g_AB.load_weights('saved_model/flip_0831/type1_AB_0110.h5')\n",
    "gan.g_BA.load_weights('saved_model/flip_0831/type1_BA_0110.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "    plt.imshow(img_trans)\n",
    "    plt.show()\n",
    "#     cv2.imwrite(os.path.join(save_path, 'trans_%04d.png'%i), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/bong/data/depth_train'\n",
    "save_path = '/bong/data/type1'\n",
    "img_list = sorted(glob.glob(img_path + '/*.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_list)):\n",
    "    imgA = cv2.cvtColor(cv2.imread(img_list[i]),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    img_trans = 0.5 * predictB[0,...] + 0.5\n",
    "    img_trans = np.clip((img_trans * 255).astype(np.uint8), 0, 255)\n",
    "#      plt.imshow(img_trans)\n",
    "#      plt.show()\n",
    "    cv2.imwrite(os.path.join(save_path, 'type1_'+os.path.basename(img_list[i].split('.')[0])+'.png'), cv2.cvtColor(img_trans, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(len(data_file)):\n",
    "    path = glob.glob(os.path.join(data_path, data_file[j]) + '/*.png')\n",
    "    up_file_name = 'type1_%04d_'%j\n",
    "    for i in range(len(path)):\n",
    "        ori = cv2.imread(path[i])\n",
    "        img = ori[:,:256,:]\n",
    "        dep = ori[:,256:,:]\n",
    "        cv2.imwrite(os.path.join(save_img_path, up_file_name + os.path.basename(path[i])), img)\n",
    "        cv2.imwrite(os.path.join(save_dep_path, up_file_name + os.path.basename(path[i])), dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pathA = 'images/A/0001_image_0456.png'\n",
    "img_pathB = 'images/B/0000.png'\n",
    "imgA = cv2.cvtColor(cv2.imread(img_pathA),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1\n",
    "imgB = cv2.cvtColor(cv2.imread(img_pathB),cv2.COLOR_BGR2RGB)[np.newaxis,...]/127.5 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'saved_model/flip_0831'\n",
    "AB_save_list = sorted(glob.glob(save_path + '/type1_AB_*.h5'))\n",
    "BA_save_list = sorted(glob.glob(save_path + '/type1_BA_*.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "for i in range(len(AB_save_list)):\n",
    "    gan.g_AB.load_weights(AB_save_list[i])\n",
    "    gan.g_BA.load_weights(BA_save_list[i])\n",
    "    predictB = gan.g_AB.predict(imgA)\n",
    "    predictA = gan.g_BA.predict(predictB)\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(0.5 * imgA[0,...] + 0.5)\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(0.5 * predictB[0,...] + 0.5)\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(0.5 * predictA[0,...] + 0.5)\n",
    "    plt.show()\n",
    "    print('iter :', AB_save_list[i], 'loss :' , np.mean(np.abs(predictA - imgA)))\n",
    "    loss.append(np.mean(np.abs(predictA - imgA)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(loss).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = [i for i in range(len(loss)) if loss[i] == 0.04742805290815668]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AB_save_list[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "368cec5af9d55f3645ce893451bf6ff97de02598cc0bc19aa254d796841e01eb"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit ('tensor2': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.8"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}